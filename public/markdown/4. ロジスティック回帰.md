---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

## <!-- fit -->📘 第4講：ロジスティック回帰

---

## ✅ 1.1 シナリオ導入

### 📌 問題1：医療診断

> 医者が患者の指標（年齢、BMI、血圧、血糖値など）を入力し、  
> **この人が糖尿病かどうかを判断したい。**

* 出力：**はい／いいえ（1または0）**  
* 「発病確率が－0.2」や「2.4」といった不正な結果は不要

---

### 📌 問題2：スパムメール判定

> メールの特徴（割引文言の有無、大量の画像の有無など）をもとに、  
> このメールがスパムかどうかを判定したい。

* 出力：**はい／いいえ**  
* **線形回帰は使えない**：出力が負になったり1を超えたりし、確率とみなせない

---

### 📌 問題3：顧客離反予測

> 通信会社が顧客の利用行動から、解約するかどうかを予測したい。

* 特徴例：プラン種類、先月の請求額、クレーム履歴…  
* 出力：**離反するかどうか？**

---

### 💡 小まとめ

これらの問題に共通しているのは：

* ✅ 出力は **2つのクラス**（0または1）  
* ✅ モデルから「〇〇％の確率で離反する」といった**確率**を出力したい  
* ❌ **線形回帰では出力を\[0,1\]に保証できない**

---

## ✅ 1.2 ロジスティック回帰とは

### 🔍 問題のおさらい

> 任意の特徴ベクトル $x$ から、どうやって 0～1 の間の確率を得るか？

---

### 📐 ロジスティック回帰の仕組み

線形回帰と同様の構造を使う：

$$
z = w^T x + b
$$

ただし出力時には $z$ をそのまま使わず、**Sigmoid 関数**を通して確率に変換する：

$$
\hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}}
$$

これにより出力が必ず：

$$
0 < \hat{y} < 1
$$

![bg right:40% 90%](https://s.ar8.top/img/picgo/20250514164421594.webp)

---

### 🔁 例

| $z$（線形出力） | $\hat{y}$（確率） |
| -------------- | ---------------- |
| -100           | ≈ 0.00           |
| 0              | 0.5              |
| +100           | ≈ 1.00           |

📌 確率を得た後、しきい値（例：0.5）でクラス判定：

$$
\hat{y} \ge 0.5 \Rightarrow \text{クラス1（正例）}
$$

$$
\hat{y} < 0.5 \Rightarrow \text{クラス0（負例）}
$$

---

## ✅ 小まとめ

* ロジスティック回帰は「回帰」ではなく、**線形構造による分類**  
* 監督学習における**最も基本的な分類アルゴリズム**  
* 出力は**正例に属する確率**  
* より複雑な分類モデル（ニューラルネットワークやSoftmaxなど）の基礎

---

# 🎓 第2節：モデル構造と Sigmoid 関数

## ✅ 2.1 モデル構造のおさらい

前節で述べた通り、ロジスティック回帰の基礎構造は線形結合：

$$
z = w^T x + b
$$

ただし直接 $z$ を出力せず、**活性化関数** Sigmoid を通す：

$$
\hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}}
$$

> ✅ 任意の実数 $z\in(-\infty, +\infty)$ を確率区間 $(0,1)$ に写像できる


---

## ✅ 2.2 なぜ Sigmoid が必要か？

モデルに求めるのは：

* クラス1に属する確率 $P(y=1\mid x)$  
* この確率は**0から1の連続値**でなければならない

線形回帰の出力 $z=w^T x + b$ には制約がなく、負になったり1を超えたりする可能性がある。  
📌 そこで「圧縮器」として Sigmoid を使い、出力を合法な確率にする。

---

## ✅ 2.3 Sigmoid 関数の定義

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

その形状は滑らかな S 字曲線：

* $z=0$ のとき $\sigma(0)=0.5$  
* $z\to+\infty$ のとき $\sigma(z)\to1$  
* $z\to-\infty$ のとき $\sigma(z)\to0$  
![bg right:40% 90%](https://s.ar8.top/img/picgo/20250514164421594.webp)

---

## ✅ 2.4 直感的なイメージ

| 領域        | 特長                                   |
| --------- | ------------------------------------ |
| $z<0$     | 確率<0.5→負例寄り                         |
| $z>0$     | 確率>0.5→正例寄り                         |
| $z=0$     | ちょうど0.5の境界                         |
| 両端で平坦   | 勾配が0に近づき学習が遅くなる（勾配飽和）       |

> 📌 Sigmoid は連続・可微分で、出力が $(0,1)$ に収まるため、確率出力に最適

---

## ✅ 2.5 二分類予測での実際の使い方

最終的に

$$
\hat{y} = \sigma(w^T x + b)
$$

を確率として扱い、しきい値0.5で判定：

* $\hat{y}\ge0.5$ → クラス1  
* $\hat{y}<0.5$ → クラス0  

---

## ✅ 小まとめ

| 項目         | 線形回帰             | ロジスティック回帰       |
| ------------ | ------------------ | -------------------- |
| 出力         | 任意の実数            | 確率 (0～1)          |
| 適用問題     | 回帰予測              | 二分類予測             |
| 活性化関数   | なし                 | Sigmoid             |

---

# 🎓 第3節：損失関数の導出と MSE を使わない理由

---

## ✅ 3.1 予測の良し悪しを評価する方法

ロジスティック回帰の出力は

$$
\hat{y}_i=\sigma(w^T x_i + b)\in(0,1)
$$

これは「正例に属する確率」を表す。  
実際のラベル $y_i\in\{0,1\}$ と比較するには、**損失関数**が必要。

---

## ❌ 3.2 なぜ MSE を使わないか

線形回帰の MSE では

$$
L_{\mathrm{MSE}}=\frac{1}{n}\sum(y_i-\hat{y}_i)^2
$$

しかしロジスティック回帰では：

| 問題               | 説明                                                  |
| ---------------- | --------------------------------------------------- |
| 📉 非凸形状          | Sigmoid と組み合わせると非線形が強くなり極小点が複数生じ、収束困難       |
| 🧮 勾配が急速に小さく | Sigmoid 両端の勾配が小さく、さらに平方で圧縮され学習が停滞しやすい     |

📌 そこで分類に適した **対数損失（Log Loss）** を用いる。

---

## ✅ 3.3 ロジスティック回帰の損失関数：対数損失

### 🚨 分類シーンの要件

* $y_i=1$ のとき、出力確率が大きいほど良い  
* $y_i=0$ のとき、出力確率が小さいほど良い  

対数を用いると：

$$
\text{単一サンプルの損失}=
\begin{cases}
-\log\hat{y}_i & (y_i=1)\\
-\log(1-\hat{y}_i) & (y_i=0)
\end{cases}
$$

まとめて書くと：

$$
L_i=-\bigl[y_i\log\hat{y}_i+(1-y_i)\log(1-\hat{y}_i)\bigr]
$$
![bg right:38% 90%](https://s.ar8.top/img/picgo/20250514175649186.webp)

---

### 🧮 全データでの平均

$$
L(w,b)=-\frac{1}{n}\sum_{i=1}^n\bigl[y_i\log\hat{y}_i+(1-y_i)\log(1-\hat{y}_i)\bigr]
$$

これがロジスティック回帰の目的関数。

---

## ✅ 3.4 なぜ Log Loss が適切か？

| 長所                | 理由                                                 |
| ----------------- | -------------------------------------------------- |
| 📈 より凸           | 単一の極小点で最適化が容易                                  |
| 📊 確率解釈が明確     | 正答に近いほど損失が小さくなる                                |
| 🔁 最大尤度法と一致  | 統計的確率モデルと整合的                                   |

---

### 🔍 例

$y=1$ のとき、予測 $\hat{y}=0.9$ と $0.1$ の損失比較：

* $\hat{y}=0.9$ → $-\log(0.9)\approx0.105$  
* $\hat{y}=0.1$ → $-\log(0.1)\approx2.3$  

誤りが大きいほどペナルティが大きい。

---

# 🎓 第4節：勾配導出とパラメータ更新（ロジスティック回帰）

## 🎯 目標

* 対数損失から勾配を導出する方法を理解  
* 勾配降下法による更新式を習得  
* 線形回帰との類似点と差異を把握

---

## ✅ 4.1 モデル構造と損失関数の復習

予測関数と損失関数は：

$$
\hat{y}_i=\sigma(z_i)=\frac{1}{1+e^{-w^T x_i - b}}
$$

$$
L(w,b)=-\frac{1}{n}\sum_{i=1}^n\bigl[y_i\log\hat{y}_i+(1-y_i)\log(1-\hat{y}_i)\bigr]
$$

この損失を**最小化**する。

---

## ✅ 4.2 $w,b$ の偏微分（手動推導）

$w$ の勾配：

$$
\frac{\partial L}{\partial w}=\frac{1}{n}\sum_{i=1}^n(\hat{y}_i-y_i)x_i
$$

行列形式で：

$$
\nabla_w L=\frac{1}{n}X^T(\hat{y}-y)
$$

バイアス $b$ の勾配：

$$
\frac{\partial L}{\partial b}=\frac{1}{n}\sum_{i=1}^n(\hat{y}_i-y_i)
$$

---

## ✅ 4.3 勾配降下法の更新式

各イテレーションで：

$$
w := w - \alpha\cdot\frac{1}{n}X^T(\hat{y}-y)
$$

$$
b := b - \alpha\cdot\frac{1}{n}\sum_{i=1}^n(\hat{y}_i-y_i)
$$

* $\alpha$：学習率

---

## ✅ 4.4 線形回帰との違い

| 項目       | 線形回帰                  | ロジスティック回帰            |
| -------- | --------------------- | ------------------------- |
| 出力       | $\hat{y}=Xw+b$       | $\hat{y}=\sigma(Xw+b)$    |
| 損失関数   | MSE                  | Log Loss                 |
| 勾配       | $X^T(\hat{y}-y)$       | 同じ形式だが$\hat{y}$がSigmoid |
| 活性化関数 | なし                   | Sigmoid                  |

> ✅ 基本構造は同一、出力層と損失関数が異なるだけ

---

# 🎓 第5節：NumPyによるロジスティック回帰の実装フロー

以下は **NumPy 実装の結果イメージ**：

![](https://s.ar8.top/img/picgo/20250514180941989.webp)

---

![](https://s.ar8.top/img/picgo/20250514181318185.webp)

---

### 🔴 左図：分類結果（決定境界）

* 背景色で予測確率領域を示す：  
  * 赤：クラス1  
  * 青：クラス0  
* 黒枠点：訓練サンプル（実ラベルで着色）  
* 境界線：$\sigma(w^T x + b)=0.5$

---

### 📉 右図：Log Loss の推移

* イテレーションごとの損失減少をプロット  
* 初期は高く、その後迅速に低下し収束

---

# 🎓 第6節：決定境界の可視化＋まとめ

---

## ✅ 6.1 モデル予測結果の可視化

### 🧪 モデル構造のおさらい

$$
\hat{y}=\sigma(w^T x + b)=\frac{1}{1+e^{-z}}\quad (z=w^T x + b)
$$

予測確率 $\hat{y}\in(0,1)$ をしきい値0.5で判定：

* $\hat{y}\ge0.5$ → クラス1  
* $\hat{y}<0.5$ → クラス0  

---

### 📊 可視化ポイント

#### 1. データ点
* 横軸：特徴1  
* 縦軸：特徴2  
* 点の色：実ラベル（赤=1、青=0）

#### 2. 決定境界
* $\sigma(z)=0.5\Leftrightarrow z=0$  
* $w_1 x_1 + w_2 x_2 + b = 0$ の直線

---

## ✅ 6.2 線形回帰との関係まとめ

| 項目       | 線形回帰                     | ロジスティック回帰                       |
| -------- | ------------------------ | ------------------------------------ |
| 出力形式   | $\hat{y}=w^T x + b$       | $\hat{y}=\sigma(w^T x + b)$          |
| 出力範囲   | $(-\infty,+\infty)$       | $(0,1)$                              |
| 問題タイプ | 回帰（数値予測）               | 分類（二値分類）                           |
| 損失関数   | MSE                       | Log Loss                             |
| 決定境界   | なし                        | $\hat{y}=0.5\Rightarrow z=0$ が境界線 |

---

**次回予告：**  
> 🔍 Softmax回帰と多クラス分類の内容
