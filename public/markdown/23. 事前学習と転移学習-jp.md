---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第23講：事前学習と転移学習（Pretraining & Transfer Learning）

---

## 一、なぜ事前学習が必要なのか？

### 1.1 深層学習のボトルネック

* 深層モデルの学習には大量のラベル付きデータが必要。
* しかし：
  * **データのラベリングは高コストで時間がかかる。**
  * **小規模データセットでは過学習しやすい。**
  * **異なるタスク間で知識を共有しにくい。**

→ そこで疑問：  
**「モデルがまず自分で“学び”、その知識を持って新しいタスクに挑む」ことはできないか？**

---

### 1.2 人間の学習からの発想

* 人間はタスクごとにゼロから学ばない。
  * まず**一般的な言語能力**（語彙・文法）を身につけ、
  * それを様々な場面で応用する。
* 機械学習も同じように：
  → **大量の非ラベルデータで事前学習し、得た知識を特定タスクに転移する。**

---

### 1.3 事前学習の目的

* **モデルに汎用的な言語知識を学ばせる：**
  * 単語の意味
  * 文脈的関係
  * 文法構造
* その結果、下流タスクで **少量データでも迅速に適応** できる。

---

## 二、単語ベクトルから事前学習言語モデルへ

### 2.1 静的単語ベクトルの限界

* 初期モデル：Word2Vec、GloVe  
  * 各単語に固定ベクトルを割り当てる。  
  * 同形異義語を区別できない：
    > “bank” = 河岸？銀行？
* 文脈によって意味が変化するため、静的ベクトルでは不十分。

---

### 2.2 文脈ベクトル（Contextual Embedding）

* 新世代のモデル（**ELMo**, **BERT**など）はこれを解決。
* アプローチ：**単語表現を文脈に応じて動的に変化させる。**
  * “bank” は “river bank” と “central bank” で異なるベクトル。

---

### 2.3 ELMo の登場（2018）

> Embeddings from Language Models

* **双方向 LSTM 言語モデル** に基づく。
* 大規模コーパスで事前学習：
  * 順方向 LSTM：次の単語を予測；
  * 逆方向 LSTM：前の単語を予測。
* 各単語の文脈依存的表現を得る。

---

### 2.4 ELMo の構造

1. 入力：単語の文字レベル表現  
2. 双方向 LSTM で文脈を符号化  
3. 出力：異なる層の隠れ状態を加重結合  
   $$
   \text{ELMo}(t) = \gamma \sum_{k} s_k h_{t,k}
   $$
   ここで $s_k$ は学習可能な重み、$\gamma$ はスケーリング係数。

---

### 2.5 ELMo の意義

* 「1単語＝1ベクトル」という制約を突破。
* モデル全体を再学習せず、事前学習済み表現を導入可能。
* NER、感情分析、QAなど多くのNLPタスクで性能大幅向上。

---

## 三、モデルの転移方法

### 3.1 2つの転移戦略

| 戦略 | 説明 | 代表例 |
|------|------|------|
| **Feature-based（特徴抽出型）** | 事前学習モデルを固定し、出力ベクトルを下流タスクの入力として利用。 | ELMo |
| **Fine-tuning（微調整型）** | 事前学習モデル全体を新タスクで再学習。 | BERT, GPT |

---

### 3.2 特徴抽出型（Feature-based）

* 方法：
  * 事前学習モデルのパラメータを凍結；
  * その出力を下流モデルに入力。
* 長所：
  * 安定しており、**破壊的忘却（catastrophic forgetting）** を防ぐ；
  * 計算コストが低い。
* 短所：
  * 新タスクの特性を十分に反映できない。

---

### 3.3 微調整型（Fine-tuning）

* 方法：
  * 事前学習済みパラメータを初期値として利用；
  * 新タスクデータで**全層を再訓練（逆伝播）**。
* 長所：
  * 高い性能、タスク適応が強い；
  * モデルがタスクの意味を理解しやすい。
* 短所：
  * 小規模データでは過学習しやすい。

---

### 3.4 例：BERT の微調整手順

1. 事前学習済み BERT モデルをロード；
2. その上にタスク特化ヘッド（分類層など）を追加；
3. タスクデータで訓練。

$$
L = - \sum_i y_i \log(\text{Softmax}(W h_{[CLS]} + b))
$$

---

### 3.5 比較まとめ

| 項目 | 特徴抽出型（ELMo） | 微調整型（BERT, GPT） |
|------|------------------|------------------|
| パラメータ更新 | 事前学習部を固定 | 全層を更新 |
| 計算コスト | 低い | 高い |
| 柔軟性 | 高い（組合せ自在） | 強い（タスク最適化） |
| 性能 | 良好 | より優秀 |

---

## 四、転移学習の実践プロセス

### 4.1 手順概要

1. **事前学習モデルの選択**  
   * ELMo, BERT, RoBERTa, GPT, DistilBERT など  
2. **モデルパラメータの読み込み**  
   * 通常 Hugging Face などから取得。  
3. **戦略選択**  
   * Feature-based または Fine-tuning。  
4. **訓練と評価**  
   * 感情分類・質問応答などのタスクデータで実施。

---

### 4.2 微調整のコツ

* **学習率は小さく**（例：$1e^{-5} \sim 3e^{-5}$）；
* **層ごとの微調整**：上位層のみ更新；
* **Dropout** による過学習防止；
* **Early Stopping** で検証損失を監視。

---

## 五、実践：事前学習埋め込みで分類精度を向上

### 5.1 タスク：映画レビュー感情分類（IMDB）

* 入力：レビュー文  
* 出力：ポジティブ / ネガティブ  

目的：  
「ランダム初期化埋め込み」と「事前学習済み埋め込み（ELMo）」を比較。

---

### 5.2 実装手順（Feature-based）

1. ELMo モデルをロード；  
2. 各文を文脈ベクトルに変換；  
3. 平均プーリングして分類器に入力（例：BiLSTM + 全結合層）；  
4. 分類層を訓練。

$$
\text{logits} = W_2 \tanh(W_1 \cdot \text{ELMo}(x)) + b
$$

---

### 5.3 実践ポイント

* 事前学習ベクトルにより収束速度が大幅向上；
* 訓練が安定；
* 小規模感情分類で精度 5〜10% 向上。

---

### 5.4 微調整型の改良版

* BERT / RoBERTa による分類微調整：
  * `[CLS]` トークンの隠れ状態を利用；
  * 直接分類ラベルを予測。

```python
from transformers import BertForSequenceClassification
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
````

---

## 六、事前学習と転移学習の意義

* **事前学習モデルはNLPの基盤となった**：

  * BERT、GPT、T5 などがあらゆるタスクを支える。
* **開発コストを大幅に削減**：

  * 少量データでも高性能。
* **マルチタスク・クロスドメイン学習を促進**。

---

## 七、発展の流れ

| 時期        | 代表モデル              | 特徴           |
| --------- | ------------------ | ------------ |
| 2013–2015 | Word2Vec, GloVe    | 静的単語ベクトル     |
| 2018      | ELMo               | 文脈動的ベクトル     |
| 2018–2019 | BERT, GPT          | 深層事前学習 + 微調整 |
| 2020以降    | T5, GPT-3, ChatGPT | 大規模生成型事前学習   |

---

## 八、まとめ

| 概念            | 説明                     |
| ------------- | ---------------------- |
| 事前学習          | 大規模コーパスで汎用言語知識を学習      |
| 転移学習          | 学んだ知識を新タスクに適用          |
| ELMo          | 双方向LSTMによる文脈表現         |
| Feature-based | 固定特徴として下流タスクに利用        |
| Fine-tuning   | 下流タスクでモデルを再訓練          |
| 代表モデル         | ELMo, BERT, GPT, T5 など |

