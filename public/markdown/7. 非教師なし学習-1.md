---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

## <!-- fit -->📘 第7講：非教師なし学習-1

---

# 🎓 第1節：クラスタリングとは？

---

1. **振り返り**

   * 教師あり学習では、$X$ とラベル $y$ を用いてモデルを訓練し予測を行います。
   * しかし、多くの実際の場面ではデータにラベルが **ない**、または **取得が難しい** ことがあります。

2. **問題提起**

   > 「手元にラベルなしのデータしかなく、データ構造やグループ、異常を発見したいときはどうするか？」

---

3. **回答**

   * **教師なし学習**：ラベルに依存せず、データ中の潜在的なパターンを自動で発掘します。
   * **クラスタリング（Clustering）**：類似したサンプルを“まとめて”いくつかの“クラスタ”を形成します。

---

## 1.2 クラスタリングの定義と直感的理解

1. **定義**

   > **クラスタリング**とは、データをいくつかのグループに分割する技術で、**同一クラスタ内**のサンプル同士の類似度が高く、**クラスタ間**の差異が大きくなるようにします。

2. **直感的な例え**

   * 色、形、材質などの特徴に基づいて雑多な物を箱に分類・整理するイメージ。
   * 例：スーパーの商品分類、図書館のテーマ別分類。

---

3. **可視化例**

   * 2次元散布図を描き、3つの点群が肉眼で分かる様子を示す。
   * 同様の“輪”を見つけ、コンピュータにも自動で識別させる。

![bg right:50% 90%](https://s.ar8.top/img/picgo/20250611170019343.webp)

---

## 1.3 クラスタリング vs 分類：本質的な違い

|       | 分類（Classification） | クラスタリング（Clustering）   |
| ----- | ------------------ | --------------------- |
| データ   | $(X, y)$ ラベルあり   | $X$ のみ、ラベルなし        |
| 目的    | 既知のクラスを予測          | 未知の構造・グループを発見         |
| モデル出力 | 離散ラベル $y$        | クラスタラベル（0,1,2…）、意味は任意 |
| 評価    | 精度、再現率など定義済み指標     | シルエット係数、クラスタ内分散、可視化   |

---

## 1.4 主なクラスタリングの応用例

1. **顧客セグメンテーション**

   * EC で購入行動に基づき「高価値」「中価値」「離脱兆候」などに分割
   * 事前ラベル不要で、精度の高いマーケティングに活用

2. **異常検知**

   * ネットワークトラフィックをクラスタリングし、「孤立点」を攻撃や障害とみなす
   * 金融取引クラスタリングで不正検出

---

3. **画像／ドキュメントのグルーピング**

   * 類似画像をまとめて管理しやすくする
   * ニュース記事を自動グループ化し編集支援

4. **次元削減 & 可視化**

   * クラスタリング後に PCA や t-SNE で各クラスタの分布を可視化

---

## 1.5 クラスタリングの基本的な流れ

1. **特徴選択**

   * 元データを特徴ベクトル（購買回数、消費金額など）に変換

2. **距離尺度の選択**

   * ユークリッド距離、コサイン類似度など

3. **アルゴリズムの選択**

   * **K-Means**：最も一般的で高速
   * **階層型クラスタリング**：デンドログラム生成、小規模データ向け
   * **DBSCAN**：密度ベースでノイズ識別可能

---

4. **クラスタリング実行**

   * 各サンプルのクラスタラベルを取得

5. **評価・可視化**

   * シルエット係数、クラスタ内分散
   * 散布図に色分けしてクラスタ分割を表示

---

# 🎓 第2節：K-Means クラスタリング

---

## 1. 理論概要

1. **目的**：$n$ 個のサンプルを $K$ 個のクラスタに分割し、同一クラスタ内で「できる限り似たもの」を、異クラスタ間で「できる限り異なるもの」を実現する。
2. **類似度の尺度**：通常は **ユークリッド距離**

   $$
   d(\mathbf{x}, \mathbf{c}) = \|\mathbf{x} - \mathbf{c}\|_2
   $$
3. **コアアイデア**：

   * $K$ 個の「重心（centroid）」${\mathbf{c}\_1,\dots,\mathbf{c}\_K}$ を管理
   * 以下を交互に繰り返し収束させる：

     1. **割り当て**：各サンプルを最も近い重心に割り当て
     2. **更新**：各クラスタの重心をそのクラスタ内全サンプルの平均に更新

---

## 2. アルゴリズムの流れ

入力：データセット $X = {x\_1, …, x\_n}$、クラスタ数 $K$

1. ランダムに $K$ 個の重心 $c\_1, …, c\_K$ を初期化（サンプルからランダム選択 or K-Means++）
2. Repeat:
   a. 割り当てステップ:
   各サンプル $x\_i$ について距離 $d(x\_i, c\_j)$ を計算し、最も近い重心に割り当て
   b. 更新ステップ:
   各クラスタ j に対して重心を更新：

   $$
   c_j \leftarrow \frac{1}{|S_j|} \sum_{x_i \in S_j} x_i
   $$
---

3. 重心が変わらなくなるか最大イテレーション回数に達したら終了
   出力：各サンプルのクラスタラベルと最終的な重心

---

## 3. sklearn による実践例

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

# 1. サンプルデータ生成
X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.8, random_state=0)

# 2. K-Means モデル訓練
k = 4
km = KMeans(n_clusters=k, init='k-means++', random_state=42)
labels = km.fit_predict(X)
centers = km.cluster_centers_

# 3. 結果可視化
plt.figure(figsize=(6,5))
plt.scatter(X[:,0], X[:,1], c=labels, cmap='tab10', s=30, edgecolor='k', alpha=0.6)
plt.scatter(centers[:,0], centers[:,1],
            c='red', marker='X', s=200, label='Centroids')
plt.title(f'K-Means Clustering (K={k})')
plt.xlabel('Feature 1'); plt.ylabel('Feature 2')
plt.legend()
plt.grid(True)
plt.show()
```

* **パラメータ説明**

  * `n_clusters`: クラスタ数 $K$
  * `init='k-means++'`: 初期化方法
  * `random_state`: 再現性のための乱数シード

---

## 4. クラスタリング評価指標

### 4.1 シルエット係数（Silhouette Score）

$$
s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}
$$

* $a(i)$: サンプル $i$ と同一クラスタ内他サンプルの平均距離
* $b(i)$: サンプル $i$ と最も近い他クラスタ内サンプルの平均距離

```python
from sklearn.metrics import silhouette_score
score = silhouette_score(X, labels)
print(f'Silhouette Score: {score:.3f}')
```

* **範囲**: $-1$ から $+1$。$+1$ に近いほど良好なクラスタリング

---

### 4.2 クラスタ内平方和（Inertia）

`scikit-learn` の `KMeans` モデル属性 `inertia_` は、全サンプルの各クラスタ重心までの**距離の二乗和**（WCSS）を示します：

$$
\text{inertia\_} = \sum_{i=1}^{n} \min_{j=1..K} \|x_i - c_j\|^2
$$

* **小さいほど**: クラスタ内が凝集している（ただし小さすぎると過剰クラスタ化の可能性）
* `肘法 (Elbow Method)` で $K$ を選定

---

# 🎓 第2節：階層型クラスタリング（Agglomerative Clustering）

---

## 1. 理論

### 1.1 ボトムアップ vs トップダウン

* **ボトムアップ（Agglomerative）**

  1. 初期状態：各サンプルが1クラスタ、計 $n$ クラスタ
  2. 最も「似ている」2クラスタを反復的に統合
  3. 目標クラスタ数 $K$ になるか、1クラスタになるまで続ける

* **トップダウン（Divisive）**

  1. 初期状態：全サンプルが1クラスタ
  2. 最も「分割すべき」クラスタを選び2つに分割
  3. 目標クラスタ数になるまで続ける

本節では **ボトムアップ手法** を扱います。

---

### 1.2 クラスタ間距離（Linkage）の定義

各ステップで統合すべき「最も近い」クラスタ対 $A,B$ を選ぶために、
クラスタ間距離 $D(A,B)$ を定義します。

#### 1.2.1 単連結（Single Linkage）

$$
D_{single}(A,B) = \min_{x \in A, y \in B} d(x,y)
$$

* 2クラスタ内全サンプル対の最小距離
* チェーン状クラスタが形成されやすい

---

#### 1.2.2 完全連結（Complete Linkage）

$$
D_{complete}(A,B) = \max_{x \in A, y \in B} d(x,y)
$$

* 2クラスタ内全サンプル対の最大距離
* よりコンパクトなクラスタを生成

#### 1.2.3 平均連結（Average Linkage）

$$
D_{average}(A,B) = \frac{1}{|A|\cdot|B|} \sum_{x\in A}\sum_{y\in B} d(x,y)
$$

* 全サンプル対の平均距離
* 単／完全連結の中間的性質

---

### 1.2.4 Ward 法（最小分散増加）

Ward 法はサンプル間距離ではなく、クラスタ内分散の増加量に基づきます。

1. クラスタ $C$ の誤差平方和 (SSE) を定義：

$$
SSE(C) = \sum_{x\in C} \|x - \mu_C\|^2,
\quad \mu_C = \frac{1}{|C|} \sum_{x\in C} x
$$

2. $A,B$ を統合したときの分散増加量：

$$
\Delta(A,B) = SSE(A\cup B) - SSE(A) - SSE(B)
$$

3. 最小 $\Delta(A,B)$ のペアを統合。

Ward 法は**クラスタ内総分散を最小化**し、均等なクラスタを得やすい。

---

## 1.3 アルゴリズム詳細

1. **初期化**

   * 各サンプル $x\_i$ が1クラスタ、計 $n$ クラスタ ${C\_1, …, C\_n}$

2. **反復処理**
   a. クラスタ間距離行列 ${D(C\_i, C\_j)}$ を計算（任意の Linkage）
   b. 最小の距離を持つクラスタ対 $(C\_p,C\_q)$ を選択
   c. 新クラスタ $C\_{new} = C\_p \cup C\_q$ を作成
   d. $C\_p, C\_q$ をリストから削除し、$C\_{new}$ を追加
   e. 他クラスタとの $D(C\_{new}, C\_k)$ を更新

---

3. **終了条件**

   * 残りクラスタ数が $K$、または1クラスタになるまで

4. **結果**

   * 各元サンプルにクラスタラベルを割り当て
   * 統合履歴からデンドログラムを描画可能

---

## 2. デンドログラム（樹状図）

* デンドログラムは統合の順序と距離を可視化
* 横軸：サンプルまたはクラスタのインデックス
* 縦軸：統合距離（類似度の逆数）
* 切断高さを選択しクラスタ数を決定可能

![bg right:50% 100%](https://s.ar8.top/img/picgo/20250611193029257.webp)

---

## 3. sklearn 実践例

```python
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt

# 既に X (n_samples, 2) のデータがあると仮定
for linkage_method in ['single', 'complete', 'average', 'ward']:
    agg = AgglomerativeClustering(n_clusters=3, linkage=linkage_method)
    labels = agg.fit_predict(X)
    plt.figure(figsize=(4,4))
    plt.scatter(X[:,0], X[:,1], c=labels, cmap='tab10', s=30, edgecolor='k')
    plt.title(f"Agglomerative (linkage={linkage_method})")
    plt.show()
```

* 注意：`ward` はユークリッド距離のみ対応。他のリンクは任意の距離尺度と併用可。

---

## 4. Linkage 方法の比較

| Linkage  | 特徴                        | 適用シーン                 |
| -------- | ------------------------- | --------------------- |
| single   | チェーン状クラスタ形成、細長いクラスタになりやすい | ノイズ多い環境には不向き          |
| complete | 最大距離を最小化し、より丸いクラスタを形成     | 均一なクラスタ形状が欲しい場合       |
| average  | 全ペア距離の平均、バランス型            | single と complete の中間 |
| ward     | クラスタ内分散増加を最小化             | ユークリッド空間で凸形クラスタに最適    |

* single は鎖状効果
* complete はより丸形
* ward が最も堅牢

---

## 5. クラスタリング評価指標

1. **シルエット係数**

   ```python
   from sklearn.metrics import silhouette_score
   score = silhouette_score(X, labels)
   print(score)
   ```
2. **Calinski-Harabasz スコア / Davies–Bouldin 指数**

   * ラベルなしクラスタ評価には**内部評価指標**を用い、Linkage 方法やクラスタ数選定を支援
