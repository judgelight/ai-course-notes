---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

## <!-- fit -->📘 第7讲：非监督学习-1

---

# 🎓 第1节：什么是聚类？

---

1. **回顾**

   * 在监督学习中，我们有 $X$ 和标签 $y$，用来训练模型做预测。
   * 但在很多实际场景中，数据 **没有** 或 **难以获取** 标签。

2. **问题**

   > “当我们手里只有一堆 unlabeled 数据，想要发现数据结构、分组或异常时，怎么办？”

3. **答案**

   * **非监督学习**：不依赖标签，自动挖掘数据中的潜在模式。
   * **聚类（Clustering）**：把相似的样本“分到一起”，形成若干个“簇”。

---

## 1.2 聚类的定义与直观理解

1. **定义**

   > **聚类**是一种将数据划分成若干组的技术，使得**同组内**样本相似度高、**组间**差异大。

2. **直观比喻**

   * 把杂乱无序的物品，根据它们的特征（颜色、形状、材质）分箱打包。
   * 比如：超市里的商品分门别类、图书馆按主题分类。

---

3. **可视化示例**

   * 画一张简单的二维散点图，用肉眼可以分辨 3 群点。
   * 就是要找到类似的“圈”，让电脑也能自动完成。

![bg right:50% 90%](https://s.ar8.top/img/picgo/20250611170019343.webp)

---

## 1.3 聚类 vs 分类：核心区别

|      | 分类（Classification） | 聚类（Clustering）    |
| ---- | ------------------ | ----------------- |
| 数据   | $(X, y)$ 有标签       | 仅 $X$ 无标签         |
| 目标   | 预测已知类别             | 发掘未知结构、分组         |
| 模型输出 | 离散标签 $y$           | 簇标签（0,1,2…），无语义含义 |
| 评价   | 精度、召回率等有标准         | 轮廓系数、簇内方差、可视化     |

---

## 1.4 常见聚类应用场景

1. **客户分群**

   * 电商根据购买行为把用户分成“高价值”“中价值”“潜在流失”
   * 无需预先标注，用于精准营销

2. **异常检测**

   * 网络流量数据聚类，发现“孤立点”可能是攻击或故障
   * 金融交易聚类，检测欺诈

---

3. **图像/文档分组**

   * 将相似图片放一起，便于管理
   * 新闻文章自动分组，辅助编辑

4. **降维 & 可视化**

   * 聚类后再用 PCA/t-SNE 可视化每个簇的分布

---

## 1.5 聚类的基本思路与流程

1. **选择特征**

   * 先把原始数据转成特征向量（如购买次数、消费金额）

2. **选择距离度量**

   * 欧氏距离、余弦相似度等

3. **选择聚类算法**

   * **K-Means**：最常用，速度快
   * **层次聚类**：可生成树状图，适合小数据
   * **DBSCAN**：基于密度，能识别噪声

---

4. **执行聚类**

   * 得到每个样本的“簇标签”

5. **评价与可视化**

   * 轮廓系数、簇内方差
   * 散点图 + 不同颜色展示簇划分

---

# 🎓 第2节： K-Means 聚类

---

## 1. 原理概述

1. **目标**：将 $n$ 个样本分成 $K$ 个簇，使得同簇内样本彼此“尽量相似”、不同簇间“尽量不同”。
2. **度量相似度**：常用**欧氏距离**

   $$
   d(\mathbf{x}, \mathbf{c}) = \|\mathbf{x} - \mathbf{c}\|_2
   $$
3. **核心思想**：

   * 维护 $K$ 个“质心”（centroid）$\{\mathbf{c}_1,\dots,\mathbf{c}_K\}$
   * 交替两步直至收敛：

     1. **分配**：每个样本分配到最近质心
     2. **更新**：每个簇的质心更新为该簇所有样本的平均值

---

## 2. 算法流程

输入：数据集 $X={x₁,…,xₙ}$，簇数 K

1. 随机初始化 K 个质心 $c₁,…,c_K$（可随机选样本或 K-Means++）
2. Repeat:
   a. Assignment Step:
      对每个样本 $xᵢ$，计算距离 $d(xᵢ, cⱼ)$，
      将 $xᵢ$ 分配给最近质心所属簇
   b. Update Step:
      对每个簇 j，更新质心：
        $$
         cⱼ ← (1/|Sⱼ|) ∑_{xᵢ∈Sⱼ} xᵢ
        $$
3. 直到质心不再变化或达到最大迭代次数
输出：每个样本的簇标签，以及最终质心

---

## 3. sklearn 实战示例

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

# 1. 生成示例数据
X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.8, random_state=0)

# 2. 训练 K-Means 模型
k = 4
km = KMeans(n_clusters=k, init='k-means++', random_state=42)
labels = km.fit_predict(X)
centers = km.cluster_centers_

# 3. 可视化结果
plt.figure(figsize=(6,5))
plt.scatter(X[:,0], X[:,1], c=labels, cmap='tab10', s=30, edgecolor='k', alpha=0.6)
plt.scatter(centers[:,0], centers[:,1], 
            c='red', marker='X', s=200, label='Centroids')
plt.title(f'K-Means Clustering (K={k})')
plt.xlabel('Feature 1'); plt.ylabel('Feature 2')
plt.legend()
plt.grid(True)
plt.show()
```

* **参数说明**

  * `n_clusters`：簇数 $K$
  * `init='k-means++'`：初始化方法
  * `random_state`：随机种子，保证结果可复现

---

## 4. 聚类效果评估

### 4.1 轮廓系数（Silhouette Score）

$$
s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}
$$

* $a(i)$：样本 $i$ 与同簇内其他样本的平均距离
* $b(i)$：样本 $i$ 与最近邻簇内样本的平均距离

```python
from sklearn.metrics import silhouette_score
score = silhouette_score(X, labels)
print(f'Silhouette Score: {score:.3f}')
```

* **取值范围**：$-1$ 到 $+1$，越接近 $+1$ 表示聚类效果越好

---

### 4.2 簇内平方和（Inertia）

在 `scikit-learn` 的 `KMeans` 模型中，属性 `inertia_` 表示 **簇内平方和（Within-Cluster Sum of Squares，WCSS）**，即所有样本到各自簇质心的**欧氏距离的平方和**：

$$
\text{inertia\_} = \sum_{i=1}^{n} \min_{j=1..K} \|x_i - c_j\|^2
$$

* **值越小**：各簇内部越紧凑，聚类效果越好（但过小可能意味着簇数过多）。
* 可以结合“肘部法则”（Elbow Method）通过绘制 `inertia_` 随 `K` 变化的曲线，来选取合适的簇数。

---

## 示例代码

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

# 1. 生成示例数据
X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.8, random_state=42)

# 2. 训练不同簇数的 KMeans，并收集 inertia_
inertias = []
K_range = range(1, 10)
for k in K_range:
    km = KMeans(n_clusters=k, init='k-means++', random_state=42)
    km.fit(X)
    inertias.append(km.inertia_)

# 3. 绘制肘部曲线
plt.figure(figsize=(6,4))
plt.plot(K_range, inertias, 'o-', linewidth=2)
plt.title("Elbow Method: Inertia vs. Number of Clusters")
plt.xlabel("Number of clusters (K)")
plt.ylabel("Inertia (WCSS)")
plt.xticks(K_range)
plt.grid(True)
plt.show()
```

* `km.inertia_` 在每次 `fit` 之后即可读取。
* 上面代码演示了如何针对不同的 `K` 值收集 `inertia_`，并绘制“肘部曲线”帮助我们选取合适的聚类数。

---

## 5. 优缺点与实践建议

| 优点                 | 缺点              |
| ------------------ | --------------- |
| 实现简单、效率高           | 需预先指定簇数 K       |
| 对大规模数据表现良好（线性复杂度）  | 对噪声、离群点敏感       |
| K-Means++ 初始化可加速收敛 | 只适合凸形簇，无法发现任意形状 |

**实践建议**：

* 先用手肘法（Elbow Method）或轮廓系数确定合适 $K$
* 对数据做缩放/标准化
* 对噪声敏感，可结合 DBSCAN 做二次清洗

---

# 🎓 第2节：层次聚类（Agglomerative Clustering）

---

## 1. 原理

### 1.1. 自底向上 vs 自顶向下

* **自底向上（Agglomerative）**

  1. 初始时：每个样本各自成簇，共有 $n$ 个簇。
  2. 迭代合并：不断将“最相似”的两个簇合并成一个新簇。
  3. 直到簇数降到目标 $K$ 或所有样本合并为单一簇。

* **自顶向下（Divisive）**

  1. 初始时：所有样本在一个簇中。
  2. 迭代拆分：不断选择一个簇，将它拆分成两个子簇。
  3. 直到得到目标簇数。

本节重点讲 **自底向上**。

---

### 1.2. “最近”如何定义：Linkage 计算方式

在每一步，我们都要在当前的簇集合中寻找一对“最近”的簇 $A, B$ 合并。关键在于定义“簇间距离” $D(A, B)$。常见有：

#### 1.2.1 单链接（Single Linkage）

$$
D_{\text{single}}(A, B) 
= \min_{x \in A,\; y \in B} \; d(x, y)
$$

* 取两簇中所有样本对之间的最小距离。
* 容易产生“链式效应”，簇可能拉长。

---

#### 1.2.2 全链接（Complete Linkage）

$$
D_{\text{complete}}(A, B) 
= \max_{x \in A,\; y \in B} \; d(x, y)
$$

* 取两簇中所有样本对之间的最大距离。
* 倾向生成形状更紧凑的簇。


#### 1.2.3 平均链接（Average Linkage）

$$
D_{\text{average}}(A, B) 
= \frac{1}{|A|\;|B|} 
  \sum_{x \in A} \sum_{y \in B} d(x, y)
$$

* 取两簇中所有样本对距离的算术平均。
* 在单/全链接之间取得平衡。

---

### 1.2.4 Ward’s 方法（最小方差增量）

Ward 方法不是直接基于不同簇样本间距离，而是基于簇内方差增量：

1. 定义簇 $C$ 的代价（误差平方和）：

$$
   \mathrm{SSE}(C)
   = \sum_{x \in C} \|x - \mu_C\|^2,
   \quad \mu_C = \frac{1}{|C|} \sum_{x \in C} x
$$

2. 合并 $A,B$ 后增量：

   $$
   \Delta(A, B)
   = \mathrm{SSE}(A \cup B) - \mathrm{SSE}(A) - \mathrm{SSE}(B)
   $$

3. 选择使 $\Delta(A, B)$ **最小** 的一对簇进行合并。

Ward 方法等价于**最小化簇内的总方差**，通常产生较均衡的簇。

---

## 1.3. 算法详细流程

1. **初始化**

   * 每个样本 $x_i$ 自成一个簇；共有 $n$ 个簇 $\{C_1, \dots, C_n\}$。

2. **重复以下步骤**
   a. **计算簇间距离矩阵** $\{D(C_i, C_j)\}$（任选一种 Linkage）
   b. 找出距离最小的一对簇 $(C_p, C_q)$
   c. 将它们合并成新簇

   $$
   C_{\text{new}} = C_p \cup C_q
   $$

   d. 从簇列表中移除 $C_p, C_q$，加入 $C_{\text{new}}$。
   e. 更新距离矩阵：对所有其他簇 $C_k$ 重新计算
   $\;D(C_{\text{new}}, C_k)$（根据 Linkage 公式）。

---

3. **结束条件**

   * 当剩余簇数 $= K$ 时停止，或者合并到单簇。

4. **结果**

   * 每个原始样本得到一个簇标签。
   * 中间合并历史可用于绘制树状图（Dendrogram）。

---

## 2. Dendrogram（树状图）简介

* **树状图** 展示了每次合并的顺序与距离
* 横轴：样本索引或簇；纵轴：合并距离（相似度的反量）
* 可以直观地**选择截断高度**来确定聚类数

![bg right:50% 100%](https://s.ar8.top/img/picgo/20250611193029257.webp)

---

## 3. sklearn 实战

```python
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt

# 假定已有 X (n_samples, 2) 的数据
for linkage_method in ['single', 'complete', 'average', 'ward']:
    agg = AgglomerativeClustering(n_clusters=3, linkage=linkage_method)
    labels = agg.fit_predict(X)
    plt.figure(figsize=(4,4))
    plt.scatter(X[:,0], X[:,1], c=labels, cmap='tab10', s=30, edgecolor='k')
    plt.title(f"Agglomerative (linkage={linkage_method})")
    plt.show()
```

* **注意**：`ward` 仅适用于欧氏距离；其他链接可配合任意度量。

---

## 4. 链接方式对比

| Linkage  | 特点                   | 适用场景                 |
| -------- | -------------------- | -------------------- |
| single   | 聚簇易被“链式”连接，可能产生“长条”簇 | 噪声较多时不太理想            |
| complete | 簇内最远距离最小化，簇形紧凑       | 希望较均匀簇形              |
| average  | 综合考虑所有点对距离           | 平衡 single 与 complete |
| ward     | 最小化簇间方差增量            | 欧氏空间、凸形簇最佳           |


* single 会形成连锁效应
* complete 效果更圆形
* ward 最稳健

---

## 5. 聚类效果评估

1. **轮廓系数（Silhouette Score）**

   ```python
   from sklearn.metrics import silhouette_score
   score = silhouette_score(X, labels)
   print(score)
   ```
2. **Calinski-Harabasz 分数** / **Davies–Bouldin 指数**

解释：无标签聚类评价需用**内部指标**，帮助选择链接方式和簇数。

