---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

## <!-- fit -->📘 第6讲：神经网络基础（多层感知机，MLP）

---

# 🎓 第1节：引入 —— 为什么需要神经网络？

---

## ✅ 1.1 回顾我们已经学过的模型

我们在前几节课学过：

* **逻辑回归**：用于二分类，适合线性可分问题
* **Softmax 回归**：用于多分类，扩展了输出方式

它们的共同点是：

$$
\hat{y} = \text{Softmax}(W x + b)
$$

这个形式其实是一个**线性模型**。

---

## ❗ 1.2 问题来了：如果数据本身**不是线性可分**怎么办？

### 🧠 异或（XOR）问题：

| $x_1$ | $x_2$ | 类别 |
| ----- | ----- | -- |
| 0     | 0     | 0  |
| 0     | 1     | 1  |
| 1     | 0     | 1  |
| 1     | 1     | 0  |

* 没有任何一条直线可以把类 0 和类 1 完全分开
* 这说明：**线性模型是无法解决这个问题的！**

📌 Softmax 或逻辑回归在面对 XOR 时，会陷入性能瓶颈

---

## 🔍 1.3 这时我们需要什么？

我们希望模型能学会：

* “如果 $x_1 ≠ x_2$，就预测 1”
* “否则预测 0”

这个模式是非线性的，需要“中间计算一些更复杂的特征”才有可能正确分类。

✅ 这就是神经网络的设计理念：

> **通过“隐藏层”+“非线性变换”，提取更高级、更复杂的特征，从而解决非线性问题。**

---

# 🎓 第2节：神经网络的结构组成（感知机 → 多层感知机）

---

## ✅ 2.1 从“感知机”开始

> 感知机（Perceptron）是神经网络最早的雏形

它的结构非常简单：

$$
\hat{y} = \text{sign}(w^T x + b)
$$

* $x \in \mathbb{R}^d$：输入向量
* $w \in \mathbb{R}^d$：权重向量
* $b \in \mathbb{R}$：偏置
* 输出是 +1 或 -1，用于二分类

📌 本质上，它是一个**线性分类器**，只能划出一条超平面做分类

---

### 📉 感知机的局限

* 它只能解决**线性可分问题**
* 对于复杂决策边界（如 XOR 异或），完全无能为力

![bg right:50% 100%](https://s.ar8.top/img/picgo/20250528164758270.webp)

---

## ✅ 2.2 什么是多层感知机（MLP）？

> MLP = 多层神经网络，包含一个或多个**隐藏层（Hidden Layer）**

### 最基本的两层结构（单隐藏层）：

```
Input → Hidden Layer → Output
```

形式表达为：

$$
h = f^{(1)}(W^{(1)} x + b^{(1)})
$$

$$
\hat{y} = f^{(2)}(W^{(2)} h + b^{(2)})
$$

* 第一层提取“中间特征”
* 第二层再进行分类或预测

✅ 如果加入非线性激活函数（如 ReLU、sigmoid），就可以解决非线性问题

---

### ✏️ 网络结构图：

```
输入层：   x1 — x2 — x3
             \   |   /
隐藏层：     h1 — h2 — h3 — h4 (ReLU)
               \  |  /
输出层：         y1 — y2 — y3 (Softmax)
```

---

## ✅ 2.3 各层的作用总结

| 层级  | 作用             | 输出大小                         |
| --- | -------------- | ---------------------------- |
| 输入层 | 接收特征           | $x \in \mathbb{R}^d$         |
| 隐藏层 | 提取非线性中间特征      | $h \in \mathbb{R}^{n_h}$     |
| 输出层 | 映射到分类结果（多类/概率） | $\hat{y} \in \mathbb{R}^{K}$ |

📌 多层叠加本质上是“函数的函数”，也称为**函数复合**（function composition）：

$$
\hat{y} = f_3(f_2(f_1(x)))
$$

---

## ✅ 2.4 多层结构的能力

* **一个隐藏层 + ReLU** → 可以拟合任意连续函数（万能近似定理）
* 隐藏层越多 → 表达能力越强
* 神经网络本质上是一个**参数化的函数近似器**

> ✅ Softmax 回归只是 MLP 的特殊情况（没有隐藏层）

---

## 🧠 小结

* 感知机只能处理线性问题
* 多层感知机（MLP）通过隐藏层+激活函数解决非线性问题
* MLP 的基本结构是：**输入 → 线性变换 → 激活 → 输出**
* MLP 可以通过层层堆叠构建出强大的表达能力，是现代深度学习的基础

---

# 🎓 第3节：前向传播机制（Forward Propagation）

---

## ✅ 3.1 什么是前向传播？

> 前向传播（forward propagation）就是神经网络的“预测过程”。

从输入开始，层层向前传递，直到最终输出预测结果。

每一层的计算公式统一为：

$$
\text{输出} = \text{激活函数} \left( W \cdot \text{输入} + b \right)
$$

---

## ✅ 3.2 两层 MLP 的前向传播结构

设：

* 输入特征：$x \in \mathbb{R}^d$
* 第一层权重：$W^{[1]} \in \mathbb{R}^{h \times d}$，偏置 $b^{[1]} \in \mathbb{R}^h$
* 第二层权重：$W^{[2]} \in \mathbb{R}^{K \times h}$，偏置 $b^{[2]} \in \mathbb{R}^K$

---

### 💡 计算流程：

#### ① 隐藏层线性变换：

$$
z^{[1]} = W^{[1]} x + b^{[1]}
$$

#### ② 隐藏层激活（非线性变换）：

$$
a^{[1]} = \text{ReLU}(z^{[1]})
$$

#### ③ 输出层线性变换：

$$
z^{[2]} = W^{[2]} a^{[1]} + b^{[2]}
$$

#### ④ 输出层激活（分类任务常用 Softmax）：

$$
\hat{y} = \text{Softmax}(z^{[2]})
$$

---

## ✅ 3.3 为什么必须使用激活函数？

如果不加激活函数，网络所有层都是线性变换：

$$
f(x) = W_3 W_2 W_1 x + ...
$$

🔁 本质上等价于一个“超级线性层”，没有意义！

所以我们必须在每一层后插入**非线性函数**，如 ReLU、Sigmoid、Tanh 等。

✅ 这样模型才能表达“非线性决策边界”

---

## ✅ 3.4 一张图看懂前向传播：

```
x1 — x2 — x3
 \   |   /     ← Linear: W[1] x + b[1]
  h1 h2 h3 h4   ← ReLU activation
   \  |  /
    y1 y2 y3    ← Softmax(W[2] h + b[2])
```

* 每一层是：点乘 → 加偏置 → 激活
* 输出的是每个类别的概率

![bg right:50% 100%](https://s.ar8.top/img/picgo/20250528183604562.webp)

---

## ✅ 3.5 支持批量输入（Batch）处理

实际训练中，我们常用批量输入：

* 一次输入 $n$ 个样本：$X \in \mathbb{R}^{n \times d}$

前向传播公式调整为：

$$
Z^{[1]} = X W^{[1]T} + b^{[1]}
\quad,\quad
A^{[1]} = \text{ReLU}(Z^{[1]})
$$

继续推导输出层即可。

✅ 这样可以加速训练，提高收敛效率

---

## 🧠 小结

| 步骤   | 内容                                |
| ---- | --------------------------------- |
| 输入层  | 接收原始特征向量 $x$                      |
| 隐藏层  | 做线性变换 → 非线性激活                     |
| 输出层  | 最终分类概率（用 Softmax）                 |
| 整体结构 | 函数嵌套 $\hat{y} = f_3(f_2(f_1(x)))$ |

---

# 🎓 第4节：激活函数的作用（ReLU / Sigmoid / Tanh）

---

## ✅ 4.1 激活函数的必要性

> 没有激活函数，神经网络每层只做线性运算，整体仍是线性模型
> 有了激活函数，就可以模拟复杂的**非线性函数关系**

📌 **非线性激活函数**是神经网络真正具备“学习力”的来源！

---

## ✅ 4.2 三种常见激活函数

| 函数      | 数学表达式                         | 常用范围            |
| ------- | ----------------------------- | --------------- |
| ReLU    | $f(x) = \max(0, x)$           | 默认首选            |
| Sigmoid | $f(x) = \frac{1}{1 + e^{-x}}$ | 输出在 0\~1，适合概率输出 |
| Tanh    | $f(x) = \tanh(x)$             | 输出在 -1\~1，中心对称  |

---

### 🟢 ReLU（Rectified Linear Unit）

* 优点：

  * 计算快，导数为 0 或 1
  * 在正区间不饱和（梯度恒定），**收敛速度快**
* 缺点：

  * 负区间恒为 0，神经元可能“死亡”

🔁 导数图：

* $x > 0$ → 导数为 1
* $x \leq 0$ → 导数为 0

![bg right:40% 100%](https://s.ar8.top/img/picgo/20250528183836082.webp)

---

### 🔵 Sigmoid

* 输出范围在 (0, 1)，常用于输出层（做二分类）
* 缺点：

  * 在 $x \ll 0$ 和 $x \gg 0$ 处梯度趋近于 0，**容易梯度消失**
* 导数最陡的点在 $x = 0$

![bg right:40% 100%](https://s.ar8.top/img/picgo/20250528183957187.webp)

---

### 🟠 Tanh（双曲正切）

* 输出范围：(-1, 1)，是“居中版 sigmoid”
* 对称性好，有时在隐藏层中表现优于 sigmoid
* 缺点：仍然存在梯度消失问题

![bg right:40% 100%](https://s.ar8.top/img/picgo/20250528184043680.webp)

---

## ✅ 4.3 应用建议与经验总结

| 场景           | 推荐激活函数                |
| ------------ | --------------------- |
| 隐藏层          | ReLU（默认），或 Leaky ReLU |
| 输出层（2 类）     | Sigmoid（配合 BCE）       |
| 输出层（多类）      | Softmax（配合交叉熵）        |
| 要求输出在 -1 到 1 | Tanh                  |

---

## ✅ 小结

* 激活函数是神经网络中引入非线性的关键
* 每层的激活函数选择会影响训练速度和收敛质量
* ReLU 是现代默认选择，Sigmoid/Tanh 更适用于特定需求

---

# 🎓 第5节：反向传播简介与训练流程

---

## ✅ 5.1 神经网络是怎么“学习”的？

我们已经知道：

* 前向传播：从输入开始，逐层计算，得到预测 $\hat{y}$
* 计算损失：$L(\hat{y}, y)$，衡量预测和真实标签之间的误差

> ❓ 那接下来，我们如何让网络学会“改进自己”？

答案是：**反向传播（Backpropagation） + 梯度下降**

---

## ✅ 5.2 反向传播机制简要说明

### 🧠 基本思路：

1. **从输出层开始**，计算损失函数对每个参数的导数（梯度）
2. 使用**链式法则**，将梯度一层一层“传递回去”
3. 每一层参数根据梯度进行更新

---

### 🧩 举例说明（二层 MLP）：

模型结构：

$$
x \xrightarrow{W^{[1]}, b^{[1]}} h \xrightarrow{\text{ReLU}} a \xrightarrow{W^{[2]}, b^{[2]}} \hat{y} \xrightarrow{\text{Softmax}} \text{loss}
$$

* 输出层计算梯度 $\nabla_{W^{[2]}}$
* 然后利用链式法则传播到隐藏层，得到 $\nabla_{W^{[1]}}$

📌 每一层都执行一个“局部误差 × 局部导数”的过程（∂loss/∂z × ∂z/∂W）

---

### 🧰 每层参数更新公式（梯度下降）：

$$
W^{[l]} := W^{[l]} - \alpha \cdot \nabla W^{[l]}
\quad , \quad
b^{[l]} := b^{[l]} - \alpha \cdot \nabla b^{[l]}
$$

* $\alpha$：学习率，控制更新幅度
* $\nabla W^{[l]}$：当前层损失对参数的导数

---

## 🔁 5.3 训练流程总结

每一次训练迭代（epoch）：

1. 前向传播，得到预测值 $\hat{y}$
2. 计算损失函数 $L(\hat{y}, y)$
3. **反向传播**计算所有参数的梯度
4. 用梯度下降更新权重
5. 重复若干轮，直到损失收敛

---

## 📉 5.4 可视化建议

![](https://s.ar8.top/img/picgo/20250528185253083.webp)

---

## ✅ 小结

* 反向传播是神经网络训练的核心算法
* 本质是链式法则 + 多层导数的乘积
* 每层参数都根据它对总损失的影响进行更新
* 搭配学习率 $\alpha$，使用梯度下降优化模型性能

---

# 🎓 第6节：NumPy 手动实现的两层 MLP（多层感知机）分类模型

![](https://s.ar8.top/img/picgo/20250528191929878.webp)

---

## 📍左图：MLP 分类器的决策边界（2→10→3 结构）

* 输入维度为 2，隐藏层包含 10 个 ReLU 单元，输出层用 Softmax 分 3 类
* 背景颜色表示模型预测类别区域

  * 每种颜色对应一个类别
* 中心散点为训练样本，颜色为真实标签
* 可见：模型成功学会了复杂的非线性边界（非线性决策）

---

## 📉右图：训练过程中的交叉熵损失变化

* 表明模型在持续学习，误差不断下降
* 大约在前 300\~500 个 epoch 收敛

---

## ✅ 本节实现要点总结：

| 步骤   | 内容                  |
| ---- | ------------------- |
| 前向传播 | 输入 → ReLU → Softmax |
| 损失函数 | 多类交叉熵               |
| 反向传播 | 手动求梯度，逐层更新          |
| 可视化  | 决策边界 + 损失曲线         |

---

# 🎓 第7节：总结 + 后续预告

---

### 📦 模型结构：

* 输入层 → **隐藏层（激活函数 ReLU）** → 输出层（Softmax，多分类）

---

### 📈 学到的知识点：

| 模块   | 内容                                     |
| ---- | -------------------------------------- |
| 网络构成 | 多层结构、隐藏层、激活函数                          |
| 前向传播 | Linear → Activation → Linear → Softmax |
| 激活函数 | ReLU/Sigmoid/Tanh 用法与优缺点               |
| 损失函数 | 交叉熵（适用于多分类）                            |
| 反向传播 | 用链式法则逐层传播误差                            |
| 参数更新 | 使用学习率 + 梯度下降法                          |
| 可视化  | 决策边界 + 损失下降趋势                          |

---

> 多层感知机（MLP）是**所有深度学习网络的基础结构**

* 理解了 MLP，你就能理解：

  * 神经元是如何组织的
  * 网络如何“学习”从错误中优化自己
  * 各类现代神经网络（CNN / RNN / Transformer）都可看作对它的扩展

---


**下一节预告：**
> 🔍 探索数据的隐藏结构——非监督学习入门