---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第23讲：预训练与迁移学习（Pretraining & Transfer Learning）

---

## 一、为什么需要预训练？

### 1.1 深度学习的瓶颈

* 深度模型训练需要大量标注数据。
* 然而：
  * **数据标注昂贵、耗时。**
  * **小数据集容易过拟合。**
  * **不同任务之间共享信息困难。**

→ 我们希望：**能否让模型先“自学”，再“带着知识”去解决新任务？**

---

### 1.2 灵感来源：人类学习方式

* 人类不会每学一个任务都从零开始。
  * 我们先学习**通用语言能力**（词汇、语法）；
  * 然后在不同任务中加以运用。
* 机器也可以这样做：
  → **先在海量无标注数据上预训练，学通用知识，再迁移到特定任务。**

---

### 1.3 预训练的目标

* **让模型学到通用语言知识：**
  * 单词语义
  * 上下文关系
  * 语法结构
* 之后在下游任务中 **少量样本就能快速适应**。

---

## 二、从词向量到预训练语言模型

### 2.1 静态词向量的局限

* 早期模型：Word2Vec、GloVe  
  * 为每个词分配一个固定向量。  
  * 无法区分同形异义词：
    > “bank” = 河岸？银行？
* 句子上下文对词义影响巨大，静态词向量无法捕捉。

---

### 2.2 上下文词向量（Contextual Embedding）

* 新一代模型（如 **ELMo**、**BERT**）解决了这个问题。
* 思路：**词的表示由上下文动态决定**。
  * “bank” 在 “river bank” 与 “central bank” 中向量不同。

---

### 2.3 ELMo 的提出（2018）

> Embeddings from Language Models

* 基于 **双向 LSTM 语言模型**。
* 在大规模文本上预训练：
  * 正向 LSTM 预测下一个词；
  * 反向 LSTM 预测上一个词。
* 得到每个词在上下文中的动态表示。

---

### 2.4 ELMo 的结构

1. 输入：词的字符级表示  
2. 双向 LSTM 编码上下文  
3. 输出：结合不同层的隐藏状态  
   $$
   \text{ELMo}(t) = \gamma \sum_{k} s_k h_{t,k}
   $$
   其中 $s_k$ 是可学习权重，$\gamma$ 是缩放因子。

---

### 2.5 ELMo 的意义

* 打破“一个词一个向量”的局限。
* 不需要重新训练整个模型，只需加载预训练表示。
* 极大提升 NLP 任务性能（命名实体识别、情感分析、问答等）。

---

## 三、模型迁移

### 3.1 两种迁移方式

| 策略 | 说明 | 示例 |
|------|------|------|
| **Feature-based（特征提取）** | 预训练模型作为固定特征提取器，输出向量供下游模型使用。 | ELMo |
| **Fine-tuning（微调）** | 在预训练模型基础上继续训练整个网络。 | BERT、GPT |

---

### 3.2 特征提取方式（Feature-based）

* 思路：
  * 冻结预训练模型参数；
  * 把输出作为输入特征传入下游模型。
* 优点：
  * 训练稳定，避免灾难性遗忘；
  * 计算量小。
* 缺点：
  * 无法完全适应新任务特性。

---

### 3.3 微调方式（Fine-tuning）

* 思路：
  * 使用预训练模型参数作为初始化；
  * 在新任务上 **继续反向传播优化全部参数**。
* 优点：
  * 性能更好，充分利用任务信号；
  * 模型能真正“理解”任务。
* 缺点：
  * 容易过拟合小数据；

---

### 3.4 示例：BERT 的微调过程

1. 加载预训练好的 BERT 模型；
2. 在其顶层加一个任务头（如分类层）；
3. 用任务数据继续训练。

$$
L = - \sum_i y_i \log(\text{Softmax}(W h_{[CLS]} + b))
$$

---

### 3.5 对比总结

| 方面 | 特征提取（ELMo） | 微调（BERT、GPT） |
|------|------------------|------------------|
| 参数更新 | 冻结预训练部分 | 全部参与训练 |
| 计算成本 | 较低 | 较高 |
| 灵活性 | 高（可组合） | 强（任务定制） |
| 性能 | 良好 | 通常更优 |

---

## 四、迁移学习的实践流程

### 4.1 步骤概览

1. **选择预训练模型**
   * ELMo、BERT、RoBERTa、GPT、DistilBERT 等。
2. **加载模型参数**
   * 通常来自开源模型（Hugging Face）。
3. **选择策略**
   * Feature-based 或 Fine-tuning。
4. **训练与评估**
   * 使用任务数据（如情感分类、问答等）。

---

### 4.2 微调技巧

* **学习率要小**（如 $1e^{-5}$ ～ $3e^{-5}$）。
* **层级微调**：只训练顶部几层。
* **Dropout** 防止过拟合。
* **早停法（Early Stopping）** 监控验证集损失。

---

## 五、实践：用预训练嵌入提升分类效果

### 5.1 任务：电影评论情感分类（IMDB）

* 输入：影评文本  
* 输出：正面 / 负面情绪标签  

目标：对比 “随机初始化嵌入” 与 “预训练嵌入（ELMo）”。

---

### 5.2 实现思路（Feature-based）

1. 加载 ELMo 预训练模型；
2. 将每个句子转换为上下文向量；
3. 平均池化后输入分类器（如 BiLSTM + 全连接层）；
4. 训练分类层。

$$
\text{logits} = W_2 \tanh(W_1 \cdot \text{ELMo}(x)) + b
$$

---

### 5.3 实践要点

* 使用预训练向量可显著提升收敛速度；
* 训练更稳定；
* 在小样本情感分析任务上可提升 5~10% 准确率。

---

### 5.4 微调方式的改进版

* 用 BERT / RoBERTa 微调分类：
  * 输入 `[CLS]` token 的隐藏状态；
  * 直接预测分类标签。
* Hugging Face 一行实现：
```python
  from transformers import BertForSequenceClassification
  model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
```

---

## 六、预训练与迁移学习的意义

* **预训练模型成为 NLP 的基础设施**：

  * BERT、GPT、T5 等支撑几乎所有任务。
* **降低了任务开发门槛**：

  * 小样本也能获得强大效果。
* **促进了多任务与跨领域学习**。

---

## 七、发展趋势

| 阶段        | 模型代表               | 特点         |
| --------- | ------------------ | ---------- |
| 2013–2015 | Word2Vec, GloVe    | 静态词向量      |
| 2018      | ELMo               | 上下文动态向量    |
| 2018–2019 | BERT, GPT          | 深层预训练 + 微调 |
| 2020+     | T5, GPT-3, ChatGPT | 大规模生成式预训练  |

---

## 八、总结

| 概念            | 说明                 |
| ------------- | ------------------ |
| 预训练           | 在大规模语料上学习通用语言知识    |
| 迁移学习          | 将已学知识迁移到新任务        |
| ELMo          | 双向 LSTM 的上下文词向量    |
| Feature-based | 提取固定特征进行下游任务       |
| Fine-tuning   | 在下游任务上继续训练模型       |
| 典型模型          | ELMo、BERT、GPT、T5 等 |

