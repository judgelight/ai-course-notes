---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

## <!-- fit -->📘 第4讲：逻辑回归

---

## ✅ 1.1 场景导入

### 📌 问题 1：医疗判断

> 医生输入患者的指标（年龄、BMI、血压、糖分等），希望判断：
> **这个人是否患有糖尿病？**

* 输出要求：**是 / 否（1 或 0）**
* 我们不需要“患病概率是 -0.2” 或 “2.4”，这种不合法结果

---

### 📌 问题 2：垃圾邮件识别

> 给定一封邮件的内容特征（是否有优惠字样？是否有大量图片？），
> 要判断这封邮件是否是垃圾邮件

* 输出要求：**是 / 否**
* **不能直接使用线性回归**：输出值可能是负数或超过 1，不是概率

---

### 📌 问题 3：客户流失预测

> 电信公司希望根据客户使用行为预测是否会解约

* 特征可以是：套餐类型、上月账单、投诉记录……
* 输出仍然是：**是否会流失？**

---

### 💡 小结：

这些问题都有共同特点：

* ✅ 输出是 **两个类别**（0 或 1）
* ✅ 需要模型输出一个概率（例如 85% 的可能会流失）
* ❌ **线性回归不能保证输出在 \[0, 1]**

---

## ✅ 1.2 什么是逻辑回归

### 🔍 问题回顾：

> 如何从任意特征输入 $x$，得到一个 0\~1 之间的概率？

---

### 📐 逻辑回归做了什么？

逻辑回归使用与线性回归相似的结构：

$$
z = w^T x + b
$$

但输出时，不直接使用 $z$，而是将它经过一个 **Sigmoid 函数** 转换为概率：

$$
\hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}}
$$

这样就能确保输出在：

$$
0 < \hat{y} < 1
$$

![bg right:40% 90%](https://s.ar8.top/img/picgo/20250514164421594.webp)

---

### 🔁 举个例子：

| $z$（线性输出） | $\hat{y}$（概率） |
| --------- | ------------- |
| -100      | ≈ 0.00        |
| 0         | 0.5           |
| +100      | ≈ 1.00        |

📌 模型预测概率后，我们可以用一个阈值（例如 0.5）判断是否属于正类：

$$
\text{如果 } \hat{y} \geq 0.5 \Rightarrow \text{预测为 1（正类）}
$$

$$
\text{否则 } \hat{y} < 0.5 \Rightarrow \text{预测为 0（负类）}
$$

---

## ✅ 小结：

* 逻辑回归不是“做回归”，而是**用线性结构来做分类**
* 它是监督学习中**最基础的分类算法**
* 输出的是**属于正类的概率**
* 是构建更复杂分类模型（如神经网络、Softmax）的基础模块

---

# 🎓第2节：模型结构与 Sigmoid 函数

## ✅ 2.1 模型结构回顾

在上一节中我们提到：

逻辑回归模型的基础结构仍然是线性组合：

$$
z = w^T x + b
$$

但我们并不直接用 $z$ 来输出结果，而是将它送入一个**激活函数** —— Sigmoid 函数：

$$
\hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}}
$$

> ✅ 这样就能把任何实数 $z \in (-\infty, +\infty)$ 映射到概率区间 $(0, 1)$

---

## ✅ 2.2 为什么需要 Sigmoid？

我们希望模型输出：

* 属于类别 1 的概率 $P(y=1|x)$
* 这个概率必须是**介于 0 和 1 的连续值**

而线性回归输出的 $z = w^T x + b$ 没有任何限制，可能是负数，也可能超过 1。

📌 所以我们需要一个“压缩器” —— sigmoid，来保证输出结果合法。

---

## ✅ 2.3 Sigmoid 函数的数学形式

Sigmoid 函数定义如下：

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

其图像形状是一个**平滑的 S 形曲线**：

* $z = 0$ 时，$\sigma(0) = 0.5$
* 当 $z \to +\infty$，$\sigma(z) \to 1$
* 当 $z \to -\infty$，$\sigma(z) \to 0$
![bg right:40% 90%](https://s.ar8.top/img/picgo/20250514164421594.webp)
---

## ✅ 2.4 图像直观理解


| 区域      | 特性                  |
| ------- | ------------------- |
| $z < 0$ | 概率小于 0.5，预测趋向负类     |
| $z > 0$ | 概率大于 0.5，预测趋向正类     |
| $z = 0$ | 正好处于 0.5 的分界线上      |
| 两端平坦    | 导数趋近于 0，学习速度慢（梯度饱和） |

> 📌 Sigmoid 是连续、可导的，且输出在 $(0, 1)$，完美满足“概率输出”的需求。

---

## ✅ 2.5 二分类预测的实际使用

最终我们用这个输出 $\hat{y} = \sigma(w^T x + b)$ 作为概率，如果：

* $\hat{y} \geq 0.5$，就预测为 1（正类）
* $\hat{y} < 0.5$，就预测为 0（负类）

---

## ✅ 小结

| 项目   | 线性回归 | 逻辑回归      |
| ---- | ---- | --------- |
| 输出   | 任意实数 | 概率 (0\~1) |
| 适用问题 | 回归预测 | 二分类预测     |
| 激活函数 | 无    | Sigmoid   |


---

# 🎓 第3节：损失函数推导与为什么不用均方误差

---

## ✅ 3.1 学会“如何衡量预测的好坏” 

我们知道逻辑回归模型输出的是：

$$
\hat{y}_i = \sigma(w^T x_i + b) \in (0, 1)
$$

这表示的是“属于正类（1）”的概率。
我们现在要回答一个问题：

> ✨ 怎么评估这个预测概率和真实标签 $y_i \in \{0, 1\}$ 的差距？

这就需要一个损失函数（loss function）。

---

## ❌ 3.2 为什么不能用 MSE（均方误差）？

虽然在线性回归中我们使用：

$$
L_{\text{MSE}} = \frac{1}{n} \sum (y_i - \hat{y}_i)^2
$$

但是在逻辑回归中，**MSE 有两个严重问题**：

| 问题        | 解释                                         |
| --------- | ------------------------------------------ |
| 📉 非凸形状   | Sigmoid 本身非线性，加上平方后形状复杂，容易有多个极值点（梯度下降难收敛）  |
| 🧮 梯度变小太快 | Sigmoid 两端梯度趋近 0，MSE 的导数进一步“压缩”，导致训练变慢甚至停滞 |

📌 所以我们要换成更适合分类任务的函数 —— **对数损失函数（Log Loss）**

---

## ✅ 3.3 逻辑回归的损失函数：对数损失（Log Loss）

### 🚨 分类场景要求：

* 如果真实标签是 1，模型输出概率越大越好
* 如果真实标签是 0，模型输出概率越小越好

这可以用对数概率来量化：

$$
\text{单个样本的 loss} =
\begin{cases}
- \log \hat{y}_i & \text{如果 } y_i = 1 \\
- \log (1 - \hat{y}_i) & \text{如果 } y_i = 0
\end{cases}
$$

把两种情况统一写法：

$$
L_i = - \left[ y_i \log \hat{y}_i + (1 - y_i) \log (1 - \hat{y}_i) \right]
$$

这是我们要最小化的每个样本的损失。

![bg right:40% 90%](https://s.ar8.top/img/picgo/20250514175649186.webp)
---

### 🧮 对整个数据集求平均：

$$
L(w, b) = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log \hat{y}_i + (1 - y_i) \log (1 - \hat{y}_i) \right]
$$

✅ 这就是逻辑回归的核心目标函数（对数损失）。

---

## ✅ 3.4 为什么 Log Loss 更合适？

| 优点            | 原因                   |
| ------------- | -------------------- |
| 📈 更凸         | 曲线更平滑、单极小值，便于优化      |
| 📊 概率解释清晰     | 预测越接近正确标签，对数值越大，惩罚越小 |
| 🔁 可直接与最大似然结合 | 与统计学习的概率建模一致         |

---

### 🔍 举个例子：

假设 $y = 1$，我们希望模型给出概率 $\hat{y} = 0.9$

* 若预测 $\hat{y} = 0.9$，则 loss ≈ -log(0.9) ≈ 0.105
* 若预测 $\hat{y} = 0.1$，则 loss ≈ -log(0.1) ≈ 2.3

➡️ 错得越多，惩罚越大。

---

# 🎓 第4节：梯度推导与参数更新（逻辑回归）

## 🎯 目标

* 理解如何从对数损失函数推导出梯度
* 掌握梯度下降在逻辑回归中的具体更新公式
* 对比线性回归，发现结构上的相似性与关键差异

---

## ✅ 4.1 回顾：模型结构与损失函数

我们在上一节中得到了逻辑回归的预测函数和损失函数：

$$
\hat{y}_i = \sigma(z_i) = \frac{1}{1 + e^{-w^T x_i - b}}
$$

$$
L(w, b) = -\frac{1}{n} \sum_{i=1}^{n} \left[y_i \log \hat{y}_i + (1 - y_i) \log(1 - \hat{y}_i)\right]
$$

我们现在的目标是：**最小化这个损失函数**

---

## ✅ 4.2 对 $w$、$b$ 求偏导（手动推导）

我们先推导 $w$ 的梯度：

$$
\frac{\partial L}{\partial w} = \frac{1}{n} \sum_{i=1}^{n} \left( \hat{y}_i - y_i \right) x_i
$$

这可以简写为向量形式（矩阵乘法）：

$$
\nabla_w L = \frac{1}{n} X^T (\hat{y} - y)
$$

其中：

* $X \in \mathbb{R}^{n \times d}$：特征矩阵，每行为一个样本
* $\hat{y} \in \mathbb{R}^n$：预测概率向量
* $y \in \mathbb{R}^n$：真实标签向量

---

对于偏置项 $b$，导数非常直观：

$$
\frac{\partial L}{\partial b} = \frac{1}{n} \sum_{i=1}^{n} \left( \hat{y}_i - y_i \right)
$$

---

## ✅ 4.3 梯度下降更新公式

每轮迭代中，我们使用如下规则更新参数：

$$
w := w - \alpha \cdot \frac{1}{n} X^T (\hat{y} - y)
$$

$$
b := b - \alpha \cdot \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)
$$

其中：

* $\alpha$：学习率（learning rate），控制“往下降一步”的步长

---

## ✅ 4.4 和线性回归有什么不同？

| 项目   | 线性回归                | 逻辑回归                        |
| ---- | ------------------- | --------------------------- |
| 输出   | $\hat{y} = Xw + b$  | $\hat{y} = \sigma(Xw + b)$  |
| 损失函数 | MSE                 | Log Loss                    |
| 梯度   | $X^T (\hat{y} - y)$ | 一样，但 $\hat{y}$ 是 sigmoid 输出 |
| 激活函数 | 无                   | sigmoid                     |

> ✅ 所以**逻辑回归和线性回归的训练结构高度一致**，只是在输出层和损失函数上有所不同。

---

# 🎓 第5节：NumPy 实现逻辑回归的完整训练流程

如图所示，这是完整的 **逻辑回归 NumPy 实现结果**：

![](https://s.ar8.top/img/picgo/20250514180941989.webp)

---

![](https://s.ar8.top/img/picgo/20250514181318185.webp)

---

### 🔴 左图：模型分类效果（决策边界）

* 背景颜色表示预测概率分界：

  * 红色区域：模型预测为类别 1（正类）
  * 蓝色区域：模型预测为类别 0（负类）
* 黑边点为训练样本，颜色代表真实类别
* 中间的边界线是 $\sigma(w^T x + b) = 0.5$，即模型分界面

---

### 📉 右图：训练过程中 Log Loss 的变化

* 显示了梯度下降过程中损失逐步下降的过程
* 初始较高，快速下降后逐渐趋于平稳，说明模型已经收敛

---

### 🔴 左图：逻辑回归决策边界（NumPy 版）

* 蓝色区域：模型预测为类别 0
* 红色区域：模型预测为类别 1
* 数据点是模型训练用的样本，颜色为真实类别
* 决策边界即 $\sigma(w^T x + b) = 0.5$，是模型分类的分界线

---

### 📉 右图：训练过程中损失函数的变化（Log Loss）

* 初始损失较高，随着训练逐渐下降，最后趋于平稳
* 说明模型收敛良好，预测效果逐步提升

---

# 🎓 第6节：可视化决策边界 + 总结回顾

---

## ✅ 6.1 可视化模型预测结果

### 🧪 模型结构回顾：

逻辑回归最终模型为：

$$
\hat{y} = \sigma(w^T x + b) = \frac{1}{1 + e^{-z}} \quad \text{其中} \ z = w^T x + b
$$

我们预测的结果是 $\hat{y} \in (0, 1)$，通常设置**分类阈值为 0.5**：

* $\hat{y} \geq 0.5$ → 预测为类别 1
* $\hat{y} < 0.5$ → 预测为类别 0

---

### 📊 可视化要点：

#### 1. 原始数据点

* 横轴：特征 1
* 纵轴：特征 2
* 每个点的颜色表示真实类别（红=1，蓝=0）

#### 2. 决策边界

* 满足 $\sigma(z) = 0.5$ ⇔ $z = 0$
* 即 $w_1 x_1 + w_2 x_2 + b = 0$，是一个**线性分界线**

在图上，这就是**红蓝区域交界的那条线**。

---

## ✅ 6.2 总结回顾：逻辑回归与线性回归的关系

| 项目   | 线性回归                  | 逻辑回归                                   |
| ---- | --------------------- | -------------------------------------- |
| 输出形式 | $\hat{y} = w^T x + b$ | $\hat{y} = \sigma(w^T x + b)$          |
| 输出范围 | $(-\infty, +\infty)$  | $(0, 1)$                               |
| 任务类型 | 回归（数值预测）              | 分类（二分类预测）                              |
| 损失函数 | 均方误差（MSE）             | 对数损失（Log Loss）                         |
| 决策边界 | 无明显分类线                | $\hat{y} = 0.5 \Rightarrow z = 0$ 是边界线 |

---

**下一节预告：**
> 🔍 Softmax回归与多分类的内容