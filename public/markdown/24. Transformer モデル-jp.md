---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->ğŸ“˜ ç¬¬24è¬›ï¼šTransformer ãƒ¢ãƒ‡ãƒ«ï¼ˆTransformer Modelï¼‰

---

## ä¸€ã€èƒŒæ™¯ã¨å‹•æ©Ÿ

### 1.1 RNNã‹ã‚‰Transformerã¸

* RNN / LSTMæ§‹é€ ã®æ¬ ç‚¹ï¼š

  * **è¨ˆç®—ãŒä¸¦åˆ—åŒ–ã§ããªã„**ï¼ˆç³»åˆ—ä¾å­˜ã®ãŸã‚ï¼‰
  * **é•·è·é›¢ä¾å­˜ã‚’æ‰ãˆã«ãã„**
  * **å­¦ç¿’ãŒé…ãã€æ‹¡å¼µãŒå›°é›£**

* 2017å¹´ã€GoogleãŒé©å‘½çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ææ¡ˆï¼š

  > **Transformer: Attention Is All You Need**

* æ ¸å¿ƒæ€æƒ³ï¼šå†å¸°æ§‹é€ ã‚’å®Œå…¨ã«æ’é™¤ã—ã€**Attentionæ©Ÿæ§‹**ã®ã¿ã«ä¾å­˜ã€‚

---

### 1.2 Transformerã®åŸºæœ¬ã‚³ãƒ³ã‚»ãƒ—ãƒˆ

* **Self-Attention**ã§ç³»åˆ—å†…éƒ¨ã®ä¾å­˜é–¢ä¿‚ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–
* **Multi-Head Attention**ã§ç•°ãªã‚‹æ„å‘³é–¢ä¿‚ã‚’åŒæ™‚å­¦ç¿’
* **Positional Encoding**ã§é †åºæƒ…å ±ã‚’æ³¨å…¥
* Encoderâ€“Decoderå±¤ã‚’ç©ã¿é‡ã­ã¦æ·±å±¤æ§‹é€ ã‚’æ§‹ç¯‰

---

## äºŒã€Transformerã®å…¨ä½“æ§‹é€ 

### 2.1 ãƒ¢ãƒ‡ãƒ«ã®æ§‹æˆ

Transformerã¯æ¬¡ã®2éƒ¨åˆ†ã§æ§‹æˆã•ã‚Œã‚‹ï¼š

1. **Encoderï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼‰**

   * å…¥åŠ›æ–‡ã‚’å—ã‘å–ã‚Šã€æ–‡è„ˆè¡¨ç¾ã‚’ç”Ÿæˆ
2. **Decoderï¼ˆãƒ‡ã‚³ãƒ¼ãƒ€ï¼‰**

   * ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã‚’ã‚‚ã¨ã«ã€å‡ºåŠ›ç³»åˆ—ã‚’é€æ¬¡ç”Ÿæˆ

---

![](https://s.ar8.top/img/picgo/20251126184516285.webp)

---

### 2.2 å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æ§‹æˆ

| ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«   | æ§‹æˆè¦ç´                                                              | ä¸»è¦ãƒ¡ã‚«ãƒ‹ã‚ºãƒ    |
| ------- | ---------------------------------------------------------------- | --------- |
| Encoder | Self-Attention + Feed Forward                                    | å…¥åŠ›ç³»åˆ—ã‚’ä¸¦åˆ—å‡¦ç† |
| Decoder | Masked Self-Attention + Encoderâ€“Decoder Attention + Feed Forward | å‡ºåŠ›ç³»åˆ—ã‚’ç”Ÿæˆ   |

---

### 2.3 ç›´æ„Ÿçš„ç†è§£

* **Encoder**ï¼šæ–‡å…¨ä½“ã‚’èª­ã¿å–ã‚Šã€æ„å‘³ã‚’ç†è§£
* **Decoder**ï¼šå„å˜èªç”Ÿæˆæ™‚ã«Attentionã§å…¥åŠ›ã‚’å‚ç…§
* å…¨ä½“ã¯æ™‚é–“é †åºã§ã¯ãªãã€**æ³¨æ„é‡ã¿è¡Œåˆ—**ã«åŸºã¥ã„ã¦å‹•ä½œã™ã‚‹

---

## ä¸‰ã€Self-Attentionï¼ˆè‡ªå·±æ³¨æ„æ©Ÿæ§‹ï¼‰

### 3.1 åŸºæœ¬æ¦‚å¿µ

> ç³»åˆ—å†…ã®å„è¦ç´ ãŒä»–ã®ã™ã¹ã¦ã®è¦ç´ ã‚’ã€Œè¦‹ã‚‹ã€ã“ã¨ã‚’å¯èƒ½ã«ã—ã€
> é–¢é€£åº¦ã«å¿œã˜ã¦è‡ªåˆ†ã®è¡¨ç¾ã‚’èª¿æ•´ã™ã‚‹ã€‚

---

### 3.2 å…¥åŠ›ã¨3ã¤ã®ãƒ™ã‚¯ãƒˆãƒ«

å„å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ« $x_i$ ã‹ã‚‰3ç¨®é¡ã®å°„å½±ã‚’ç”Ÿæˆï¼š

$$
Q_i = W_Q x_i, \quad K_i = W_K x_i, \quad V_i = W_V x_i
$$

* **Query (Q)**ï¼šå•ã„åˆã‚ã›ãƒ™ã‚¯ãƒˆãƒ«
* **Key (K)**ï¼šã‚­ãƒ¼ã¨ãªã‚‹ãƒ™ã‚¯ãƒˆãƒ«
* **Value (V)**ï¼šæƒ…å ±ãƒ™ã‚¯ãƒˆãƒ«

---

### 3.3 æ³¨æ„è¨ˆç®—ã®å¼

$$
\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{Q K^\top}{\sqrt{d_k}}\right)V
$$

èª¬æ˜ï¼š

1. $QK^\top$ï¼šå˜èªé–“ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
2. $\sqrt{d_k}$ï¼šã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å› å­ï¼ˆå‹¾é…ã®éå¤§é˜²æ­¢ï¼‰
3. Softmaxï¼šé‡ã¿ã®æ­£è¦åŒ–
4. é‡ã¿ä»˜ãå’Œã§æ–°ã—ã„è¡¨ç¾ã‚’å¾—ã‚‹

---

### 3.4 ç›´æ„Ÿçš„ç†è§£

* $Q$ï¼šç¾åœ¨ã®å˜èªãŒã€Œå°‹ã­ã‚‹è³ªå•ã€
* $K$ï¼šä»–ã®å˜èªãŒæŒã¤ã€Œæ‰‹ãŒã‹ã‚Šã€
* $V$ï¼šå®Ÿéš›ã«å–ã‚Šå‡ºã™ã€Œæƒ…å ±ã€
* æ³¨æ„æ©Ÿæ§‹ã«ã‚ˆã‚Šãƒ¢ãƒ‡ãƒ«ã¯ **ã€Œèª°ã‚’è¦‹ã‚‹ã¹ãã‹ã€** ã‚’å­¦ç¿’ã™ã‚‹

---

### 3.5 ä¾‹ï¼šæ–‡ä¸­ã®ä¾å­˜é–¢ä¿‚

æ–‡ï¼š

> "The animal didn't cross the street because it was too tired."

ãƒ¢ãƒ‡ãƒ«ã¯ä»¥ä¸‹ã‚’å­¦ç¿’ï¼š

* â€œitâ€ ã®AttentionãŒ â€œanimalâ€ ã«å¼·ãå‘ã
* â€œstreetâ€ ã§ã¯ãªã„
  â†’ ãƒ¢ãƒ‡ãƒ«ãŒ**æ„å‘³çš„å¯¾å¿œ**ã‚’ç†è§£ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã™

---

## å››ã€å¤šé ­æ³¨æ„æ©Ÿæ§‹ï¼ˆMulti-Head Attentionï¼‰

### 4.1 ãªãœè¤‡æ•°ã®ãƒ˜ãƒƒãƒ‰ãŒå¿…è¦ï¼Ÿ

* å˜ä¸€ã®ãƒ˜ãƒƒãƒ‰ã§ã¯ä¸€ç¨®é¡ã®é–¢ä¿‚ã—ã‹å­¦ç¿’ã§ããªã„
* è¤‡æ•°ãƒ˜ãƒƒãƒ‰ã«ã‚ˆã‚Šã€ç•°ãªã‚‹æ„å‘³çš„ä¾å­˜ã‚’ä¸¦åˆ—ã«å­¦ç¿’ã§ãã‚‹

---

### 4.2 æ•°å¼å®šç¾©

$$
\text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)
$$

$$
\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O
$$

* $h$ï¼šæ³¨æ„ãƒ˜ãƒƒãƒ‰ã®æ•°ï¼ˆä¾‹ï¼š8å€‹ï¼‰
* å„ãƒ˜ãƒƒãƒ‰ã¯ç‹¬ç«‹ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤
* é€£çµå¾Œã«ç·šå½¢å¤‰æ›ã‚’é©ç”¨

---

### 4.3 å¤šé ­ã®åŠ¹æœ

å„ãƒ˜ãƒƒãƒ‰ãŒç•°ãªã‚‹é–¢ä¿‚ã‚’å­¦ç¿’ï¼š

| ãƒ˜ãƒƒãƒ‰ç•ªå·  | æ³¨ç›®å¯¾è±¡    | å­¦ç¿’ã™ã‚‹é–¢ä¿‚ |
| ------ | ------- | ------ |
| Head 1 | å‹•è©ã¨ä¸»èª   | æ–‡æ³•æ§‹é€    |
| Head 2 | ä»£åè©ã¨å…ˆè¡Œè© | æŒ‡ç¤ºé–¢ä¿‚   |
| Head 3 | ä¿®é£¾èªã¨åè©  | æ„å‘³ä¿®é£¾   |
| Head 4 | æ–‡è„ˆã®æ™‚åˆ¶   | æ„å‘³çš„ä¸€è²«æ€§ |

â†’ å¤šé ­ = å¤šè¦–ç‚¹ç†è§£

---

## äº”ã€ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆPositional Encodingï¼‰

### 5.1 ãªãœå¿…è¦ï¼Ÿ

* Transformerã«ã¯å†å¸°æ§‹é€ ãŒãªãã€é †åºã‚’èªè­˜ã§ããªã„
* ãã®ãŸã‚ã€å„å˜èªã«ã€Œä½ç½®æƒ…å ±ã€ã‚’æ³¨å…¥ã™ã‚‹å¿…è¦ãŒã‚ã‚‹

---

### 5.2 æ­£å¼¦æ³¢ã«ã‚ˆã‚‹ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

å›ºå®šå¼ã§å„ä½ç½®ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆï¼š

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

* æ¬¡å…ƒã”ã¨ã«å‘¨æœŸãŒç•°ãªã‚‹
* ç›¸å¯¾ãƒ»çµ¶å¯¾ä½ç½®é–¢ä¿‚ã‚’å­¦ç¿’å¯èƒ½ã«ã™ã‚‹

---

## å…­ã€Transformerã®æ§‹é€ è©³ç´°

### 6.1 Encoderå±¤ã®æ§‹æˆ

å„å±¤ã¯ä»¥ä¸‹ã‚’å«ã‚€ï¼š

1. Multi-Head Self-Attention
2. Add & LayerNorm
3. Feed Forwardï¼ˆ2å±¤å…¨çµåˆï¼‰

å‡ºåŠ›ï¼š
$$
H_l = \text{LayerNorm}(H_{l-1} + \text{SelfAttention}(H_{l-1}))
$$
$$
H_l = \text{LayerNorm}(H_l + \text{FFN}(H_l))
$$

---

### 6.2 Decoderå±¤ã®æ§‹æˆ

3ã¤ã®ã‚µãƒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å«ã‚€ï¼š

1. Masked Self-Attentionï¼ˆæœªæ¥ã®å˜èªã‚’è¦‹ãªã„ï¼‰
2. Encoderâ€“Decoder Attention
3. Feed Forward + æ®‹å·®æ¥ç¶š + LayerNorm

---

### 6.3 æ®‹å·®æ¥ç¶šã¨å±¤æ­£è¦åŒ–

* **æ®‹å·®æ¥ç¶šï¼ˆResidual Connectionï¼‰**ï¼š
  å‹¾é…æ¶ˆå¤±ã‚’é˜²ãã€åæŸã‚’åŠ é€Ÿ
  $$
  x' = \text{LayerNorm}(x + \text{SubLayer}(x))
  $$

* **Layer Normalization**ï¼š
  å„ã‚µãƒ³ãƒ—ãƒ«å†…ã®ç‰¹å¾´æ¬¡å…ƒã‚’æ­£è¦åŒ–ã—ã€å­¦ç¿’ã‚’å®‰å®šåŒ–

---

## ä¸ƒã€å®Ÿè·µï¼šTransformerã«ã‚ˆã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’

### 7.1 å®Ÿé¨“ç›®çš„

* ã‚¿ã‚¹ã‚¯ï¼šæ–‡ä¸­ã®æ¬¡å˜èªã‚’äºˆæ¸¬
  â†’ **è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLanguage Modelingï¼‰**
* ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼šå°è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆä¾‹ï¼šWikiText2ï¼‰
* ãƒ¢ãƒ‡ãƒ«ï¼šç°¡æ˜“ç‰ˆTransformer Encoderæ§‹é€ 

---

### 7.2 ãƒ¢ãƒ‡ãƒ«æ¦‚è¦

1. **å…¥åŠ›å±¤**ï¼šå˜èªåŸ‹ã‚è¾¼ã¿ + ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
2. **Encoderå±¤**ï¼šMulti-Head Self-Attention + FFNã®ç©å±¤
3. **å‡ºåŠ›å±¤**ï¼šSoftmaxã§æ¬¡å˜èªã‚’äºˆæ¸¬

---

### 7.3 PyTorchå®Ÿè£…ä¾‹

```python
import torch
import torch.nn as nn
from torch.nn import TransformerEncoder, TransformerEncoderLayer

d_model = 256
nhead = 8
num_layers = 4
dim_feedforward = 512

encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward)
transformer = TransformerEncoder(encoder_layer, num_layers=num_layers)

x = torch.rand(20, 32, d_model)  # (seq_len, batch, dim)
out = transformer(x)
print(out.shape)  # [20, 32, 256]
````

---

### 7.4 è¨€èªãƒ¢ãƒ‡ãƒ«ã®æå¤±é–¢æ•°

$$
L = -\sum_t \log P(w_t | w_{<t})
$$

æ¬¡å˜èªäºˆæ¸¬ã®ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã€‚
Transformerã¯å…¨ä½ç½®ã‚’åŒæ™‚ã«è¨ˆç®—ã§ãã€ä¸¦åˆ—å­¦ç¿’ãŒå¯èƒ½ã€‚

---

### 7.5 å®Ÿè·µã®ãƒã‚¤ãƒ³ãƒˆ

* ãƒ‡ãƒ¼ã‚¿ã¯æ™‚ç³»åˆ—é †ã«æ•´åˆ—
* Maskã§æœªæ¥å˜èªã‚’é®æ–­
* Dropoutã§éå­¦ç¿’ã‚’é˜²æ­¢
* å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã¯ã€ŒWarmup + Cosine Decayã€ãŒæœ‰åŠ¹

---

### 7.6 æ€§èƒ½å‘ä¸Šã®å·¥å¤«

* å±¤æ•°ãƒ»ãƒ˜ãƒƒãƒ‰æ•°ã®å¢—åŠ  â†’ è¤‡é›‘ãªæ„å‘³ã‚’æ‰ãˆã‚‹
* å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹ã§å­¦ç¿’ â†’ è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¿’å¾—
* äº‹å‰å­¦ç¿’ï¼ˆGPT / BERTãªã©ï¼‰ â†’ æ±åŒ–æ€§èƒ½å‘ä¸Š

---

## å…«ã€Transformerã®åˆ©ç‚¹ã¨å½±éŸ¿

### 8.1 é•·æ‰€

* å®Œå…¨ãªä¸¦åˆ—å­¦ç¿’ãŒå¯èƒ½
* é•·è·é›¢ä¾å­˜ã®æ•æ‰ã«å¼·ã„
* å·¨å¤§ãƒ¢ãƒ‡ãƒ«ã¸ã®æ‹¡å¼µæ€§
* æ³¨æ„å¯è¦–åŒ–ã«ã‚ˆã‚‹é«˜ã„è§£é‡ˆæ€§

---

### 8.2 åˆ¶é™

* è¨ˆç®—é‡ãŒ $O(n^2)$ ã®ãŸã‚é•·ç³»åˆ—ã«ä¸åˆ©
* æ™‚ç³»åˆ—çš„æ„Ÿè¦šã®æ¬ å¦‚
* ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒè†¨å¤§ã§å­¦ç¿’ã‚³ã‚¹ãƒˆé«˜

---

### 8.3 æ”¹è‰¯ãƒ¢ãƒ‡ãƒ«ã®æ–¹å‘æ€§

| ãƒ¢ãƒ‡ãƒ«                  | æ”¹è‰¯ç‚¹              |
| -------------------- | ---------------- |
| Transformer-XL       | å†å¸°çš„ãƒ¡ãƒ¢ãƒªã‚’å°å…¥ã—é•·æ–‡å¯¾å¿œ   |
| Longformer / BigBird | ç–ãªAttentionã§è¨ˆç®—å‰Šæ¸› |
| Performer            | ã‚«ãƒ¼ãƒãƒ«è¿‘ä¼¼ã§ç·šå½¢åŒ–       |
| GPTç³»åˆ—                | Decoderã®ã¿æ®‹ã—ç”Ÿæˆç‰¹åŒ–  |

---

## ä¹ã€Transformerã®æ„ç¾©

* **NLPã¨AIã®ç ”ç©¶ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’æ ¹åº•ã‹ã‚‰å¤‰é©**
* ã€Œå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«æ™‚ä»£ã€ã®å¹•é–‹ã‘ï¼š

  * BERTã€GPTã€T5ã€ViTã€Whisper ãªã©ã¯ã™ã¹ã¦Transformerãƒ™ãƒ¼ã‚¹
* ãƒ†ã‚­ã‚¹ãƒˆãƒ»ç”»åƒãƒ»éŸ³å£°ã‚’æ¨ªæ–­ã™ã‚‹çµ±ä¸€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¸

---

## åã€ã¾ã¨ã‚

| æ¦‚å¿µ                   | èª¬æ˜              |
| -------------------- | --------------- |
| Self-Attention       | ç³»åˆ—å†…è¦ç´ é–“ã®é–¢ä¿‚ã‚’å­¦ç¿’    |
| Multi-Head Attention | è¤‡æ•°ã®ä¾å­˜é–¢ä¿‚ã‚’ä¸¦åˆ—å­¦ç¿’    |
| Positional Encoding  | ä½ç½®æƒ…å ±ã§é †åºã‚’èªè­˜      |
| Feed Forward         | éç·šå½¢å¤‰æ›å±¤          |
| Encoderâ€“Decoder      | ä¸¦åˆ—å…¥åŠ›ã¨è‡ªå›å¸°å‡ºåŠ›æ§‹é€     |
| æ ¸å¿ƒã®å¼·ã¿                | ä¸¦åˆ—ãƒ»é«˜åŠ¹ç‡ãƒ»é•·è·é›¢ä¾å­˜ã®æ•æ‰ |

