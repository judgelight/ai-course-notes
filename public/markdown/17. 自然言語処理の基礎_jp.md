---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第17講：自然言語処理の基礎

---

# 自然言語処理（NLP）の基礎紹介

## 一、NLP の定義と目標

* **定義**：自然言語処理（Natural Language Processing，NLP）は人工知能と計算言語学の重要な分野であり、コンピュータが人間の言語を理解・生成・処理できるようにすることを目的とする。
* **目標**：

  1. **理解**：機械が言語を「読解」する（例：感情分析、QA システム）。
  2. **生成**：機械が言語を「書く／話す」（例：機械翻訳、文章生成）。
  3. **対話**：人と自然に対話できるようにする（例：音声アシスタント、チャットボット）。

---

## 二、NLP の課題

* **言語の多様性**：言語ごとに大きな違い（中国語 vs 英語 vs 日本語）。
* **曖昧性**：同じ単語・文が複数の意味を持つ可能性。

  * 例：中国語「苹果」→ 果物 or 会社。
* **文脈依存**：語の意味は文脈によって変化。

  * 例：「彼は相手を打ち負かした」「彼はバスケットボールをした」。
* **世界知識**：言語はしばしば常識を含意する。

  * 例：「彼は水を3杯飲んだが、まだ喉が渇いている」→「水はすぐに渇きを癒せない」という知識が必要。

---

## 三、NLP の主要タスク

1. **分類系タスク**

   * テキスト分類（スパム検出、感情分析）
   * トピックモデリング：大量のテキストから潜在トピックを自動発見
2. **理解系タスク**

   * 固有表現認識（NER）：人名・地名・組織名・日付などを抽出
   * 依存構文解析：文中の文法的関係を分析
   * 読解（QA）：与えられた文章に基づいて質問に回答

---

3. **生成系タスク**

   * 機械翻訳
   * テキスト要約
   * 対話システム
4. **検索系タスク**

   * 情報検索（検索エンジン）
   * 知識抽出：非構造テキストから構造化知識（エンティティ・関係・イベント）を抽出

---

## 四、方法論の進化：統計的方法 vs 深層学習的方法

### **統計的方法**

* **ルールベース**：人手でルールを作成（初期 NLP システム、例：ELIZA）
* **確率モデル**：n-gram、隠れマルコフモデル（HMM）、最大エントロピーモデル
* 長所：解釈性が高い
* 短所：複雑な意味処理は困難、性能に限界

---

### **深層学習的方法**

* 2010 年以降に急速発展
* コア思想：ニューラルネットがデータから特徴を自動学習
* 代表的手法：

  * Word2Vec（2013） → 単語ベクトル
  * Seq2Seq（2014） → ニューラル機械翻訳
  * Transformer（2017） → BERT、GPT、大規模言語モデル
* 長所：大幅な性能向上、特に大規模データで強力
* 短所：解釈性が低い、計算資源とデータ依存が大きい

---

## 五、よく使われるツールとフレームワーク

1. **従来の NLP ツール**

   * **NLTK**：学術的な研究・教育で広く利用、機能が豊富。
   * **spaCy**：産業向け NLP ライブラリ、高速で API が使いやすい。
2. **現代の深層学習ツール**

   * **PyTorch / TensorFlow**：深層学習フレームワーク
   * **Hugging Face Transformers**：事前学習モデルライブラリBERT、GPT など
   * **datasets ライブラリ**：オープン NLP データセット集
3. **総合プラットフォーム**

   * Google Colab / Kaggle Notebook：迅速な実験環境
   * Jupyter Notebook：対話型コード＋ドキュメント

---

## 六、授業ミニ演習

1. **ツールのインストール**

```bash
pip install nltk spacy torch transformers
```

2. **NLTK で簡単な分かち書き**
```python
import nltk
nltk.download("punkt")
nltk.download("punkt_tab")
text = "I love studying Natural Language Processing."
print(nltk.word_tokenize(text))
```

3. **spaCy で品詞タグ付け**
```python
import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("Apple is looking at buying a startup in Japan")
for token in doc:
    print(token.text, token.pos_)
```

---

# テキスト前処理

## 一、なぜテキスト前処理が必要か？

* **テキストは非構造データ**であり、そのままではモデルに入力できない。
* 前処理の目的：**自然言語 → 数値表現**に変換。
* 前処理手法の違いはモデル性能に直接影響。

---

## 二、テキストクリーニングと正規化

1. **大文字小文字の統一**

   * "Apple" と "apple" を統一 → "apple"
   * モデルが大文字小文字を別単語と誤認しないようにする。

2. **特殊記号の除去**

   * 句読点、絵文字、HTML タグなど
   * タスク次第では保持する場合あり（例：感情分析で絵文字を利用）。

---

3. **ストップワードの除去**

   * 情報量の少ない高頻度語（*is, the, of* など）
   * NLTK が一般的なストップワード表を提供。

4. **ステミングとレンマ化**

   * ステミング：語幹抽出（running → runn）
   * レンマ化：意味に基づく語形統一（running → run）
   * **違い**：レンマ化の方が正確だが遅い。

---

## 三、分かち書き（Tokenization）

* **英語**：スペース＋句読点で分割。
* **中国語／日本語**：空白がないため、分かち書き器が必要。

  * 中国語：jieba、HanLP
  * 日本語：MeCab、Sudachi

### 例

```python
import nltk
nltk.download("punkt")
nltk.download("punkt_tab")
text = "I love studying Natural Language Processing."
print(nltk.word_tokenize(text))
# 出力: ['I', 'love', 'studying', 'Natural', 'Language', 'Processing', '.']
```

---

## 四、N-gram モデル

* **定義**：連続する n 個の単語を 1 単位とみなす。
* **例**：

  * 文: "I love NLP"
  * Unigram: \[I], \[love], \[NLP]
  * Bigram: \[I love], \[love NLP]
  * Trigram: \[I love NLP]
* **用途**：言語モデル、特徴量抽出

---

## 五、ベクトル化表現

1. **One-hot Encoding**

   * 各単語を高次元疎ベクトルで表現
   * 短所：語義の類似性を表現できない、次元が高すぎる

2. **Bag of Words (BoW)**

   * 単語の出現頻度を統計、語順を無視
   * 例：

     * 文書1："I love NLP" → {I:1, love:1, NLP:1}
     * 文書2："I love AI" → {I:1, love:1, AI:1}

---

3. **TF-IDF (Term Frequency – Inverse Document Frequency)**

   * 高頻度語の情報量不足を解決
   * 数式：

     $$
     \text{TF-IDF}(t, d) = \text{TF}(t, d) \times \log \frac{N}{df(t)}
     $$

     * TF：語の文書内出現頻度
     * df：語を含む文書数
     * N：全体の文書数
   * **直感的理解**：ある単語が一部の文書に多く出現し、全体ではまれ → 区別に有効

---

## 六、コード実践：IMDB 映画レビュー感情分析（前処理部分）

```python
import nltk
import string
from sklearn.feature_extraction.text import TfidfVectorizer

# 示例文本
corpus = [
    "I love this movie, it was amazing!",
    "I hate this film, it was terrible..."
]

# 1. 分词 + 去标点
def preprocess(text):
    tokens = nltk.word_tokenize(text.lower())  # 小写化 + 分词
    tokens = [t for t in tokens if t not in string.punctuation]  # 去标点
    return tokens

print(preprocess("I love this movie, it was amazing!"))

# 2. TF-IDF 向量化
vectorizer = TfidfVectorizer(tokenizer=preprocess)
X = vectorizer.fit_transform(corpus)

print("词表:", vectorizer.get_feature_names_out())
print("TF-IDF 矩阵:\n", X.toarray())
```

出力例：

```
語彙: ['amazing' 'film' 'hate' 'it' 'love' 'movie' 'terrible' 'this' 'was']
TF-IDF 行列:
[[0.46 0.    0.    0.33 0.46 0.46 0.    0.33 0.33]
 [0.    0.46 0.46 0.33 0.    0.    0.46 0.33 0.33]]
```

---

# 単語表現法

## 一、なぜ単語表現が必要か？

* コンピュータは自然言語テキストを直接理解できない → 数値化が必要
* **単語表現法**の目的：単語をベクトル空間にマッピングし、計算可能にする
* 理想の単語表現：**意味的類似性**を反映（例：*king* と *queen* は *king* と *apple* より近い）

---

## 二、One-hot Encoding（ワンホット符号化）

### 原理

* 語彙（Vocabulary）サイズ \$V\$ の場合、各単語を \$V\$ 次元ベクトルで表す
* ベクトル中 1 つだけが 1、それ以外は 0

### 例

語彙：\["I", "love", "NLP"]

* "I" → \[1, 0, 0]
* "love" → \[0, 1, 0]
* "NLP" → \[0, 0, 1]

---

### 長所と短所

* ✅ シンプルで直感的
* ❌ 高次元疎ベクトル、ストレージを浪費
* ❌ 意味類似性を表現できない（"dog" と "cat" は完全に独立）

```python
from sklearn.preprocessing import OneHotEncoder
import numpy as np

words = np.array(["I", "love", "NLP"]).reshape(-1, 1)
encoder = OneHotEncoder()
onehot = encoder.fit_transform(words)

print("词表:", encoder.categories_)
print("One-hot 编码:\n", onehot)
```

---

## 三、Bag of Words（BoW, 単語袋モデル）

### 原理

* 単語順を無視し、文書を「単語の集合」として扱う
* 各単語の出現回数をカウント

### 例

文書1："I love NLP"
文書2："I love AI"
語彙：\["I", "love", "NLP", "AI"]

* 文書1 → \[1, 1, 1, 0]
* 文書2 → \[1, 1, 0, 1]

---

### 長所と短所

* ✅ シンプルで小規模タスク（例：スパム検出）に有効
* ❌ 語順を無視
* ❌ 意味を失う（"good" と "bad" も高頻度なら区別できない）

```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    "I love NLP",
    "I love AI"
]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)

print("词表:", vectorizer.get_feature_names_out())
print("BoW 矩阵:\n", X.toarray())
```

---

## 四、TF-IDF（単語頻度-逆文書頻度）

### 原理

* **TF**：単語の文書内頻度
* **IDF**：単語の希少性
* 数式：

  $$
  \text{TF-IDF}(t, d) = \text{TF}(t, d) \times \log \frac{N}{df(t)}
  $$

---

### 例

コーパス：

* 文書1："I love NLP"
* 文書2："I love AI"
* 文書3："I love love love AI"

結果：

* "love" → 全文書に出現 → IDF 低 → 重み減少
* "NLP" → 1 文書のみ → IDF 高 → 重み増加

---

### 長所と短所

* ✅ BoW より重要語を強調
* ❌ 語順や文脈を無視
* ❌ 長文や未知語には弱い

```python
from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
    "I love NLP",
    "I love AI",
    "I love love love AI"
]

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)

print("词表:", vectorizer.get_feature_names_out())
print("TF-IDF 矩阵:\n", X.toarray())
```

---

## 五、三手法の比較

| 方法      | 特徴 | 長所          | 短所             |
| ------- | -- | ----------- | -------------- |
| One-hot | 離散 | シンプル直感的     | 意味情報なし、高次元疎    |
| BoW     | 統計 | 実装簡単、小タスク向き | 語順無視、意味喪失      |
| TF-IDF  | 加重 | キーワードを強調    | 意味情報なし、同義語対応不可 |

