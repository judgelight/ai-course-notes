---

marp: true
theme: default
paginate: true
math: katex
-----------

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

## <!-- fit -->📘 第6講：ニューラルネットワーク基礎（多層パーセプトロン，MLP）

---

# 🎓 第1節：導入 — なぜニューラルネットワークが必要なのか？

---

## ✅ 1.1 これまでに学んだモデルの復習

前の講で学んだモデル：

* **ロジスティック回帰**：二値分類，線形分離可能な問題に適用
* **Softmax回帰**：多クラス分類，出力方式を拡張

共通点：

$$
\hat{y} = \text{Softmax}(W x + b)
$$

これは**線形モデル**にほかならない。

---

## ❗ 1.2 問題提起：データが**線形分離不可能**なら？

### 🧠 XOR問題：

| $x_1$ | $x_2$ | クラス |
| -------- | -------- | --- |
| 0        | 0        | 0   |
| 0        | 1        | 1   |
| 1        | 0        | 1   |
| 1        | 1        | 0   |

* どの直線でもクラス0とクラス1を完全に分けられない
* つまり、**線形モデルでは解けない！**

📌 Softmaxやロジスティック回帰はXORでは性能天井に達する

---

## 🔍 1.3 何が必要か？

次のようなルールを学ばせたい：

* “$x_1 \neq x_2$ のとき1を予測”
* “それ以外は0を予測”

このような非線形パターンを捉えるには，
“さらに複雑な特徴を中間で計算”する仕組みが必要。

✅ それがニューラルネットワークの設計思想：

> **「隠れ層」と「非線形変換」で，より高度・複雑な特徴を抽出し，非線形問題を解決する。**

---

# 🎓 第2節：ニューラルネットワークの構成要素（パーセプトロン → MLP）

---

## ✅ 2.1 パーセプトロンから始めよう

> パーセプトロン（Perceptron）はニューラルネットワークの最初の形

構造は非常にシンプル：

$$
\hat{y} = \text{sign}(w^T x + b)
$$

* $x \in \mathbb{R}^d$：入力ベクトル
* $w \in \mathbb{R}^d$：重みベクトル
* $b \in \mathbb{R}$：バイアス
* 出力は+1または-1（二値分類）

📌 本質は**線形分類器**で，超平面による分類しかできない

---

### 📉 パーセプトロンの限界

* 線形分離可能な問題しか解けない
* XORなど複雑な境界は不可能

![bg right:50% 100%](https://s.ar8.top/img/picgo/20250528164758270.webp)

---

## ✅ 2.2 多層パーセプトロン（MLP）とは？

> MLP = 多層ニューラルネットワーク，隠れ層を一つ以上含む

### 最も基本的な二層構造（単一隠れ層）：

```
入力 → 隠れ層 → 出力
```

数式で表すと：

$$
h = f^{(1)}(W^{(1)} x + b^{(1)})
$$

$$
\hat{y} = f^{(2)}(W^{(2)} h + b^{(2)})
$$

* 第一層で「中間特徴」を抽出
* 第二層で分類／予測

✅ 非線形活性化関数（ReLU, sigmoid など）を入れれば非線形問題を解ける

---

### ✏️ ネットワーク構造図：

```
入力層：   x1 — x2 — x3
             \   |   /
隠れ層：     h1 — h2 — h3 — h4 (ReLU)
               \  |  /
出力層：         y1 — y2 — y3 (Softmax)
```

---

## ✅ 2.3 各層の役割まとめ

| 層   | 役割              | 出力サイズ                          |
| --- | --------------- | ------------------------------ |
| 入力層 | 特徴を受け取る         | $x \in \mathbb{R}^d$         |
| 隠れ層 | 非線形中間特徴を抽出      | $h \in \mathbb{R}^{n_h}$    |
| 出力層 | 分類結果（クラス確率等）を出力 | $\hat{y} \in \mathbb{R}^{K}$ |

📌 多層化は「関数の関数」を重ねる，いわゆる**関数合成**：

$$
\hat{y} = f_3(f_2(f_1(x)))
$$

---

## ✅ 2.4 多層構造の表現力

* **隠れ層1層 + ReLU** → 任意の連続関数を近似可能（普遍近似定理）
* 隠れ層を増やすほど表現力アップ
* ニューラルネットは**パラメータ化された関数近似器**

> ✅ Softmax回帰はMLPの特別な場合（隠れ層なし）

---

## 🧠 小結

* パーセプトロンは線形問題のみ扱う
* MLPは隠れ層＋活性化関数で非線形問題を解決
* 基本構造：**入力 → 線形変換 → 活性化 → 出力**
* MLPを理解すれば，あらゆる深層学習の基礎が掴める

---

# 🎓 第3節：順伝播（Forward Propagation）

---

## ✅ 3.1 順伝播とは？

> 順伝播はニューラルネットの「予測過程」

入力から出力まで各層を通じて計算を行う。

各層の計算式は：

$$
\text{出力} = \text{活性化関数}\bigl(W \cdot \text{入力} + b\bigr)
$$

---

## ✅ 3.2 二層MLPの順伝播

定義：

* 入力特徴：$x \in \mathbb{R}^d$
* 第一層重み：$W^{\[1]} \in \mathbb{R}^{h \times d}$，バイアス $b^{\[1]} \in \mathbb{R}^h$
* 第二層重み：$W^{\[2]} \in \mathbb{R}^{K \times h}$，バイアス $b^{\[2]} \in \mathbb{R}^K$

---

### 💡 計算の流れ：

#### ① 隠れ層の線形変換：

$$
z^{[1]} = W^{[1]} x + b^{[1]}
$$

#### ② 隠れ層の活性化（非線形変換）：

$$
a^{[1]} = \text{ReLU}(z^{[1]})
$$

#### ③ 出力層の線形変換：

$$
z^{[2]} = W^{[2]} a^{[1]} + b^{[2]}
$$

#### ④ 出力層の活性化（分類ではSoftmax）：

$$
\hat{y} = \text{Softmax}(z^{[2]})
$$

---

## ✅ 3.3 なぜ活性化関数が必要か？

活性化関数なしでは，全層が線形：

$$
f(x) = W_3 W_2 W_1 x + ...
$$

🔁 実質は一層の線形変換に過ぎず，意味がない。

だから必ず各層後に**非線形関数**を挿入する。

✅ こうして非線形境界を表現できる。

---

## ✅ 3.4 順伝播のイメージ図：

```
x1 — x2 — x3
 \   |   /     ← 線形: W[1]x + b[1]
  h1 h2 h3 h4   ← ReLU 活性化
   \  |  /
    y1 y2 y3    ← Softmax(W[2]h + b[2])
```

* 各層：内積 → バイアス加算 → 活性化
* 出力はクラスごとの確率

![bg right:50% 100%](https://s.ar8.top/img/picgo/20250528183604562.webp)

---

## ✅ 3.5 バッチ処理のサポート

実際の学習ではバッチ処理を行う：

* 一度にn個のサンプル入力：$X \in \mathbb{R}^{n \times d}$

順伝播式は：

$$
Z^{[1]} = XW^{[1]T} + b^{[1]}
,\quad
A^{[1]} = \text{ReLU}(Z^{[1]})
$$

あとは出力層まで同様に計算。

✅ 学習を高速化し，収束を安定させる。

---

## 🧠 小結

| ステップ | 内容                                     |
| ---- | -------------------------------------- |
| 入力層  | 元の特徴ベクトル$x$を受け取る                     |
| 隠れ層  | 線形変換 → 非線形活性化                          |
| 出力層  | 最終的なクラス確率（Softmax）                     |
| 全体構造 | 関数合成：$\hat{y} = f_3(f_2(f_1(x)))$ |

---

# 🎓 第4節：活性化関数の役割（ReLU / Sigmoid / Tanh）

---

## ✅ 4.1 活性化関数の必要性

> 活性化関数なしでは，全層が線形変換のみで，ネットワーク全体が線形モデルになる。
> 活性化関数があって初めて複雑な**非線形関係**を学習できる。

📌 **非線形活性化関数**こそがニューラルネットの「学習力」源泉！

---

## ✅ 4.2 主要な3種類の活性化関数

| 関数      | 数学的定義                       | 用途        |
| ------- | --------------------------- | --------- |
| ReLU    | $f(x)=\max(0,x)$          | 隠れ層のデフォルト |
| Sigmoid | $f(x)=\frac{1}{1+e^{-x}}$ | 確率（0–1）出力 |
| Tanh    | $f(x)=\tanh(x)$           | 出力が-1–1   |

---

### 🟢 ReLU（Rectified Linear Unit）

* 長所：

  * 計算が高速，導関数は0または1
  * 正の領域で飽和しにくく，**収束が速い**
* 短所：

  * 負の領域で常に0，ニューロンが「死ぬ」可能性

🔁 導関数：

* $x>0$ → 1
* $x\le0$ → 0

![bg right:40% 100%](https://s.ar8.top/img/picgo/20250528183836082.webp)

---

### 🔵 Sigmoid

* 出力範囲は(0,1)，二値分類の出力層で用いられる
* 短所：

  * $x\ll0$や$x\gg0$で勾配消失しやすい

![bg right:40% 100%](https://s.ar8.top/img/picgo/20250528183957187.webp)

---

### 🟠 Tanh（双曲正接）

* 出力範囲は(-1,1)，Sigmoidの中央対称版
* 対称性に優れ，隠れ層で有効な場合あり
* 短所：勾配消失の課題は残る

![bg right:40% 100%](https://s.ar8.top/img/picgo/20250528184043680.webp)

---

## ✅ 4.3 活性化関数の選択ガイド

| シーン       | 推奨活性化関数                   |
| --------- | ------------------------- |
| 隠れ層       | ReLU（デフォルト），またはLeaky ReLU |
| 出力層（二クラス） | Sigmoid + BCE             |
| 出力層（多クラス） | Softmax + 交差エントロピー        |
| 出力が-1–1必須 | Tanh                      |

---

## ✅ 小結

* 活性化関数は非線形性を導入し，ネットワークに学習力を与える
* 選択次第で収束速度や性能に影響あり
* ReLUは現代のデフォルト，Sigmoid/Tanhは用途に応じて使い分け

---

# 🎓 第5節：逆伝播と訓練フロー（Backpropagation）

---

## ✅ 5.1 ニューラルネットワークはどうやって学習する？

知っていること：

* 順伝播で予測$\hat{y}$を得る
* 損失$L(\hat{y},y)$で予測誤差を測る

> ❓ 次にモデルはどう改善する？

答えは**逆伝播 + 勾配降下法**

---

## ✅ 5.2 逆伝播の概要

### 🧠 アルゴリズム：

1. 出力層から損失関数のパラメータへの
   勾配を計算
2. チェーンルールで各層へ勾配を逆伝播
3. 各パラメータを勾配に沿って更新

---

### 🧩 二層MLPの例：

$$
x \xrightarrow{W^{[1]},b^{[1]}} h \xrightarrow{\text{ReLU}} a \xrightarrow{W^{[2]},b^{[2]}} \hat{y} \xrightarrow{\text{Softmax}} \text{loss}
$$

* 出力層で$\nabla_{W^{[2]}}$を計算
* チェーンルールで隠れ層まで遡り$\nabla_{W^{[1]}}$を得る

📌 各層で「局所誤差×局所導関数」を計算(∂loss/∂z × ∂z/∂W)

---

### 🧰 勾配降下によるパラメータ更新：

$$

W^{\[l]} := W^{\[l]} - \alpha \cdot \nabla W^{\[l]}
,\quad
b^{\[l]} := b^{\[l]} - \alpha \cdot \nabla b^{\[l]}

$$

* $\alpha$：学習率
* $\nabla W^{[l]}$：損失の勾配

---

## 🔁 5.3 訓練フローまとめ

1. 順伝播 → 予測$\hat{y}$
2. 損失計算$L(\hat{y},y)$
3. 逆伝播で全パラメータの勾配取得
4. 勾配降下で重み更新
5. 反復して損失収束を待つ

---

## 📉 5.4 可視化例

![](https://s.ar8.top/img/picgo/20250528185253083.webp)

---

## ✅ 小結

* 逆伝播は学習の要
* チェーンルール×多層微分の積
* 各層のパラメータは総損失への寄与度で更新
* 学習率と組み合わせて最適化を行う

---

# 🎓 第6節：NumPyで実装する二層MLP分類モデル

![](https://s.ar8.top/img/picgo/20250528191929878.webp)

---

## 📍 左：MLPの決定境界（2→10→3構造）

* 入力2次元，隠れ層10ユニット(ReLU)，出力3クラス(Softmax)
* 背景色は予測クラス領域
* 点は訓練データ，色は真のラベル
* 複雑な非線形境界を学習できたことが分かる

---

## 📉 右：訓練中の交差エントロピー損失推移

* 損失が継続的に減少
* 約300〜500エポックで収束

---

## ✅ 本節の実装ポイントまとめ

| ステップ   | 内容                     |
| -------- | ---------------------- |
| 順伝播    | 入力 → ReLU → Softmax     |
| 損失関数  | 多クラス交差エントロピー   |
| 逆伝播    | 手動で勾配を計算し逐次更新 |
| 可視化    | 決定境界 + 損失曲線        |

---

# 🎓 第7節：まとめと次回予告

---

### 📦 モデル構造：
* 入力層 → **隠れ層（ReLU）** → 出力層（Softmax, 多クラス）

---

### 📈 学んだポイント：

| モジュール   | 内容                                       |
| ---------- | ---------------------------------------- |
| ネットワーク構成 | 多層構造，隠れ層，活性化関数                 |
| 順伝播        | 線形 → 活性化 → 線形 → Softmax               |
| 活性化関数     | ReLU/Sigmoid/Tanh の使い分けと特徴            |
| 損失関数      | 交差エントロピー（多クラス向け）             |
| 逆伝播        | チェーンルールでの誤差逆伝播                  |
| パラメータ更新   | 学習率 + 勾配降下法                         |
| 可視化        | 決定境界と損失下降トレンド                   |

---

> 多層パーセプトロン(MLP)は**現代深層学習の基礎**
* MLPを理解すれば，CNN/RNN/Transformerも拡張として理解可能

---

**次回予告：**
> 🔍 データの潜在構造を探る — 非教師あり学習入門

