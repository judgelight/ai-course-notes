---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第18讲：词向量嵌入

---

## 一、为什么需要词向量嵌入？

* **One-hot / TF-IDF 的局限**

  * 向量高维稀疏（维度 = 词表大小）
  * 词之间没有语义关系（“dog” 与 “cat” 的向量完全独立）

* **词向量嵌入的核心思想**

  * 把每个词表示成 **低维稠密向量**（如 100 维、300 维）。
  * 向量空间能捕捉语义关系：

    $$
    \text{king} - \text{man} + \text{woman} \approx \text{queen}
    $$

---

![bg 70%](https://s.ar8.top/img/picgo/20250924170742451.webp)

---

## 二、分布式假设（Distributional Hypothesis）

* **核心观点**：词义由上下文决定。
* “You shall know a word by the company it keeps.”
* 例：

  * “dog” 常出现在 “bark, pet, animal” 的上下文
  * “cat” 常出现在 “meow, pet, animal” 的上下文
    → 说明它们语义相近。

---

## 三、Word2Vec（2013，Google）

* Word2Vec 是 Google 在 2013 年提出的一种方法。
* 目标：把**每一个词**变成一个**低维的数字向量**（比如 100 维、300 维），让“意思相近的词”在向量空间里也靠得近。
* 举例：

  * "猫" 和 "狗" → 向量相似度很高
  * "猫" 和 "汽车" → 向量相似度很低
* Word2Vec 的关键思想：**词的意义来自它的上下文**。

  * 比如“猫”经常和“喵、毛、宠物”一起出现，“狗”经常和“汪、宠物”一起出现 → 它们的向量也应该接近。

---

## 四. Word2Vec 的原理

* Word2Vec 不是靠人写规则，而是靠一个小神经网络来“预测词和上下文的关系”。
* 它的训练方式：

  * 给定一个句子，把它切成单词序列。
  * 用一个**滑动窗口**看某个词的上下文。
  * 训练网络：

    * **CBOW**：根据上下文预测中心词。
    * **Skip-gram**：根据中心词预测上下文。
* 经过大量语料训练，模型会自动学出好的词向量。

---

原句：There is an apple on the table

处理后：["there","is","an","apple","on","the","table"]

---

1. **Input layer（输入层）**

   * 输入的是 **one-hot 向量**，长度等于词表大小 $V$。
   * 比如词表有 10,000 个词，如果输入单词是 *apple*，那么对应的位置是 1，其余都是 0。

---
2. **Hidden layer（隐藏层）**

   * 权重矩阵 $W_{V \times N}$

     * $V$ = 词表大小
     * $N$ = 词向量维度（比如 100、300）
   * 计算：

     $$
     h = X \cdot W
     $$

     因为 $X$ 是 one-hot，结果就是直接取出 $W$ 的一行 → 这就是该单词的 **词向量**。

---

3. **Output layer（输出层）**

   * 权重矩阵 $W'_{N \times V}$
   * 计算：

     $$
     y = h \cdot W'
     $$
   * 输出长度是 $V$，相当于对所有词打分，表示“在这个上下文里，下一个/中心的词是它的概率”。
   * 再用 Softmax 把这些分数归一化成概率分布。

---

![bg 70%](https://s.ar8.top/img/picgo/20250924171846139.webp)

---

### 两个权重矩阵的意义
* **$W$（输入层权重，$V \times N$）**

  * 每一行就是一个单词的词向量。
  * 训练完成后，这个矩阵通常被保存下来作为 **Embedding 表**。

* **$W'$（输出层权重，$N \times V$）**

  * 训练时必须有，用来把隐藏层映射回词表做预测。
  * 训练结束后，它其实也包含语义信息，但在大多数应用里我们只取 $W$ 作为词向量。
  * 有些论文或工具（比如 gensim）会选择 **把 $W$ 和 $W'^T$ 平均**，作为最终词向量。

---

## 五. CBOW 模式（Continuous Bag of Words）

* **任务**：给定上下文，预测中心词。
* 例子：

  * 句子："There is an apple on the table"
  * 预测目标：中心词 "on"
  * 输入上下文：\["apple", "the"]
  * 模型学会：在这种上下文里，最可能的中心词是"on"。
* 特点：

  * 训练快，适合高频词。
  * 但对低频词学习效果一般。

---

  * 输入层是上下文词的 one-hot 向量们 → 通过 $W$ 取出多个词向量 → 求平均 → 得到隐藏层表示 $h$。
  * 输出层预测中心词的概率分布。
  * **目标**：让真实中心词的概率最高。

![bg right:50% 90%](https://s.ar8.top/img/picgo/20250924174405533.webp)

---

## 六. Skip-gram 模式

* **任务**：给定中心词，预测上下文。
* 例子：

  * 句子："There is an apple on the table"
  * 中心词："on"
  * 预测目标：\["apple", "the"]
* 特点：

  * 更适合学习**低频词**的表示。
  * 实际应用里更常用。

---

  * 输入层是一个中心词 → 通过 $W$ 得到它的向量 $h$。
  * 输出层预测多个上下文词。
  * **目标**：让真实上下文词的概率最高。

![bg right:50% 90%](https://s.ar8.top/img/picgo/20250924174737514.webp)

---

## 七. 什么是 GloVe？

* GloVe 是斯坦福大学在 2014 年提出的另一种词向量方法，全称 **Global Vectors for Word Representation**。
* Word2Vec 用的是“预测”思路（根据上下文来预测词）。
* GloVe 用的是“统计”思路（统计词和词在整个语料库中出现的次数）。
* 它把“全局的共现信息”利用起来，让词向量更稳定。

---

## 八. GloVe 的原理

* GloVe 从一个大语料库里，数一数：

  * 每个词 **和其他词一起出现的频率**。
  * 比如：“冰 (ice)”经常和“冷 (cold)”一起出现；“蒸汽 (steam)”经常和“热 (hot)”一起出现。
* 它会构建一个“共现矩阵”，表示词与词之间出现的强度。
* 然后，它让词向量去拟合这些统计规律：

  * 如果两个词经常一起出现，它们的向量距离就要近。
  * 如果两个词几乎不一起出现，它们的向量距离就要远。
* 最终结果：

  * "king - man + woman ≈ queen"

---

公式：

$$
\text{Loss} = \sum_{i,j} f(X_{ij}) (w_i^T \tilde{w_j} + b_i + \tilde{b_j} - \log X_{ij})^2
$$

* $X\_{ij}$：词 $i$ 与词 $j$ 的共现次数
* $w\_i$：词向量

---

✅ **简要对比**

* **Word2Vec**：通过预测上下文来学 → 更偏“局部”关系，训练快。
* **GloVe**：通过全局统计来学 → 融合“整体”语料分布，更稳健。

---


### 代码示例 Word2Vec

```python
# pip install gensim
from gensim.models import Word2Vec

# ===== 1) 准备语料（英文示例：已分词） =====
# 英文（已分词的句子列表）
sentences = [
    "there is an apple on the table",
    "an orange is on the desk",
    "i love natural language processing",
    "word embeddings capture semantics",
]
sentences = [s.split() for s in sentences]


# ===== 2) 训练 Word2Vec：CBOW（sg=0） =====
model = Word2Vec(
    sentences,
    vector_size=100,   # 词向量维度
    window=5,          # 上下文窗口
    min_count=1,       # 出现次数 <1 的词将被忽略
    sg=0,              # 0 = CBOW，1 = Skip-gram
    negative=10,       # 负采样个数
    epochs=20,
    workers=4
)

# ===== 3) 使用：取向量 / 相似词 / 类比 =====
wv = model.wv  # KeyedVectors
print("vector(dim=100) of 'apple':", wv["apple"][:8])

print("most similar to 'apple':", wv.most_similar("apple", topn=5))

# 类比：king - man + woman ≈ queen（若语料太小可能不准）
# print(wv.most_similar(positive=["king","woman"], negative=["man"], topn=3))

# ===== 4) 句子向量（平均词向量）示例 =====
import numpy as np
def sent_vec(tokens, wv):
    vecs = [wv[w] for w in tokens if w in wv]
    return np.mean(vecs, axis=0) if vecs else np.zeros(wv.vector_size)

v1 = sent_vec("there is an apple".split(), wv)
v2 = sent_vec("an orange is on the desk".split(), wv)
cos = float(np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2)+1e-9))
print("cosine(sent1, sent2) =", cos)

# ===== 5) 保存 / 加载 =====
wv.save_word2vec_format("cbow_vectors.txt")  # 文本格式，兼容很多工具
# from gensim.models import KeyedVectors
# wv2 = KeyedVectors.load_word2vec_format("cbow_vectors.txt")

```

---

### 使用预训练 GloVe

```python
import gensim.downloader as api

# 加载预训练 GloVe (50维)
glove = api.load("glove-wiki-gigaword-50")

print("向量维度:", glove["cat"].shape)
print("cat vs dog 相似度:", glove.similarity("cat", "dog"))
print("king - man + woman ≈", glove.most_similar(positive=["king", "woman"], negative=["man"], topn=1))
```

