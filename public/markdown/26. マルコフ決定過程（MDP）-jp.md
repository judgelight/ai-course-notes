---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第26講：マルコフ決定過程（MDP）

---

## 一言でわかるマルコフ決定過程（MDP）

**マルコフ決定過程（Markov Decision Process、MDP）**とは、  
「**不確実な環境の中で、継続的に意思決定を行い、長期的な報酬を最大化する**」ための数学モデルです。

つまり、次のような問いに答えます。

> 「今、何をすれば、将来を含めた結果がいちばん良くなるのか？」

---

## 直感的な日常例

例えば、あなたが**RPG（ロールプレイングゲーム）**を遊んでいるとします。

- 現在のあなたには **HP・所持金・レベル** がある
- 毎ターン、次の行動を選べる：
  - モンスターを倒す
  - 装備を買う
  - 休んで回復する
- 選択によって：
  - その場で利益／損失が発生する
  - 同時に **次の状態（状況）** が変わる

---

さらにゲームにはランダム性があります。

- 戦闘が必ず成功するとは限らない
- ドロップアイテムは確率で決まる

あなたの目的は「この1ターンで最大得点」ではなく、

> **できるだけ長く生き残り、強くなり、クリアすること**

つまり「**いまの状態で最適な行動を選び、長期的な利益を最大化する**」という問題で、これが本質的に **MDP** です。

---

## MDP は何で構成されるか？

完全なマルコフ決定過程は **5つの要素** で表されます。

### 1️⃣ 状態（State）

**状態 = 現在の状況を表す完全な記述**

例：
- ゲーム：HP、レベル、所持金
- ロボット：位置、速度、バッテリー残量
- 推薦システム：ユーザー行動履歴、現在のページ

重要な点：

> **状態は「十分に完全」である必要がある。現在の状態だけを見れば、次に何が起こり得るかが決まる**

---

### 2️⃣ 行動（Action）

**行動 = ある状態で選べる選択肢**

例：
- 左に進む／右に進む
- 買う／買わない
- 戦う／逃げる

状態によって、選べる行動が変わることもあります。

---

### 3️⃣ 状態遷移（Transition）

**状態遷移 = 行動を選んだ後に環境がどう変化するか**

特徴：
- 多くの場合 **確率的**
- 同じ行動でも結果が同じとは限らない

例：
- 戦闘成功 70%、失敗 30%
- 明日の天気は確率で変わる

これが「不確実性」です。

---

### 4️⃣ 報酬（Reward）

**報酬 = 各ステップで得られる即時のフィードバック**

例：
- スコア
- お金
- 正負の値（罰）

例：
- モンスター撃破 +10
- ダメージ −5
- 壁に衝突 −100

重要：

**報酬は短期的だが、私たちが最大化したいのは長期的な累積報酬**です。

---

### 5️⃣ 方策（Policy）

**方策 = 「この状態なら何をするべきか」を決めるルール**

例：
- HP が 30% 未満なら回復
- 資金が十分なら投資
- 天候が悪い日は外出しない

方策は本質的に写像です：

> **状態 → 行動**

（確率的に行動を選ぶ方策もあります）

---

## 「マルコフ」とは何か？

「マルコフ」とは重要な仮定を指します。

> **未来は「現在の状態」にのみ依存し、それより前の履歴には依存しない**

言い換えると：

- 現在の状態がわかっていれば
- そこに至るまでの過去の経路は、未来の予測に不要

これは「過去が存在しない」という意味ではなく、

> **現在の状態が、未来に必要な情報をすべて含んでいる**

という意味です。

---

## MDP が解きたい本質問題

MDP の目的は

- 「次の1手で報酬を最大化する」

ではなく、

> **長期的な累積報酬が最大となる方策を見つけること**

そのため、時には

- 一時的に損をしてでも
- 将来の大きな利益を狙う

という判断が最適になります。

---

## 直感に反するが重要な点

例：

- 今すぐ +10 点を取れる
- あるいは今は 0 点だが、将来毎回 +5 点を取れるようになる

短視眼的には +10 を選びがちですが、**最適方策**は後者を選ぶかもしれません。

MDP はこの「**短期 vs 長期**」のトレードオフを扱う枠組みです。

---

## なぜ MDP が重要か？

現実世界の多くの課題が MDP として表現できます。

- 強化学習
- 自動運転
- ロボット制御
- ゲームAI
- 資源スケジューリング
- 推薦システム
- 投資意思決定

---

次の条件を満たす問題は：

- 状態がある
- 選択がある
- 不確実性がある
- 長期目標を最適化したい

**ほぼ確実に MDP としてモデル化できます。**

---

## まとめ

> マルコフ決定過程は、不確実な環境においてエージェントが現在の状態に基づいて行動を選び、長期的な累積報酬を最大化することで最適な意思決定を行うための数学的枠組みである。

---

## 0. まず MDP の状況を固定する

次を考える：

- **状態** $(s)$：現在の状況
- **行動** $(a)$：選べる意思決定
- **遷移**：$(a)$ を行うと次の状態 $(s')$ へ（確率的な場合あり）
- **報酬** $(r)$：各ステップで得る即時フィードバック
- **方策** $(\pi(a|s))$：状態 $(s)$ で行動 $(a)$ を選ぶ規則（確率的でもよい）

目的は：**長期利益を最大化する方策 $(\pi)$ を見つけること。**

---

## 1. 「即時報酬」から「長期リターン」へ

多くの問題の本質は：

**目先の報酬よりも、長期的な累積効果が重要**だという点です。

そこで「リターン（Return）」を定義します：

> ある時刻から先に得る報酬の合計（通常は割引する）。

---

一般的な書き方：

$$
[
G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + ...
]
$$

ここで $(\gamma)$ は **割引率（0〜1）**：

- $(\gamma)$ が小さい：短期を重視
- $(\gamma)$ が大きい：長期を重視

直感的には：

**遠い未来の報酬ほど影響が小さくなる**（数学的に発散を防ぐ意味もあります）。

---

## 2. 「価値」とは何か：状態／行動の長期的な良さを測る

リターン $(G_t)$ を定義したら次の問いが自然に出ます：

> 「今この状態にいることは、長期的に見てどれだけ良いのか？」

そこで **状態価値関数**を定義します。

### 2.1 状態価値 $(V^\pi(s))$

$$
[
V^\pi(s) = \mathbb{E}_\pi[ G_t \mid s_t=s ]
]
$$

意味：方策 $(\pi)$ のもとで、状態 $(s)$ にいるときの将来リターンの**期待値**。

- 値が大きいほど、その方策の下で「見込みがある状態」。

---

### 2.2 行動価値 $(Q^\pi(s,a))$

状態の良さだけでなく、

> 「状態 $(s)$ で、どの行動を選ぶべきか？」

も知りたいので、**行動価値関数**を定義します。

$$
[
Q^\pi(s,a) = \mathbb{E}_\pi[ G_t \mid s_t=s, a_t=a ]
]
$$

意味：方策 $(\pi)$ のもとで、状態 $(s)$ で行動 $(a)$ を実行した場合の将来リターン期待値。

直感：

- $(V^\pi(s))$：「その状態は全体としてどうか」
- $(Q^\pi(s,a))$：「その状態でその行動を取るとどうか」

---

## 3. 価値はどう計算するか：ベルマン方程式（再帰関係）

価値関数が計算可能なのは、次の重要な再帰関係があるからです：

> **現在の価値 = 即時報酬 +（割引した）将来価値の期待値**

これが **ベルマン期待方程式（Bellman Expectation Equation）**です。

---

### 3.1 状態価値に対して

$$
[
V^\pi(s)=\sum_a \pi(a|s)\sum_{s'} P(s'|s,a)\bigl[R(s,a,s')+\gamma V^\pi(s')\bigr]
]
$$

意味：

- 状態 $(s)$ で方策 $(\pi)$ に従い行動 $(a)$ を選ぶ
- 環境は確率 $P(s'|s,a)$ で次状態 $(s')$ へ遷移する
- 即時報酬 $R(s,a,s')$ を得る
- その後の価値 $V^\pi(s')$ を割引 $\gamma$ して足し合わせる

つまり「**次の価値から現在の価値を逆算できる**」関係です。

---

## 4. 価値からより良い方策へ：貪欲（グリーディ）改良

もし $(Q^\pi(s,a))$ が分かっていれば、各状態で選ぶべき行動は明確です：

$$
[
\pi'(s)=\arg\max_a Q^\pi(s,a)
]
$$

意味：

**各状態で、長期リターンが最大となる行動を選ぶ。**

これを **方策改良（Policy Improvement）**と呼びます。

重要な結論：

> $(Q^\pi)$ に基づく貪欲改良で得られる新方策 $(\pi')$ は、元の方策 $(\pi)$ より悪くならず、通常は良くなる。

---

## 5. 2ステップを繰り返す：方策評価 + 方策改良 → 最適へ近づく

よくある手順は次の反復です。

### 5.1 方策反復（Policy Iteration）

繰り返し：

1. **方策評価**：与えられた $(\pi)$ について $(V^\pi)$ または $(Q^\pi)$ を求める  
2. **方策改良**：$(\arg\max_a Q^\pi(s,a))$ によりより良い方策 $(\pi')$ を得る

方策が変わらなくなるまで続けます。

---

直感的には：

- 評価：この「戦い方」は長期的にどれくらい得か？
- 改良：評価結果に基づいて戦い方をアップデート
- さらに評価、さらに改良……

最終的に最適方策 $(\pi^*)$ に収束します。

---

## 6. 最適価値と最適方策：最終ゴール

最適状態価値：

$$
[
V^*(s) = \max_\pi V^\pi(s)
]
$$

最適行動価値：

$$
[
Q^*(s,a) = \max_\pi Q^\pi(s,a)
]
$$

最適方策は：

$$
[
\pi^*(s)=\arg\max_a Q^*(s,a)
]
$$

すなわち：

**各状態で、最適行動価値が最大の行動を選ぶ方策**です。

---

## 7. 現実の大問題：遷移確率と報酬モデルが分からないことが多い

ここまでの式は美しいですが、前提として

- $(P(s'|s,a))$
- $(R(s,a,s'))$

を知っている必要があります。

しかし現実には未知であることが多い。ではどうするか？

---

強化学習でよく言う2つの路線があります：

- **モデルベース（Model-based）**：まず $(P,R)$ を学習し、その上で計画（Planning）する
- **モデルフリー（Model-free）**：モデルは学ばず、相互作用の経験から直接 $(V/Q)$ を推定する

代表的なモデルフリー手法として、**サンプルに基づく価値更新**（TD、Q-learning、SARSA など）があります。

---

## 8. 流れのまとめ

MDP では各ステップで即時報酬を得ますが、重要なのは将来報酬の累積であるリターンです。  
状態や行動が長期的に「どれだけ良いか」を測るために価値関数 $(V)$ と $(Q)$ を定義します。  
価値はベルマン再帰関係を満たすため計算（または推定）できます。  
次に「各状態で $(Q)$ が最大の行動を選ぶ」ことで方策を改良します。  
この「方策評価 → 方策改良」を繰り返すことで、最適方策へ段階的に近づいていきます。
