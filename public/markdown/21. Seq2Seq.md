---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第21讲：Seq2Seq（编码器-解码器框架）

---

## 一、什么是 Seq2Seq？

### 1.1 提出背景

* 传统的 RNN / LSTM 只能做“序列 → 单个输出”的任务。
  * 例如：情感分类、语音识别标签。
* 但许多任务是 **输入和输出都是序列**：
  * **机器翻译**：英文句子 → 中文句子  
  * **对话生成**：问题 → 回答  
  * **语音识别**：声音特征序列 → 文字序列
* 为了解决“输入序列长度 ≠ 输出序列长度”的问题，  
  2014 年 Google 提出了 **Seq2Seq (Sequence to Sequence)** 模型。

---

### 1.2 核心思想

* 使用 **两个 RNN（或 LSTM/GRU）网络**：
  * **Encoder（编码器）**：读取输入序列，压缩成一个固定长度的“语义向量”。
  * **Decoder（解码器）**：根据该向量，逐步生成目标序列。
* 整个模型可以看作：
  $$
  X = (x_1, x_2, \dots, x_T) \Rightarrow Y = (y_1, y_2, \dots, y_{T'})
  $$
  输入和输出序列长度可以不同。

---

## 二、Seq2Seq 的结构与工作原理

### 2.1 模型结构

1. **Encoder**（编码器）  
   * 读取输入序列 $x_1, x_2, \dots, x_T$  
   * 每一步输出隐藏状态 $h_t$  
   * 最后一个隐藏状态 $h_T$ 被视为整个序列的“语义总结”。

2. **Decoder**（解码器）  
   * 初始输入为编码器输出的语义向量 $h_T$  
   * 然后逐步预测输出词：
     $$
     y_t = f(y_{t-1}, s_{t-1}, h_T)
     $$

---

### 2.2 Encoder 的计算流程

在每个时间步：

$$
h_t = f(h_{t-1}, x_t)
$$

对于 LSTM，可具体写作：

$$
h_t, C_t = \text{LSTM}(x_t, (h_{t-1}, C_{t-1}))
$$

最终输出：

* 最后一个隐藏状态 $h_T$
* 或整个状态序列 $(h_1, h_2, ..., h_T)$

---

### 2.3 Decoder 的计算流程

解码器也是一个 RNN / LSTM，它的任务是：

* 给定前一时刻输出的单词 $y_{t-1}$（或其 embedding）
* 和上一步隐藏状态 $s_{t-1}$
* 输出当前预测 $y_t$

$$
s_t = f(s_{t-1}, y_{t-1}, c)
$$
$$
P(y_t | y_{<t}, X) = \text{Softmax}(W \cdot s_t + b)
$$

其中 $c$ 是来自 Encoder 的“上下文向量”（context vector）。

---
![bg 90%](https://s.ar8.top/img/picgo/20251015194825321.webp)

---

## 三、训练与推理过程

### 3.1 训练阶段（Teacher Forcing）

* **输入：**
  * 源语言句子 $(x_1, ..., x_T)$
  * 目标语言句子 $(y_1, ..., y_{T'})$
* **目标：**
  * 最大化每个时间步预测正确下一个词的概率：
    $$
    \max \sum_t \log P(y_t | y_{<t}, X)
    $$
* **Teacher Forcing 技术：**
  * 在训练时，解码器的输入是“真实的上一个词”（而不是模型自己生成的）。

---

### 3.2 推理阶段（Inference）

* 没有“标准答案”可以提供给解码器。
* 因此采用 **自回归生成**（autoregressive generation）：
  1. 用 Encoder 得到语义向量 $c$
  2. 第一步输入 `<SOS>`（开始符号）
  3. 解码器生成第一个词 $y_1$
  4. 把 $y_1$ 再输入解码器生成 $y_2$
  5. 循环直到输出 `<EOS>`（结束符号）

---

### 3.3 Beam Search（束搜索）

* 为了提高生成质量，不直接选择每步概率最大的词（贪心）。
* 而是同时保留多个候选序列（称为“束宽”）。
* 最后选择总概率最高的序列作为输出。

---

## 四、数学建模

### 4.1 整体概率建模

Seq2Seq 模型学习条件概率：

$$
P(Y|X) = \prod_{t=1}^{T'} P(y_t | y_{<t}, X)
$$

每个条件概率通过解码器的 Softmax 计算。

---

### 4.2 损失函数

* 常用 **交叉熵损失（Cross Entropy Loss）**：
  $$
  L = - \sum_{t=1}^{T'} \log P(y_t^{(true)} | y_{<t}, X)
  $$
* 本质上是在最小化“生成正确句子的负对数似然”。

---

## 五、Seq2Seq 的改进与演化

### 5.1 问题一：信息压缩瓶颈

* 编码器把整句压缩成一个固定长度向量 $c$。
* 对长句来说，这个向量往往不能完整保留信息。
* 结果：解码器“忘掉”前面的上下文。

---

### 5.2 Attention 机制的引入（2015）

* 由 Bahdanau 等人提出的 **Attention Seq2Seq**。
* 改进点：
  * 解码时不再只依赖最后一个隐藏状态。
  * 而是对 Encoder 的所有隐藏状态加权求和：
    $$
    c_t = \sum_i \alpha_{t,i} h_i
    $$
  * 权重 $\alpha_{t,i}$ 由注意力函数计算：
    $$
    \alpha_{t,i} = \text{Softmax}(s_t^\top W_a h_i)
    $$

* 意义：模型在生成每个词时“关注”输入句子的不同部分。

---

### 5.3 双向 Encoder

* 编码器可采用 **双向 LSTM（Bi-LSTM）**：
  * 一条从左到右编码（顺序信息）
  * 一条从右到左编码（反向信息）
* 两个方向的隐藏状态拼接：
  $$
  h_i = [\overrightarrow{h_i}; \overleftarrow{h_i}]
  $$

---

### 5.4 Copy & Coverage 机制（进一步改进）

* **Copy 机制**：允许模型直接“复制”输入中的词到输出（解决未知词问题）。
* **Coverage 机制**：跟踪哪些输入已经被关注过，避免重复生成。

---

## 六、Seq2Seq 的应用场景

### 6.1 机器翻译（Neural Machine Translation）

* Seq2Seq 最初的诞生场景。
* Encoder 读取源语言句子（如英文），Decoder 生成目标语言句子（如中文）。
* 示例：
  * 输入："I love you"
  * 输出："我爱你"

---

### 6.2 文本摘要（Text Summarization）

* 输入：长文档
* 输出：简短摘要
* 模型学习“提炼主要信息”。
* 注意力机制非常关键。

---

### 6.3 对话系统（Chatbot）

* 输入：用户问题或上一轮对话
* 输出：回复句子
* Encoder–Decoder 是早期聊天机器人的核心结构。

---

### 6.4 语音识别 / 语音到文本

* 输入：语音特征帧序列 (MFCC)
* 输出：文本字符序列
* 常结合 CTC 或 Attention，用 Seq2Seq 框架实现端到端识别。

---

## 七、Seq2Seq 与 Transformer 的关系

### 7.1 Transformer 的出现（2017）

* Transformer 本质上是 **Seq2Seq 的一种形式**：
  * Encoder–Decoder 结构依然存在；
  * 只是把 RNN/LSTM 替换成 **Self-Attention**。
* 优点：
  * 并行化计算
  * 更强的长距离依赖建模能力
  * 易于扩展为大型语言模型（GPT、BERT）

---

### 7.2 对比总结

| 项目 | RNN/LSTM Seq2Seq | Transformer |
|------|------------------|--------------|
| 底层结构 | 递归（顺序） | 注意力（并行） |
| 训练速度 | 慢 | 快 |
| 长距离依赖 | 弱 | 强 |
| 可解释性 | 一定可视化（门机制） | 可视化注意力权重 |
| 应用时代 | 2014~2017 主流 | 2018 以后主导 |

---

## 八、Seq2Seq 的意义与影响

* **统一了解决“输入输出皆为序列”的问题**。
* 为 **机器翻译、摘要、语音识别、对话生成** 奠定基础。
* 为 **Attention 机制与 Transformer** 的诞生铺路。
* 至今仍在轻量化任务与教学中被广泛使用。

---

## 九、总结

| 模块 | 功能 | 关键思想 |
|------|------|-----------|
| Encoder | 编码输入序列 | 将语义压缩为上下文向量 |
| Decoder | 解码输出序列 | 逐步生成目标句子 |
| Attention | 改善长句信息丢失 | 动态关注不同部分 |
| Seq2Seq 意义 | 输入输出序列建模框架 | 奠定了现代生成模型的基础 |

---

## 十、实践方向建议

* **实现一个最小可运行的 Seq2Seq 模型（PyTorch）**
  * 数据：英中句对或数字翻译任务（如 "one two" → "一 二"）
  * 模型：Encoder + Decoder（均用 LSTM）
  * 损失函数：交叉熵
  * 训练策略：Teacher Forcing + Beam Search
* 进一步可加入：
  * Attention 机制
  * 双向编码器
  * 词嵌入（Word2Vec / GloVe）

