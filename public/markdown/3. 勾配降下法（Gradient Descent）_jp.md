---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

## <!-- fit -->📘 第3講：勾配降下法（Gradient Descent）

---

## ✅ 1. シナリオ導入：『下山』から始める

### 🧠 導入質問：

> 「あなたは山の頂上に立っていて、天候が悪くて周りが見えません。どうすれば最短で山のふもとにたどり着けますか？」

---

### 🎯 比喩の導入：

- あなたは**複雑な地形関数**の上にいて、各点に“高さ”があります。  
- 地図はありませんが、“どちらがより急か”を足で感じ取れます。  
- そこで次の戦略を取ります：  
  - **現在地の傾き（導関数）を感じ取る**  
  - **最も急な下り方向へ少し歩く**  
  - **これを繰り返す**

---

### 📌 比喩の対応：

| 登山の課題     | 機械学習の課題               |
|--------------|------------------------------|
| 山の高さ       | 損失関数の値                 |
| 現在の地点     | モデルパラメータ（例：$w,b$）|
| 下山する       | 損失を減らす                 |
| 傾きの方向     | 勾配                         |
| 一歩踏み出す   | パラメータの更新             |
| 下山の成功     | 最適解（最小損失）の発見     |

![bg right:45% 100%](https://s.ar8.top/img/picgo/20250429225534445.webp)

---

### 🔑 小結：

> 「これが**勾配降下法**の基本アイデア：**損失関数の勾配に沿って“降り続け”、最適解を探す。**」

---

## ✅ 2. “勾配”とは何か？

---

### ✨ 2.1 一次元空間で（導数）

- 微分で学ぶ導数：  
  $$f'(x) = \frac{d}{dx} f(x)$$  
  - ある点での“傾き”を表す。  
- 勾配降下法での“勾配”は導数そのもの：  
  - 例：$f(x) = (x-3)^2$ の場合  
    $$f'(x) = 2(x - 3)$$  
    現在の $x$ が大きいほど傾きも大きい。  
- すべきことは **負の傾き方向に沿って $x$ を更新し、最低点に近づく** こと。

---

### 🧭 一次元勾配降下の流れ：

- 初期値 $x_0 = 0$  
- 学習率 $\alpha = 0.1$  
- 更新式：  
  $$x_{t+1} = x_t - \alpha \cdot f'(x_t)$$

| t | $x_t$ | $f'(x_t)$ | $x_{t+1}$ |
|---|-------|-----------|-----------|
| 0 | 0     | -6        | 0.6       |
| 1 | 0.6   | -4.8      | 1.08      |
| 2 | 1.08  | -3.84     | 1.464     |
| … | …     | …         | …         |

✅ 最終的に $x=3$（最小値点）に収束

---

### 📐 2.2 多次元空間で（ベクトル勾配）

関数が：

$$
f(w, b) = \frac{1}{n} \sum_{i=1}^{n} (y_i - (w x_i + b))^2
$$

- $w, b$ は最適化すべき“位置”  
- 勾配は**ベクトル**で、各成分が“その方向の最急上昇方向”を示す  

多次元更新式：

$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \alpha \cdot \nabla_{\boldsymbol{\theta}} f(\boldsymbol{\theta}_t)
$$

ここで $\boldsymbol{\theta}=[w,b]$

---

### 📌 勾配降下の直感的理解

> 勾配降下は「目を閉じて斜面を谷底へ下る」プロセス。  
> - 勾配が進むべき方向（急な坂）を教えてくれる  
> - 学習率が一歩の大きさを決める  
> - 繰り返すことで徐々に最低点へ近づく  

![50% 90%](https://s.ar8.top/img/picgo/20250430000057416.webp)

---

### ✅ 小結：

> 「勾配は進むべき方向を示すコンパス、勾配降下はそのコンパスに従い谷底（最小損失）へ向かう道のり。」

---

# ✅ 3. 勾配降下の仕組み

### 🧭 3.1 勾配降下のフローチャート（5ステップ）

以下の擬似コード／図解で全体像を示す：

```
1. パラメータ（例：w,b）を初期化
2. 以下を収束するまで繰り返す：
   a. 現在のパラメータで予測値 y_hat を計算
   b. 損失関数 L(w,b) を計算
   c. パラメータに関する勾配 ∇L を求める
   d. 勾配の逆方向に沿ってパラメータを更新
```

---

### 📉 3.2 数学的更新ルール（線形回帰を例に）

#### モデル仮定：

$$
\hat{y}_i = w x_i + b
$$

#### 損失関数（平均二乗誤差）：

$$
L(w,b)
= \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2
= \frac{1}{n}\sum_{i=1}^n (y_i - w x_i - b)^2
$$

---

#### 偏導：

$$
\frac{\partial L}{\partial w}
= -\frac{2}{n}\sum_i x_i (y_i - \hat{y}_i)
$$

$$
\frac{\partial L}{\partial b}
= -\frac{2}{n}\sum_i (y_i - \hat{y}_i)
$$

---

#### 更新式：

$$
w := w - \alpha \cdot \frac{\partial L}{\partial w},
\quad
b := b - \alpha \cdot \frac{\partial L}{\partial b}
$$

- $\alpha$：学習率  
- $n$：サンプル数  

---

### 📌 3.3 ハイパーパラメータの役割

#### 🔧 学習率 $\alpha$

- 小さすぎると：収束が遅い  
- 大きすぎると：振動または発散する可能性  

✅ ベストプラクティス：**適切に設定し、動的調整や減衰を検討**

---

#### 🔁 イテレーション数（Epoch）

- 1 Epoch = 全サンプルで1回パラメータ更新  
- 通常、数十～数千回繰り返す  
  - 収束判定（損失の変化が小さい）  
  - 最大イテレーション回数到達  

---

### 🧠 3.4 勾配降下の直感的まとめ

> 勾配降下は「目を閉じて斜面を谷底へ下る」ようなもの：  
> - 勾配が方向を示す  
> - 学習率が一歩の大きさを制御  
> - 繰り返しで最低点に近づく  

---

# ✅ 4. 線形回帰への勾配降下適用例（手計算1回分）

一元線形回帰モデル：

$$
\hat{y}_i = w x_i + b
$$

初期設定：

- $w=0,\; b=0$  
- サンプル $(x=2, y=10)$  
- 学習率 $\alpha=0.1$

予測値の計算：

$$
\hat{y} = 0\cdot2 + 0 = 0
$$

---

誤差の計算：

$$
e = y - \hat{y} = 10 - 0 = 10
$$

勾配の計算：

$$
\frac{\partial L}{\partial w}
= -2x \cdot e
= -2\cdot2\cdot10 = -40
\quad\Rightarrow\quad
w := 0 - 0.1\cdot(-40) = 4.0
$$

$$
\frac{\partial L}{\partial b}
= -2\cdot e = -20
\quad\Rightarrow\quad
b := 0 - 0.1\cdot(-20) = 2.0
$$

✅ 1回の更新後、$w=4,\; b=2$ に近づいた！

---

## 📌 小結

| 内容         | 要点                                        |
|-------------|---------------------------------------------|
| 損失関数     | $(y - \hat{y})^2$ を最小化                  |
| 勾配         | パラメータに対する誤差の導数                |
| 更新ルール   | $\theta := \theta - \alpha\cdot\nabla_\theta$ |
| メリット     | 行列の逆行列不要。大規模データに適合         |

---

# ✅ 第5部：NumPyで線形回帰の勾配降下を実装

## 🧩 シミュレーションデータ（訓練データ）

面積＋階数 → 価格 の簡単な線形データセットを使用

---

## 📌 まとめ

| 項目       | 説明                                       |
|-----------|--------------------------------------------|
| 勾配計算   | 誤差の逆伝播で最も重要                      |
| 重み更新   | 勾配の逆方向×学習率で調整                  |
| 収束判定   | 損失の安定度合いで確認                      |
| 解釈可能性 | 各重みが物理的に意味を持つ                  |

---

**次回予告：**  
> 🔍 ロジスティック回帰
