---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

## <!-- fit -->📘 第1講：線形回帰
---

## 一．線形回帰とは何か
### 🔍 1. 実際の問題から出発

> あなたはデータ分析担当者として不動産会社で働いています。上司から新築住宅の販売価格を予測するように依頼されました。手元にはいくつかの過去データがあります：

![bg right:35% 80%](https://s.ar8.top/img/picgo/20250423171045665.webp)

上司はこう尋ねます：

>「100平方メートルの新築物件があるんだけど、おおよそいくらで売れるか教えてくれる？」

---

### 💡 2. 線形回帰（Linear Regression）とは

線形回帰は上記のような問題を解くための手法です：

> 🔑 **目的**：  
> データ点の集合に「最も適した直線」を当てはめ、新しい入力に対して値を予測できるようにする。

この直線の数式は「線形回帰方程式」と呼ばれます：

$$
\hat{y} = w x + b
$$

ここで：

- $\hat{y}$：予測される出力（例：房价）
- $x$：入力変数（例：面積）
- $w$：傾き。入力が1増えると出力がどれだけ増えるかを示す
- $b$：切片。$x=0$のときの$y$の初期値

---

### 📉 3. 図による理解

- 横軸は面積 $x$、縦軸は価格 $y$
- 各点は実際の販売データを表す
- 赤い「予測直線」を見つけ、できるだけ全ての点に近づける

> ✨ この直線は「全ての点を通る」わけではなく、「全体の傾向に最も合う」もの  
> 📌 言い換えれば：**誤差が最小となる直線**を選ぶ

![bg right:40% 100%](https://s.ar8.top/img/picgo/20250423165653565.webp)

---

### 🧠 4. “回帰”の意味

「回帰」は「戻る」を意味するわけではなく、統計学上の用語で、

> 既存のデータから**連続値を予測**すること

代表的な回帰問題：

- 房价予測（面積 → 価格）
- 給与予測（勤続年数・学歴 → 給与）
- 気温予測（時間・湿度 → 翌日の気温）

---

## 二．単回帰分析（一元線形回帰）

以下のデータがあります：

| 面積（㎡） | 価格（万元） |
|------------|--------------|
| 50         | 200          |
| 60         | 240          |
| 70         | 280          |
| 80         | 320          |
| 90         | 360          |

面積 $x$ から価格 $y$ を予測する直線を見つけたい。

---

### 🧮 1. モデルの仮定

房价と面積は線形関係にあると仮定：

$$
\hat{y} = w x + b
$$

---

### 📉 2. 「最良のフィット」とは？

直線を好き勝手に引くのではなく、**誤差が最小**となる最適な直線を探す。

- サンプル$i$の予測値：$\hat{y}_i = w x_i + b$
- 実際の値：$y_i$
- 誤差：$y_i - (w x_i + b)$

損失関数（総誤差）を次のように定義：

$$
L(w, b) = \sum_{i=1}^{n} \bigl(y_i - (w x_i + b)\bigr)^2
$$

![bg right:35% 100%](https://s.ar8.top/img/picgo/20250423172345134.webp)

---

> 📌 なぜ二乗するのか：  
> - 正負の誤差が打ち消し合わない  
> - 微分可能（最適解を解析的に求められる）

---

### ✏️ 3. 偏微分による最適 $w$, $b$ の導出

#### 損失関数

$$
L(w, b) = \sum_{i=1}^{n} (y_i - w x_i - b)^2
$$

偏導して0と置き、最小値を求める。

---

##### Step 1：$b$について偏微分（$w$は固定）

$$
\frac{\partial L}{\partial b}
= \sum 2(y_i - w x_i - b)(-1)
= -2 \sum (y_i - w x_i - b) = 0
$$

$$
\Rightarrow \sum y_i - w \sum x_i - n b = 0
\quad\Rightarrow\quad
b = \bar{y} - w \bar{x}
$$

---

##### Step 2：$w$について偏微分

$$
\frac{\partial L}{\partial w}
= \sum 2(y_i - w x_i - b)(-x_i)
= -2 \sum x_i (y_i - w x_i - b) = 0
$$

先ほどの $b = \bar{y} - w \bar{x}$ を代入し整理すると：

$$
w = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}
$$

これが教科書に載っている**傾きの公式**。

---

## 🧠 幾何学的解釈：残差の二乗和を最小にする直線

各点と回帰直線の間に引かれる「残差線」の長さの二乗和を最小にする直線が線形回帰。

---

#### 最終的な解

$$
w = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}, 
\quad
b = \bar{y} - w \bar{x}
$$

ここで：

- $\bar{x} = \frac{1}{n}\sum x_i$：入力の平均  
- $\bar{y} = \frac{1}{n}\sum y_i$：出力の平均

> ✅ これが最適解としての**解析解**。

---

## 三．重回帰分析（二元線形回帰）

上司から「面積だけでなく**階数**も価格に影響する」と告げられました。

以下のデータ（特徴が2つ）を用意します：

| 面積（㎡） $x_1$ | 階数 $x_2$ | 価格 $y$（万元） |
|-------------------|-------------|-------------------|
| 50                | 1           | 200               |
| 60                | 2           | 250               |
| 70                | 3           | 300               |
| 80                | 4           | 350               |
| 90                | 5           | 400               |

---

### 💡 1. モデルの仮定

2つの特徴で価格を予測：

$$
\hat{y} = w_1 x_1 + w_2 x_2 + b
$$

- $x_1$：面積  
- $x_2$：階数  
- $w_1, w_2$：重み（係数）  
- $b$：バイアス（切片）

---

### 🎯 2. 損失関数

最小二乗法：

$$
L(w_1, w_2, b)
= \sum_{i=1}^{n} \bigl(y_i - (w_1 x_1^{(i)} + w_2 x_2^{(i)} + b)\bigr)^2
$$

---

### ✏️ 3. 偏微分による導出

各変数に対して偏微分し0と置くことで三元一次方程式が得られる。

#### $w_1$について

$$
\frac{\partial L}{\partial w_1}
= -2 \sum x_1^{(i)} \bigl(y_i - (w_1 x_1^{(i)} + w_2 x_2^{(i)} + b)\bigr) = 0
$$

#### $w_2$について

$$
\frac{\partial L}{\partial w_2}
= -2 \sum x_2^{(i)} \bigl(y_i - (w_1 x_1^{(i)} + w_2 x_2^{(i)} + b)\bigr) = 0
$$

#### $b$について

$$
\frac{\partial L}{\partial b}
= -2 \sum \bigl(y_i - (w_1 x_1^{(i)} + w_2 x_2^{(i)} + b)\bigr) = 0
$$

---

### 🧮 4. 連立一次方程式の整理

$$
\begin{cases}
\sum x_1 y = w_1 \sum x_1^2 + w_2 \sum x_1 x_2 + b \sum x_1, \\
\sum x_2 y = w_1 \sum x_1 x_2 + w_2 \sum x_2^2 + b \sum x_2, \\
\sum y     = w_1 \sum x_1     + w_2 \sum x_2     + n b.
\end{cases}
$$

実際の実装では、次のような**行列形式**が便利です。

---

### 🧾 5. 行列形式（1分）

特徴行列 $\mathbf{X}$ とラベルベクトル $\mathbf{y}$ を構築：

$$
\mathbf{X} =
\begin{bmatrix}
x_1^{(1)} & x_2^{(1)} & 1 \\
x_1^{(2)} & x_2^{(2)} & 1 \\
\vdots    & \vdots    & \vdots \\
x_1^{(n)} & x_2^{(n)} & 1
\end{bmatrix},
\quad
\mathbf{y} =
\begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots  \\
y^{(n)}
\end{bmatrix}
$$

---

目標：

$$
\min_{\boldsymbol{w}} \|\mathbf{y} - \mathbf{X}\boldsymbol{w}\|^2
$$

正規方程式の解は：

$$
\boldsymbol{w} =
\begin{bmatrix}
w_1 \\ w_2 \\ b
\end{bmatrix}
=
(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
$$

---

### 📈 6. 図解

3次元空間では：

- 横軸：$x_1$（面積）  
- 奥行軸：$x_2$（階数）  
- 縦軸：$y$（価格）  

直線ではなく**平面**をフィットさせ、全点から平面への垂直距離の二乗和を最小化する。

![bg right:40% 90%](https://s.ar8.top/img/picgo/20250423175529271.webp)

---

## 四．多変量線形回帰

### 🧩 1. モデル

特徴量が増えると（面積、階数、築年数、寝室数、駐車場数…）、モデルは**高次元空間**での線形関係となる：

$$
\hat{y}^{(i)} = w_1 x_1^{(i)} + w_2 x_2^{(i)} + \cdots + w_d x_d^{(i)} + b
$$

ベクトル表記で：

$$
\hat{y}^{(i)} = \mathbf{x}^{(i)T}\mathbf{w} + b
$$

---

### 🧾 2. 行列形式の定義

バイアス$b$を含めた拡張ベクトルを定義：

- 拡張入力：$\tilde{\mathbf{x}}^{(i)} = [x_1^{(i)}, x_2^{(i)}, \dots, x_d^{(i)}, 1]^T$  
- 拡張パラメータ：$\tilde{\mathbf{w}} = [w_1, w_2, \dots, w_d, b]^T$

- $\mathbf{X}$：$n\times(d+1)$ の特徴行列  
- $\mathbf{y}$：$n\times1$ の出力ベクトル  
- $\boldsymbol{w}$：$(d+1)\times1$ のパラメータベクトル

予測：

$$
\hat{\mathbf{y}} = \mathbf{X}\boldsymbol{w}
$$

---

### 🎯 3. 損失関数

最小二乗法（MSE）：

$$
L(\boldsymbol{w}) = \|\mathbf{y} - \mathbf{X}\boldsymbol{w}\|^2
= (\mathbf{y} - \mathbf{X}\boldsymbol{w})^T(\mathbf{y} - \mathbf{X}\boldsymbol{w})
$$

---

### ✏️ 4. 正規方程式

$$
\nabla_{\boldsymbol{w}} L = -2\mathbf{X}^T(\mathbf{y} - \mathbf{X}\boldsymbol{w}) = 0
\quad\Rightarrow\quad
\mathbf{X}^T\mathbf{y} = \mathbf{X}^T\mathbf{X}\,\boldsymbol{w}
$$

$$
\therefore\quad
\boldsymbol{w} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
$$

これが**正規方程式**による解析解。

---

## 五．NumPyでの正規方程式による多変量線形回帰の実装

続いて、**NumPy + Matplotlib** を使って**多変量線形回帰**を一気通貫で実装し、図示するコード例を示します。

### 📊 シミュレーションデータ（房价予測）

特徴：
- 面積 $x_1$  
- 階数 $x_2$  

ラベル：
- 価格 $y$  

---

### ✅ 完全コード例

（外部参照）

---

### 🧾 出力例

```
回帰係数 w1（面積）: 4.705882352941173
回帰係数 w2（階数）: 1.1764705882352907
バイアス b: 21.17647058823536
```

フィット面の式：
$$
\hat{y} = 4.71\,x_1 + 1.18\,x_2 + 21.18
$$

---

### 📈 フィット図

![拟合图像](https://s.ar8.top/img/picgo/20250423183034079.webp)

---

### 🎯 応用例：多特徴量による房价予測

4つの特徴を用いた予測例：

| 面積（㎡） $x_1$ | 階数 $x_2$ | 築年数（年） $x_3$ | 寝室数 $x_4$ | 価格 $y$（万元） |
|-------------------|-------------|--------------------|---------------|-------------------|
| 50                | 3           | 20                 | 2             | 210               |
| 65                | 2           | 15                 | 3             | 265               |
| 80                | 5           | 10                 | 4             | 325               |
| 55                | 4           | 7                  | 2             | 255               |
| 90                | 1           | 3                  | 5             | 480               |

---

## ✅ NumPyによる4元線形回帰の実装コード

（外部参照）

---

### 🧾 出力例

```
面積 の係数: -16.8167
階数 の係数: 1.2500
築年数 の係数: -9.8333
寝室数 の係数: 259.3333
バイアス の係数: 725.0833

予測価格（75㎡, 3階, 築8年, 3寝室）: 166.92 万元
```

> 🔍 補足：面積が1㎡増えると価格は約4万元増加、築年数が1年増えると約2.5万元減少、など。

---

## ✅ 小まとめ

- 線形回帰とは何か  
- 単回帰分析（一元線形回帰）  
- 重回帰分析（二元線形回帰）  
- 多変量線形回帰  
- NumPyによる正規方程式実装  

---

**次回予告：**  
> 🔍 勾配降下法を用いた線形回帰の解法を学びます。  
