---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第20講：LSTM（長短期記憶ネットワーク）

---

## 一、LSTMとは？

### 1.1 提案の背景

* すでに学んだように、**通常のRNN** は長い系列を扱うと **勾配消失（gradient vanishing）** が起こる：

  * ずっと前の情報が、ほとんど後の出力に影響しなくなる。
  * まるで長文を読んでいて、冒頭を最後まで覚えていられないようなもの。
* 1997年、Sepp Hochreiter と Jürgen Schmidhuber により  
  **LSTM（Long Short-Term Memory）** が提案され、  
  **RNN が長期依存を捉えられない問題** を解決するために開発された。

---

### 1.2 なぜ LSTM が発明されたのか？

* 通常の RNN には **1つの隠れ状態 ($h_t$)** しかなく、  
  それが「情報の保持」と「出力の決定」を同時に行う必要があり、負担が大きい。
* LSTM は RNN に新しい「記憶装置」を追加する：

  * **セル状態（cell state）**：コンベアベルトのように情報を長距離運ぶ。
  * “ゲート機構” により、情報の出入りを制御する。
* この構造により、必要な情報を長期的に保持し、不要な情報は忘れることができる。

---

### 1.3 直感的な理解（バルブ付きの記憶セル）

* LSTM を **水道管** にたとえると、水の流れが情報：

  * **忘却ゲート** = バルブ：「古い水（情報）」をどれだけ排出するか決める。
  * **入力ゲート** = バルブ：「新しい水（情報）」をどれだけ注入するか決める。
  * **出力ゲート** = バルブ：「どれだけの水を外に出すか」決める。
* つまり、LSTM は「選択的に記憶・忘却できる配管システム」といえる。
* RNN との最大の違いは：**長期記憶を持ち、情報の出入りを制御できる** こと。

---

## 二、LSTM の動作原理

### 2.1 コア構造：セル状態 (Cell State)

* **セル状態 (Cell State)** は系列全体に貫通するコンベアベルトのようなもの。
* 重要な情報を長く保持し、失われにくい。
* ゲート機構を通して **有用な情報を保持し、不要な情報を破棄できる**。

---

![bg 90%](https://s.ar8.top/img/picgo/20251007233132986.webp)

---

### 2.2 三つの「ゲート」構造

各タイムステップ $t$ ごとに、LSTM には3つの主要なゲートがある：

1. **忘却ゲート（Forget Gate）**

   * 「古い情報をどれだけ捨てるか」を決める。
   * 入力：現在の入力 $x_t$ と前の隠れ状態 $h_{t-1}$。
   * 出力：Sigmoid により 0〜1 の値。
   * 例：

     * 「昨日スーパーで果物を買った。今日は___を食べた。」
     * 忘却ゲートは「スーパーに行った」という古い情報を徐々に薄める。

---

2. **入力ゲート（Input Gate）**

   * 「どの新しい情報を記憶に書き込むか」を決める。
   * 2つの部分から構成：

     1. 入力ゲート（Sigmoidによる開閉制御）
     2. 候補情報（Tanhで生成）
   * 例：

     * 「今朝、リンゴを食べた。」
     * 入力ゲートは「リンゴ」という新情報をセルに書き込む。

---

3. **出力ゲート（Output Gate）**

   * 「セル状態からどれだけの情報を出力（隠れ状態として）」するかを決める。
   * 例：

     * 次の単語を予測するとき、出力ゲートがどの記憶を使うか制御する。

---

## 三、LSTM の構造と数式導出

### 3.1 各タイムステップの処理流れ

時刻 $t$ において、LSTM は以下を受け取る：

* 現在の入力 $x_t$（単語の埋め込みなど）
* 前の隠れ状態 $h_{t-1}$ と セル状態 $C_{t-1}$

そして以下の手順で計算：

1. 忘却ゲート：古い情報をどれだけ残すか決める  
2. 入力ゲート：どれだけ新情報を追加するか決める  
3. 候補記憶の生成  
4. 記憶セルの更新  
5. 出力ゲート：次の隠れ状態を決める

---

### 3.2 数式の展開

#### ① 忘却ゲート

$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$

* 入力：$h_{t-1}$ と $x_t$
* 出力：0〜1 のベクトル（Sigmoid）
* 意味：1=完全保持、0=完全破棄

---

#### ② 入力ゲート

$$
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
$$

* どの新情報をセルに書き込むかを制御
* 1=完全に書き込む、0=書き込まない

---

#### ③ 候補記憶

$$
\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)
$$

* -1〜1 の範囲で新しい候補情報を生成
* 入力ゲートと組み合わせて、書き込む部分を決定

---

#### ④ セル状態の更新

$$
C_t = f_t \ast C_{t-1} + i_t \ast \tilde{C}_t
$$

* 古い記憶 $C_{t-1}$ は忘却ゲートでフィルタされる
* 入力ゲートで選ばれた新情報を加える
* 結果：更新後の記憶セル $C_t$
* **LSTM の核心：情報が流れるように伝わり、古いものを保持しつつ新しいものを追加できる**

---

#### ⑤ 出力ゲートと隠れ状態

$$
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
$$
$$
h_t = o_t \ast \tanh(C_t)
$$

* 出力ゲートがどれだけの記憶を露出させるかを決定
* 最終隠れ状態 $h_t$ = 出力ゲート制御下の記憶内容
* $h_t$ は次ステップに渡されると同時に現在時刻の出力にもなる

---

### 3.3 RNN との比較

* **通常の RNN**：

$$
h_t = \tanh(W_x x_t + W_h h_{t-1} + b)
$$

  * 忘却・記憶機構がない
  * 情報は $h_t$ 内に埋もれ、長期保持が難しい

* **LSTM**：

  * 追加されたセル状態 $C_t$
  * **忘却ゲート・入力ゲート・出力ゲート** で情報流を制御
  * 勾配消失を防ぎ、長期依存を捉えることが可能

---

## 四、LSTM の直観的理解と可視化

### 4.1 要素の整理

* **隠れ状態 $h_t$**：現在の出力表現（外部とのやり取り）
* **セル状態 $C_t$**：長期記憶の「ストレージ」
* **ゲート**：Sigmoid により 0〜1 の出力で情報流を制御：

  * 忘却ゲート $f_t$：古い情報を忘れるか
  * 入力ゲート $i_t$：新情報を追加するか
  * 出力ゲート $o_t$：どれだけ出力するか

---

### 4.2 全体式の再確認

1. 忘却ゲート  
   $$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$
2. 入力ゲート + 候補記憶  
   $$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$  
   $$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$
3. セル更新  
   $$C_t = f_t \ast C_{t-1} + i_t \ast \tilde{C}_t$$
4. 出力ゲート  
   $$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$  
   $$h_t = o_t \ast \tanh(C_t)$$

---

### 4.3 比喩：水管とバルブモデル

**セル状態 $C_t$** を1本の水管と考える：

* **忘却ゲート $f_t$** = 排水バルブ：不要な情報を捨てる  
* **入力ゲート $i_t$ + 候補記憶 $\tilde{C}_t$** = 給水バルブ：新情報を注入  
* **出力ゲート $o_t$** = 出口バルブ：どれだけ水を出すか決め、$h_t$ を生成

→ LSTM は「記憶と忘却を制御する水管システム」：

* 不要な情報を徐々に流し、
* 必要な情報を注入し、
* 出力時には必要な部分だけを取り出す。

---

### 4.4 可視化構造

LSTM セル構造の一般的な図：

![120%](https://s.ar8.top/img/picgo/20251007233132986.webp)

---

### 4.5 まとめ

* LSTM は **セル状態 $C_t$** によって長期依存を保持
* **3つのゲート** が情報流を制御：

  * 古い情報を忘れ  
  * 新情報を追加し  
  * 有用な情報を出力する

→ RNN の勾配消失を克服し、長文や長時間依存に強い。

---

## 五、LSTM の派生モデル

### 5.1 標準 LSTM

* 今まで説明したものが **標準LSTM**
* 特徴：

  * 長期依存を捉える
  * RNN より安定して学習できる
  * ただし計算量・パラメータ数が多い

---

### 5.2 Peephole LSTM（ピープホールLSTM）

* 改良点：ゲートの計算に $C_{t-1}$（前セル状態）も加える
  $$
  f_t = \sigma(W_f \cdot [h_{t-1}, x_t, C_{t-1}] + b_f)
  $$
* 効果：セル内容を「覗き見」して制御精度を上げる  
* 用途：時間依存性が高い時系列タスク

---

### 5.3 双方向LSTM（Bi-LSTM）

* 過去と未来の両方を考慮  
* 構造：

  * 順方向LSTM  
  * 逆方向LSTM  
  * 出力を結合：
    $$
    h_t = [\overrightarrow{h_t}, \overleftarrow{h_t}]
    $$
* 応用：NER、POSタグ付け、テキスト分類  
* 欠点：リアルタイム処理不可

---

### 5.4 スタック LSTM（Stacked LSTM）

* 複数層の LSTM を積み重ねる
* 出力 $h_t^{(1)}$ → 次層入力
* 長所：複雑な特徴を学習  
* 短所：過学習・計算コスト増大

---

### 5.5 GRU（ゲート付き再帰ユニット）

* LSTM の **簡易版**
* 2つのゲートのみ（更新・リセット）
* セル状態を持たず、$h_t$ を直接使用
* 長所：計算が速く、性能も近い  
* 短所：極長依存ではやや劣る

---

## 六、LSTM の応用例

### 6.1 言語モデルと機械翻訳

* LSTM は文中の長距離依存を捉えるのが得意
* **言語モデル**：前の単語列から次の単語を予測  
* **神経機械翻訳（NMT）**：Encoder-Decoder で広く利用  
  * エンコーダ：入力文を文脈表現に変換  
  * デコーダ：出力文を生成  

---

### 6.2 テキスト分類・感情分析

* 文全体を読み取り、最終出力を分類  
* 応用：  
  * 感情分析（ポジ/ネガ）  
  * ニュース分類  
  * スパム検出  

---

### 6.3 時系列予測

* 連続する時間変化をモデル化  
* 応用：
  * 株価予測  
  * センサーデータ解析  
  * 天気予報  

---

### 6.4 音声認識

* 音声信号は典型的な長系列  
* LSTM はフレームごとの特徴量（MFCCなど）を順次処理  
* Transformer・CTC登場以前は中心的手法だった  

---

## 七、LSTM の限界

### 7.1 学習コストが大きい

* ゲートが3組あり、RNNより計算・パラメータとも多い

---

### 7.2 超長系列には限界

* 数十ステップまでは良好だが、  
  数千ステップの文や音声には対応困難  

---

### 7.3 並列処理ができない

* 各ステップが前の結果に依存するため  
  Transformer のように全系列同時計算できない  

---

### 7.4 Transformer による置き換え

* 2017年の Transformer 以降、ほとんどの系列タスクで LSTM を凌駕  
* 長距離依存の表現力  
* 完全並列計算  
* 大規模モデルへの拡張性（BERT, GPT）

→ 現在は小規模・組込み・教育用に利用されることが多い

---

## 八、実践：単語レベルの感情分類（IMDB）

**目的**：前文脈から次単語や感情を予測  
**方法**：

1. テキストの分割と語彙作成  
2. Embedding でベクトル化（Word2Vec/GloVe）  
3. LSTM モデルに入力し出力分布を得る  
4. 学習：クロスエントロピー損失、評価：Perplexity  
5. 推論：系列生成や感情判定に応用
