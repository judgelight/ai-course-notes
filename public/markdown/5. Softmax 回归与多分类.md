---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

## <!-- fit -->📘 第5讲：Softmax 回归与多分类

---

# 🎓 第1节：问题引入与多分类场景

> “我们之前学了逻辑回归，可以解决二分类问题：是 / 否，正类 / 负类……
> 那如果有 3 个、10 个，甚至 100 个类别呢？逻辑回归还够用吗？”

---

## 📌 举几个实际案例：

### 🐶 图像识别（3 类）：

* 输入：一张图片（特征向量）
* 输出：是狗 / 猫 / 鸟中的哪一类？

我们不想只知道“是不是猫”，而是想知道它**属于哪个类别**。

---

### 📰 新闻文本分类（4 类）：

* 输入：一篇新闻文章（经过向量化处理）
* 输出：它属于体育 / 科技 / 娱乐 / 财经 哪一个？

---

### 🧪 故障代码识别（多类别）：

* 输入：某设备的传感器数据
* 输出：可能是故障代码 100 / 200 / 300 / 999 中的哪一个？

---

## ✅ 1.2 为什么逻辑回归不够用？

逻辑回归的输出是：

$$
\hat{y} = \sigma(w^T x + b) \in (0, 1)
$$

表示是“某个特定类别”的概率（如属于 1 类 vs 非 1 类），只能处理：

* **两个类别**（正类 / 负类）
* 不适合多个互斥类别（如狗、猫、鸟）

---

## ❌ 尝试错误方法：多个逻辑回归组合

有同学可能会说：

> “我们可以训练多个逻辑回归模型：分别预测是不是猫，是不是狗，是不是鸟”

但是会出现这些问题：

| 问题          | 说明                  |
| ----------- | ------------------- |
| ✅ 不互斥       | 模型可能同时说“像猫”也“像狗”    |
| 📉 概率无法归一化  | 所有输出加起来可能 > 1 或 < 1 |
| ❌ 无法选出最可能类别 | 缺少统一标准进行选择          |

---

## ✅ 正确思路：需要**一个能输出多个类别概率**的函数

> 我们想要的是：
> **给定一个输入 $x$，输出一个 K 维向量，表示属于每个类别的概率**

* 输出应该是：

  $$
  \hat{y} = [0.1, 0.7, 0.2]
  $$
* 这些概率必须是：

  * 非负
  * 总和为 1（形成一个分布）

这时，**Softmax 函数**应运而生。

---

# 🎓 第2节：Softmax 函数的定义与性质

---

## ✅ 2.1 Softmax 是什么

> Softmax 是一个将 **任意实数向量** 映射成 **概率分布** 的函数。

### 📐 数学定义：

对于一个长度为 $K$ 的实数向量 $z = [z_1, z_2, ..., z_K]$，Softmax 函数定义为：

$$
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} \quad \text{for } i = 1, ..., K
$$

每一个输出 $\hat{y}_i$ 表示：**输入属于第 $i$ 类的概率**

---

## ✅ 2.2 Softmax 的输出性质

### 📌 特点 1：每一维都是非负

$$
\text{Softmax}(z_i) > 0 \quad \text{因为 } e^{z_i} > 0
$$

---

### 📌 特点 2：所有维度之和等于 1（形成概率分布）

$$
\sum_{i=1}^K \text{Softmax}(z_i) = 1
$$

所以 Softmax 的输出可以解释为：**属于各类的概率**

---

### 📌 特点 3：输入差异越大，输出越“极端”

* 若一个 $z_i$ 明显大于其他值，则 Softmax 输出会非常接近 one-hot：

  $$
  \text{Softmax}(10, 0, 0) \approx [0.999, 0.0004, 0.0004]
  $$
* 若输入相差不大，输出更“平滑”：

  $$
  \text{Softmax}(1, 1, 1) = [0.33, 0.33, 0.33]
  $$

✅ 这正是我们想要的分类行为：**“越自信，越极端”**

---

## ✅ 2.3 Softmax 是 Sigmoid 的推广

如果你只预测 2 个类别：

* 输入：$z = [z_1, z_2]$
* Softmax 输出：

  $$
  \hat{y}_1 = \frac{e^{z_1}}{e^{z_1} + e^{z_2}} \quad,\quad
  \hat{y}_2 = \frac{e^{z_2}}{e^{z_1} + e^{z_2}}
  $$

设 $z = z_1 - z_2$，你会得到：

$$
\hat{y}_1 = \frac{1}{1 + e^{-z}} = \text{Sigmoid}(z)
$$

📌 所以 **Softmax 是 Sigmoid 的多类版本**

---

## ✅ 小结

| 特性 | Softmax                                        |
| -- | ---------------------------------------------- |
| 输入 | 任意实数向量 $z \in \mathbb{R}^K$                    |
| 输出 | 概率分布 $\hat{y} \in (0,1)^K, \sum \hat{y}_i = 1$ |
| 用途 | 多类别分类任务                                        |
| 对比 | 是 sigmoid 的多类扩展版本                              |

---

# 🎓 第3节：模型结构与交叉熵损失函数

---

## ✅ 3.1 模型结构回顾

### 多分类逻辑结构：

我们要做的事情是从输入特征 $x \in \mathbb{R}^d$，预测属于 $K$ 个类别中哪一个。

### 📐 模型公式：

$$
\hat{y} = \text{Softmax}(Wx + b)
$$

其中：

* $W \in \mathbb{R}^{K \times d}$：权重矩阵
* $b \in \mathbb{R}^{K}$：偏置向量
* $\hat{y} \in \mathbb{R}^{K}$：输出为属于各类别的概率分布

---

## ✅ 3.2 输出与真实标签的差距如何衡量？

我们需要设计一个损失函数，用来度量：

> **模型输出 $\hat{y}$** 和 **真实标签 $y$** 之间的差距有多大？

---

## ✅ 3.3 什么是交叉熵损失（Cross-Entropy Loss）？

假设标签 $y \in \mathbb{R}^K$ 是 **one-hot 编码**：

* 如果真实类别是第 2 类：

  $$
  y = [0, 1, 0, 0]
  $$

Softmax 的输出为概率向量 $\hat{y} = [0.1, 0.5, 0.2, 0.2]$

我们要强调的目标是：**让正确的那一项（第2个）概率越大越好**

---

### ✅ 损失函数公式：

$$
L = - \sum_{i=1}^{K} y_i \log \hat{y}_i
$$

由于 $y$ 是 one-hot 编码，上面式子其实只保留了**正确类别对应的 $\hat{y}_i$**：

$$
L = - \log (\hat{y}_{\text{true class}})
$$

---

### 📌 举例：

* 正确类别是类 2，Softmax 输出：

  $$
  \hat{y} = [0.1, \mathbf{0.7}, 0.1, 0.1]
  \Rightarrow L = -\log(0.7) \approx 0.357
  $$
* 若模型输出错误的预测：

  $$
  \hat{y} = [0.4, \mathbf{0.2}, 0.2, 0.2]
  \Rightarrow L = -\log(0.2) \approx 1.609
  $$

✅ 正确预测 → 损失小；错误预测 → 损失大

![bg right:40% 90%](https://s.ar8.top/img/picgo/20250521185248423.webp)

---

## ✅ 3.4 为什么选用交叉熵损失？

| 优点            | 解释                          |
| ------------- | --------------------------- |
| 🎯 对概率分布建模更自然 | Softmax 输出的是概率，交叉熵衡量概率分布间差距 |
| 📉 单调惩罚错误预测   | 预测越偏离真实标签，损失越大              |
| ✅ 可导性强        | 与 Softmax 联用，梯度简单明了，训练高效    |
| 🌐 与最大似然等价    | 交叉熵等价于最大化真实类别的概率（最大似然估计）    |

---

## ✅ 小结

| 项目   | 内容                                  |
| ---- | ----------------------------------- |
| 输入   | 特征向量 $x \in \mathbb{R}^d$           |
| 输出   | 类别概率向量 $\hat{y} \in \mathbb{R}^K$   |
| 模型   | $\hat{y} = \text{Softmax}(Wx + b)$  |
| 标签   | One-hot 编码向量                        |
| 损失函数 | 交叉熵 $L = - \sum y_i \log \hat{y}_i$ |

---

# 🎓 第4节：梯度推导与参数更新

---

## ✅ 4.1 模型回顾

对于每个样本 $x_i \in \mathbb{R}^d$，我们预测：

$$
\hat{y}_i = \text{Softmax}(W x_i + b) \in \mathbb{R}^K
$$

对应的交叉熵损失为：

$$
L = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{K} y_{ij} \log \hat{y}_{ij}
$$

* $Y \in \mathbb{R}^{n \times K}$：真实标签的 one-hot 编码矩阵
* $\hat{Y} \in \mathbb{R}^{n \times K}$：模型预测输出

---

## ✅ 4.2 对权重 $W$ 的梯度推导

$$
L = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{K} y_{ij} \log \hat{y}_{ij}
$$

使用链式法则，最终可以得到：

$$
\nabla_W = \frac{1}{n} X^T (\hat{Y} - Y)
$$

* $X \in \mathbb{R}^{n \times d}$：输入特征矩阵
* $\hat{Y} - Y \in \mathbb{R}^{n \times K}$：预测误差
* $\nabla_W \in \mathbb{R}^{d \times K}$

> 每一行样本 $x_i$ 都会对预测误差 $(\hat{y}_i - y_i)$ 造成影响，梯度是所有样本对 $W$ 的累计导数。

---

## ✅ 4.3 对偏置 $b$ 的梯度

类似地，我们可以得到：

$$
\nabla_b = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)
$$

或向量形式：

$$
\nabla_b = \frac{1}{n} \sum_{i=1}^{n} (\hat{Y} - Y) \in \mathbb{R}^K
$$

---

## ✅ 4.4 梯度下降更新公式

使用学习率 $\alpha$，每次更新：

$$
W := W - \alpha \cdot \nabla_W
\quad
$$
$$
\quad
b := b - \alpha \cdot \nabla_b
$$

---

## ✅ 4.5 与逻辑回归的比较

| 项目   | 逻辑回归                | Softmax 回归          |
| ---- | ------------------- | ------------------- |
| 输出维度 | 标量（1 个概率）           | 向量（K 个概率）           |
| 标签格式 | 标量（0/1）             | One-hot 向量          |
| 损失函数 | 对数损失                | 交叉熵损失               |
| 梯度表达 | $X^T (\hat{y} - y)$ | $X^T (\hat{Y} - Y)$ |

✅ 所以结构几乎一致，逻辑回归就是 Softmax 的二维版本！

---

# 🎓 第5节：使用 NumPy 手动实现的 Softmax 多分类模型训练过程

---

![](https://s.ar8.top/img/picgo/20250521192842701.webp)

---

### 📍左图：Softmax 分类器的决策边界

* 背景区域被颜色分成 3 类：

  * 红色、蓝色、浅色分别表示模型预测为类别 0、1、2 的区域
* 中心点是训练样本，颜色表示其真实类别（共 3 类）
* 中间的曲线是不同类别间的决策边界，体现出 Softmax 对多分类的处理能力

---

### 📉右图：训练过程中交叉熵损失的下降曲线

* 显示了训练中损失函数逐步减小
* 模型正在不断调整参数，以更好拟合数据

---

### ✅ 本节实现内容：

* 生成 3 类可分数据
* 构建 Softmax 前向传播与交叉熵损失函数
* 使用 NumPy 实现多分类的梯度下降训练
* 可视化决策边界与训练过程

---

# 🎓 第6节：总结 + 多分类的扩展方向

---

## ✅ 6.1 知识回顾表

| 项目   | 逻辑回归                          | Softmax 回归                        |
| ---- | ----------------------------- | --------------------------------- |
| 任务类型 | 二分类                           | 多分类（互斥类别）                         |
| 激活函数 | Sigmoid                       | Softmax                           |
| 输出维度 | 标量（概率）                        | K 维概率向量                           |
| 输出范围 | $(0, 1)$                      | $(0, 1)^K$，且 $\sum \hat{y}_i = 1$ |
| 损失函数 | 对数损失（Log Loss）                | 交叉熵损失（Cross Entropy Loss）         |
| 预测方式 | $\hat{y} > 0.5 \Rightarrow 1$ | $\hat{y} = \arg\max_i \hat{y}_i$  |


---

## ✅ 6.2 Softmax 输出是否可以看作“概率”？

答案是：**可以，但有前提。**

### 🧠 原因：

* Softmax 输出的每一项都在 0\~1 之间
* 所有输出加起来等于 1

### 📌 注意：

* 它是“模型认为属于该类的相对信心”
* 严格来说，不是**统计概率**，但可以作为概率近似使用
* 在多数分类任务中，我们用它来：

  * 选出最大值作为预测类别
  * 分数越高，模型越“自信”

---

## ✅ 6.3 模型是否可以处理多标签分类？

目前我们所讲的 Softmax 是针对**互斥类别（multi-class）**：

* 一张图片只能是“狗”或“猫”，不能同时是
* 📷 一张图片可以同时包含“狗”+“人”+“车”

✅ 这类任务叫做：**多标签分类（multi-label classification）**

### ❌ Softmax 不适用

* 因为它要求所有输出之和为 1
* 多标签任务不能强迫类别之间竞争

### ✅ 用法替代：

* 每个输出节点使用 **sigmoid** 代替 Softmax
* 使用 **多重对数损失**，对每个标签单独计算 loss

---

## ✅ 6.4 Softmax 的工程扩展应用

| 应用领域    | 说明                 |
| ------- | ------------------ |
| 图像分类    | 输出属于多个视觉类别的概率      |
| 文本情感分析  | 正向 / 负向 / 中性分类     |
| 语音命令识别  | 多种指令意图分类           |
| OCR字符识别 | 输出最可能的字符标签（a \~ z） |

---

## 🧠 本课总结：

* 你现在可以清楚地区分 **逻辑回归 vs Softmax 回归**
* 掌握了 Softmax 函数的定义与性质
* 理解了多类交叉熵损失函数，并会手动实现训练过程
* 已经能通过 NumPy 实现完整多分类训练与可视化
* 对多标签、多类、神经网络分类任务有了直觉认识

---

**下一节预告：**
> 🔍 神经网络基础（多层感知机，MLP）