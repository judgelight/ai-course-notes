---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

## <!-- fit -->📘 第8讲：非监督学习-2

---

# 🎓 非监督学习实战 · DBSCAN 聚类

---

## 1. 为什么要用 DBSCAN？

* **K-Means** 和 **层次聚类** 都需要指定簇数 $K$，且对噪声敏感、只能发现凸形簇。
* **DBSCAN**（Density-Based Spatial Clustering of Applications with Noise）基于数据点密度，可自动识别噪声，能发现任意形状的簇。

> 🔑 适用场景：
>
> * 聚簇形状复杂
> * 噪声或离群点较多
> * 簇数事先未知

---

## 2. 核心概念与参数

1. **参数**

   * $\varepsilon$ （eps）：邻域半径
   * $\mathit{MinPts}$：最小样本数阈值

2. **邻域（ε-Neighborhood）**
   对数据点 $x_i$，定义

   $$
   N_\varepsilon(x_i) = \{\,x_j \mid \|x_j - x_i\| \le \varepsilon\}\,.
   $$

3. **核心点（Core Point）**
   如果 $\lvert N_\varepsilon(x_i)\rvert \ge \mathit{MinPts}$，则 $x_i$ 是核心点。

---

4. **边界点（Border Point）**
   如果 $\lvert N_\varepsilon(x_i)\rvert < \mathit{MinPts}$ 但 $x_i$ 落在某核心点的 ε-邻域内，则为边界点。

5. **噪声点（Noise）**
   既不是核心点，也不是任何核心点邻域内的边界点。

6. **直接密度可达（Directly Density-Reachable）**
   $x_j$ 在 $x_i$ 的 ε-邻域且 $x_i$ 是核心点：

   $$
   x_j \in N_\varepsilon(x_i)
   \quad\text{and}\quad
   |N_\varepsilon(x_i)| \ge \mathit{MinPts}.
   $$

---

7. **密度可达（Density-Reachable）**
   存在一条核心点链 $x_i = p_1, p_2, \dots, p_k = x_j$ ，使得每步都是直接密度可达：

   $$
   p_{m+1} \in N_\varepsilon(p_m),\quad |N_\varepsilon(p_m)| \ge \mathit{MinPts}.
   $$

8. **密度可连通（Density-Connected）**
   如果存在某核心点 $p$，使 $x_i$ 和 $x_j$ 都密度可达于 $p$。

---

## 3. 算法流程与伪代码

输入：数据集 X, 半径 ε, 最小样本数 MinPts

初始化：所有点标记为“未访问”。

---

```text

for each point x in X:
  if x 已访问: continue
  标记 x 为已访问
  N = N_ε(x)  # 计算 ε 邻域
  if |N| < MinPts:
    标记 x 为噪声
  else:
    创建新簇 C
    将 x 加入 C
    seeds = N \ {x}
    for each point y in seeds:
      if y 未访问:
        标记 y 为已访问
        M = N_ε(y)
        if |M| ≥ MinPts:
          seeds = seeds ∪ (M \ seeds)
      if y 不属于任何簇:
        将 y 加入 C
```

* **核心思想**：从每个核心点出发，扩展其可达区域，直至没有新点加入。

---

## 4. 关键运算公式汇总
| 概念       | 公式/定义 |
| -------- | ------------------------------------------------------------ |
| ε-邻域     | $N_\varepsilon(x) = \{\,y \mid \|y - x\| \le \varepsilon\}$ |
| 核心点判定    | $\lvert N_\varepsilon(x)\rvert \ge \mathit{MinPts}$          |
| 直接密度可达   | $y \in N_\varepsilon(x)$ 与 $x$ 是核心点                          |
| 密度可达（链式） | 存在链 $p_1,\dots,p_k$ 且每步直接密度可达                                |
| 密度可连通    | 存在核心点 $p$，使 $x_i$ 和 $x_j$ 都密度可达于 $p$                         |

---

## 5. 参数选择
* **ε**

  * 太小：大部分点被视为噪声
  * 太大：所有点聚为一个簇

* **MinPts**

  * 通常 ≥ 数据维度 + 1
  * 值越大，簇越稠密

---

## 6. sklearn 实战预览

```python
from sklearn.cluster import DBSCAN

db = DBSCAN(eps=0.5, min_samples=5, metric='euclidean')
labels = db.fit_predict(X)

# labels == -1 表示噪声点
```

* `eps` 对应 ε
* `min_samples` 对应 MinPts
* `metric` 可选欧氏、曼哈顿等距离度量


---

## 7. 优缺点与实践指南

| 优点              | 缺点                |
| --------------- | ----------------- |
| 自动识别簇数，不需预设 $K$ | 对 ε 和 MinPts 参数敏感 |
| 能发现任意形状簇        | 稀疏高维数据中难以选参       |
| 自带噪声过滤          | 算法复杂度较高，海量数据需加速索引 |

**实践建议**：

1. 用 **k-distance plot**（第 $k$ 最近邻距离）辅助选 `eps`。
2. 对高维数据先做**降维**或**特征筛选**。
3. 当簇密度差异大时，考虑分层或局部参数调整。
