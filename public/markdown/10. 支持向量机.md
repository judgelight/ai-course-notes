---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

## <!-- fit -->📘 第10讲：支持向量机SVM

---

### 第1节：SVM 背景与线性可分分类

#### 1.1 分类问题回顾

* **二分类任务**：给定特征向量 $\mathbf{x}\in\mathbb R^d$，标签 $y\in\{-1,+1\}$。
* 常见方法：感知机（Perceptron）、逻辑回归（Logistic Regression）——感知机只保证分类正确，但对边界不稳定；逻辑回归优化概率对数损失。

---

#### 1.2 最大间隔的超平面

* **问题**：在所有正确分类的线性超平面中，选择哪个最“稳健”？
* **答案**：最大化“几何间隔”（geometric margin）。

  * 给定超平面 $\mathbf w^\top \mathbf x + b = 0$，对点 $(\mathbf x_i,y_i)$，其函数间隔 $y_i(\mathbf w^\top \mathbf x_i + b)$。
  * 几何间隔定义为：

    $$
      \hat\gamma_i = \frac{y_i(\mathbf w^\top \mathbf x_i + b)}{\|\mathbf w\|}.
    $$
  * 最大化最小几何间隔，提升模型对噪声的鲁棒性。

---

#### 1.3 支持向量
样本中距离超平面最近的一些点，这些点叫做支持向量

![bg right:50%](https://s.ar8.top/img/picgo/20250709181950978.webp)

---

#### 1.4 最优间隔分类器（硬间隔 SVM）

* **优化目标**：

  $$
  \begin{aligned}
    &\max_{\mathbf w, b}\;\min_i\;\hat\gamma_i
    \quad\Longleftrightarrow\quad
    \min_{\mathbf w,b}\;\frac{1}{2}\|\mathbf w\|^2
  \\
    &\text{s.t.}\quad y_i(\mathbf w^\top \mathbf x_i + b)\ge1,\;\forall i.
  \end{aligned}
  $$
* 几何解释：带距离标尺的平面，两类样本到超平面的距离至少为 1/‖w‖。
![bg right:50%](https://s.ar8.top/img/picgo/20250709181950978.webp)

---

#### 1.5 “硬间隔 SVM”优化目标深入的拆解。

##### 1.5.1. 从“最大化几何间隔”到“最小化权重范数”

###### 1.5.1.1 几何间隔（Geometric Margin）定义

* 对于超平面 $\mathbf w^\top \mathbf x + b = 0$，任意点 $\mathbf x_i$ 到该超平面的距离为

  $$
    d_i = \frac{\bigl|\mathbf w^\top \mathbf x_i + b\bigr|}{\|\mathbf w\|}.
  $$
* 如果样本带标签 $y_i\in\{+1,-1\}$，则有带符号的“几何间隔”

  $$
    \hat\gamma_i 
    = y_i\;\frac{\mathbf w^\top \mathbf x_i + b}{\|\mathbf w\|},
  $$

  这样正类点 $(y_i=+1)$ 得到正距离，负类点 $(y_i=-1)$ 得到负距离。

---

###### 1.5.1.2 最大化最小间隔

* 我们希望选择一个超平面，使得“所有点到超平面的最小几何间隔”最大，形式化为

  $$
    \max_{\mathbf w,b}\;\min_{i}\;\hat\gamma_i.
  $$
* 这保证了无论面对哪个样本，离超平面最近的那个离得尽可能远，从而获得最好的“鲁棒性”。

---

###### 1.5.1.3 等价地，将间隔规范化到 1

由于可以同时把 $(\mathbf w, b)$ 缩放一个正数 $\kappa>0$（即令 $\mathbf w'=\kappa\mathbf w,\;b'=\kappa b$），而几何间隔会按同样比例放大，直接最大化 $\min_i\hat\gamma_i$ 并不唯一。为了解决这个自由度，我们**规定**：

$$
  \min_i\;y_i\bigl(\mathbf w^\top \mathbf x_i + b\bigr) = 1.
$$

在这一“归一化”下，所有样本到无归一化超平面的函数间隔都至少为 1，即

$$
  y_i\bigl(\mathbf w^\top \mathbf x_i + b\bigr)\;\ge\;1,\quad \forall i.
$$

此时，几何间隔中的分子最小值固定为 1，于是

$$
  \min_i\,\hat\gamma_i
  = \frac{1}{\|\mathbf w\|}.
$$

因此，最大化 $\min_i\hat\gamma_i$ 变成了**最小化** $\|\mathbf w\|^{-1}$，也即**最小化** $\|\mathbf w\|$。

---

##### 1.5.2. 最终的优化问题（Primal Form）

为了便于计算，又常将目标写作 $\tfrac12\|\mathbf w\|^2$（加上 $\tfrac12$ 在后续求导时更简洁）：

$$
\begin{aligned}
  &\min_{\mathbf w,b}
  \quad \frac{1}{2}\,\|\mathbf w\|^2
  \\[6pt]
  &\text{s.t.}\quad
  y_i\bigl(\mathbf w^\top \mathbf x_i + b\bigr)\;\ge\;1,
  \quad i=1,2,\dots,N.
\end{aligned}
$$

* **目标**：让 $\|\mathbf w\|$ 尽可能小 ⇒ 间隔 $1/\|\mathbf w\|$ 尽可能大。
* **约束**：保证所有样本被正确分类，且函数间隔 ≥ 1。
![bg right:50%](https://s.ar8.top/img/picgo/20250709181950978.webp)

---

##### 1.5.3. 几何解释（带距离标尺的平面）

> **要点**：在这个规范化下，离超平面最近的正、负类点，恰好落在两条平行且距离为 $2/\|\mathbf w\|$ 的边界线上。

1. **决策面**：$\mathbf w^\top\mathbf x + b = 0$。
2. **正类边界**：$\mathbf w^\top\mathbf x + b = +1$。
3. **负类边界**：$\mathbf w^\top\mathbf x + b = -1$。

* 这两条界线与决策面平行，相距

  $$
    \frac{+1 - (-1)}{\|\mathbf w\|}
    = \frac{2}{\|\mathbf w\|}.
  $$
* 因此“间隔宽度”定义为两条边界间的距离，即 $2/\|\mathbf w\|$，而从决策面到任一边界的**半宽度**正好是 $1/\|\mathbf w\|$。

---


### 第2节：原始问题与拉格朗日对偶

#### 2.1 原始（Primal）形式

$$
\min_{\mathbf w,b}\;\frac{1}{2}\|\mathbf w\|^2
\quad
\text{s.t.}\;y_i(\mathbf w^\top \mathbf x_i + b)\ge1,\;i=1,\dots,N.
$$

#### 2.2 构造拉格朗日函数

引入拉格朗日乘子 $\alpha_i \ge0$，

$$
L(\mathbf w,b,\boldsymbol\alpha)
= \frac{1}{2}\|\mathbf w\|^2 - \sum_{i=1}^N \alpha_i\bigl[y_i(\mathbf w^\top \mathbf x_i + b)-1\bigr].
$$

---
#### 2.3 KKT 条件与对偶问题

* 对 $\mathbf w$ 和 $b$ 求偏导并令零：

  $$
  \frac{\partial L}{\partial \mathbf w}
  = \mathbf w - \sum_i \alpha_i y_i\mathbf x_i = 0
  \quad\Longrightarrow\quad
  \mathbf w = \sum_i\alpha_i y_i\mathbf x_i;
  $$

  $$
  \frac{\partial L}{\partial b}
  = -\sum_i \alpha_i y_i = 0.
  $$

---

* 代回得到对偶（Dual）问题：

  $$
  \begin{aligned}
    &\max_{\boldsymbol\alpha}\;\sum_{i=1}^N \alpha_i - \frac12\sum_{i,j}\alpha_i\alpha_j y_i y_j \mathbf x_i^\top \mathbf x_j,\\
    &\text{s.t.}\;\sum_i\alpha_i y_i=0,\;\alpha_i\ge0.
  \end{aligned}
  $$
* **支持向量**：对应 $\alpha_i>0$ 的样本点，位于间隔边界或误分类边界上。

---

#### 2.4 从对偶恢复模型

* $\mathbf w$ 如上所示 
  $$
    w = \sum_{i=1}^N \alpha_i\,y_i\,\mathbf x_i
  $$
* 求 $b$ 时，任选一个支持向量 $\mathbf x_k$：

  $$
  b = y_k - \sum_i\alpha_i y_i \mathbf x_i^\top \mathbf x_k.
  $$

---

### 第3节：Kernel Trick 与非线性分类

#### 3.1 非线性可分与特征映射

* 当数据线性不可分时，引入映射 $\phi:\mathbb R^d\to\mathcal H$（高维特征空间），再在 $\mathcal H$ 中做线性分类。

#### 3.2 核函数定义

* **核函数** $K(\mathbf x_i,\mathbf x_j)=\langle\phi(\mathbf x_i),\phi(\mathbf x_j)\rangle$。
* 优势：无需显式计算 $\phi$，只需核函数值。

---

#### 3.3 常用核函数

| 核类型         | 公式                                                 | 说明                  |
| ----------- | -------------------------------------------------- | ------------------- |
| 线性核（Linear） | $K(\mathbf x,\mathbf z)=\mathbf x^\top\mathbf z$   | 不映射，退化为线性 SVM       |
| 多项式核（Poly）  | $(\mathbf x^\top\mathbf z + r)^d$                  | 参数：阶数 $d$、常数 $r$    |
| RBF（高斯）核    | $\exp\bigl(-\gamma\|\mathbf x-\mathbf z\|^2\bigr)$ | 参数：$\gamma>0$，局部相似度 |
| Sigmoid 核   | $\tanh(\kappa\,\mathbf x^\top\mathbf z + \theta)$  | 类似神经网络激活函数          |

---

### 第4节：软间隔 SVM（容错分类）

#### 4.1 软间隔引入松弛变量

* 对每个样本引入 $\xi_i\ge0$，允许违例
* 约束调整为 $y_i(\mathbf w^\top \mathbf x_i + b)\ge1-\xi_i$。

#### 4.2 优化目标

$$
\min_{\mathbf w,b,\boldsymbol\xi}
\;\frac12\|\mathbf w\|^2 + C\sum_{i=1}^N \xi_i
\quad
\text{s.t.}\;
y_i(\mathbf w^\top \mathbf x_i + b)\ge1-\xi_i,\;\xi_i\ge0.
$$

* **参数 $C$**：权衡间隔大小与违例惩罚，$C$ 越大，越倾向于硬间隔；$C$ 越小，更容忍违例。

---

#### 4.3 对偶形式及核化

* 同样可推导含 $C$ 的对偶问题，核函数直接替换 $\mathbf x_i^\top \mathbf x_j$。

$$
\boxed{
\begin{aligned}
&\max_{\boldsymbol\alpha}
\quad
\sum_{i=1}^N \alpha_i
- \tfrac12 \sum_{i,j} \alpha_i\alpha_j\,y_i y_j\,(x_i^\top x_j),\\
&\text{s.t.}\quad
\sum_i \alpha_i y_i = 0,
\quad
0 \le \alpha_i \le C,\quad i=1,\dots,N.
\end{aligned}
}
$$

* 与硬间隔 SVM 的唯一区别：额外的上界 $\alpha_i\le C$。
* 这保证了即使某点想贡献很大，也只能贡献到 $C$ 的程度，多余的“犯错惩罚”归到 $\xi$ 上。

---

### 第5节：多类 SVM 与实践

#### 5.1 多类 SVM 策略

* **一对多（One-vs-Rest）**：针对每个类训练一个二分类 SVM
* **一对一（One-vs-One）**：每两类间训练一个 SVM，共 $K(K-1)/2$ 个分类器
* scikit-learn 默认采用一对多策略。

#### 5.2 scikit-learn 示例

```python
from sklearn import datasets
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import classification_report, ConfusionMatrixDisplay

# 加载数据
X, y = datasets.load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 参数调优
param_grid = [
    {'kernel': ['linear'], 'C': [0.1, 1, 10]},
    {'kernel': ['rbf'], 'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10]},
]
grid = GridSearchCV(SVC(), param_grid, cv=5)
grid.fit(X_train, y_train)
print("Best params:", grid.best_params_)

# 评估
y_pred = grid.predict(X_test)
print(classification_report(y_test, y_pred))
ConfusionMatrixDisplay.from_estimator(grid, X_test, y_test)
```

