---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

## <!-- fit -->📘 第5講：Softmax 回帰と多クラス分類

---

# 🎓 第1節：問題提起と多クラス分類のシナリオ

> 「これまでロジスティック回帰で二値分類（はい／いいえ、正例／負例）を学んできましたが、  
> クラスが3つ、10個、あるいは100個ある場合はどうでしょう？  
> ロジスティック回帰だけで対応できますか？」

---

## 📌 実例を挙げてみよう：

### 🐶 画像認識（3クラス）：

* 入力：1枚の画像（特徴ベクトル）  
* 出力：犬／猫／鳥 のいずれかに分類  

「これは猫かどうか？」ではなく、**どのクラスに属するか**を知りたい。

---

### 📰 ニュース記事分類（4クラス）：

* 入力：1本のニュース記事（ベクトル化済み）  
* 出力：スポーツ／テクノロジー／エンタメ／経済 のうちどれか  

---

### 🧪 故障コード識別（多クラス）：

* 入力：機器のセンサーデータ  
* 出力：故障コード 100／200／300／999 のいずれか？  

---

## ✅ 1.2 なぜロジスティック回帰だけでは不十分か？

ロジスティック回帰の出力：

$$
\hat{y} = \sigma(w^T x + b) \in (0,1)
$$

は「特定のクラスに属する確率」（例：クラス1か否か）を表し、**二値分類**にしか使えない：

* **2つのクラス**（正例／負例）のみ対応  
* 複数の排他的クラス（犬・猫・鳥など）には適さない  

---

## ❌ NGパターン：複数のロジスティック回帰を組み合わせる

「猫か？犬か？鳥か？」を個別に判定するモデルをそれぞれ作る、という方法：

| 問題点        | 説明                                          |
| ------------ | ------------------------------------------- |
| ✅ 排他性なし  | 「猫」かつ「犬」と判定されることもある            |
| 📉 確率の正規化不可 | 出力の合計が1を超えたり下回ったりする             |
| ❌ 最頻クラス選択不可 | 統一的なしきい値や基準で「最も可能性の高いクラス」を選べない |

---

## ✅ 正しいアプローチ：**複数クラスの確率を一度に出す関数**が必要

> **入力 $x$ に対し、K次元ベクトルで各クラスの確率を出力したい。**

* 期待される出力例：

  $$
  \hat{y} = [0.1,\,0.7,\,0.2]
  $$
* 出力の要件：

  * 全て非負  
  * 合計が1になる（確率分布を形成）  

ここで登場するのが **Softmax 関数**。

---

# 🎓 第2節：Softmax 関数の定義と性質

---

## ✅ 2.1 Softmax とは

> Softmax は **任意の実数ベクトル** を **確率分布** に変換する関数です。

### 📐 数学的定義

長さ $K$ の実数ベクトル $z=[z_1,\dots,z_K]$ に対し：

$$
\mathrm{Softmax}(z_i)
= \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}
\quad (i=1,\dots,K)
$$

各出力 $\hat{y}_i$ は **入力がクラス $i$ に属する確率**。

---

## ✅ 2.2 Softmax の出力特性

### 📌 特性1：各要素は非負

$$
\mathrm{Softmax}(z_i) > 0
\quad(\because e^{z_i}>0)
$$

---

### 📌 特性2：全要素の和が1（確率分布）

$$
\sum_{i=1}^K \mathrm{Softmax}(z_i) = 1
$$

出力は確率分布として解釈できる。

---

### 📌 特性3：入力差が大きいほど出力は「鋭く」なる

* ある $z_i$ が明らかに大きい場合、ほぼ one-hot に近い：

  $$
  \mathrm{Softmax}(10,0,0)\approx[0.999,\,0.0004,\,0.0004]
  $$
* 入力が均等なら平滑な分布：

  $$
  \mathrm{Softmax}(1,1,1)=[0.33,\,0.33,\,0.33]
  $$

✅ 自信が高いほど「極端な」出力になり、分類の性質に合致。

---

## ✅ 2.3 Softmax は Sigmoid の一般化

2クラスの場合：

* 入力 $z=[z_1,z_2]$  
* Softmax 出力：

  $$
  \hat{y}_1=\frac{e^{z_1}}{e^{z_1}+e^{z_2}}
  ,\quad
  \hat{y}_2=\frac{e^{z_2}}{e^{z_1}+e^{z_2}}
  $$

$z:=z_1-z_2$ とすると、

$$
\hat{y}_1=\frac{1}{1+e^{-z}}=\mathrm{Sigmoid}(z)
$$

📌 **Softmax は Sigmoid の多クラス版**。

---

## ✅ 小まとめ

| 特性   | Softmax                                             |
| ------ | --------------------------------------------------- |
| 入力   | 任意の実数ベクトル $z\in\mathbb{R}^K$                |
| 出力   | 確率分布 $\hat{y}\in(0,1)^K,\ \sum_i\hat{y}_i=1$      |
| 用途   | 多クラス分類タスク                                  |
| 比較   | Sigmoid の多クラス拡張                              |

---

# 🎓 第3節：モデル構造と交差エントロピー損失関数

---

## ✅ 3.1 モデル構造のおさらい

### 多クラス分類のネットワーク

入力特徴 $x\in\mathbb{R}^d$ から、$K$ クラスへの所属確率を予測。

### 📐 モデル式

$$
\hat{y} = \mathrm{Softmax}(W x + b)
$$

- $W\in\mathbb{R}^{K\times d}$：重み行列  
- $b\in\mathbb{R}^K$：バイアスベクトル  
- $\hat{y}\in\mathbb{R}^K$：各クラス所属確率

---

## ✅ 3.2 出力と真のラベルの差をどう測る？

モデル出力 $\hat{y}$ と真ラベル $y$ のズレを定量化する損失関数が必要。

---

## ✅ 3.3 交差エントロピー損失（Cross-Entropy Loss）とは

ラベル $y\in\mathbb{R}^K$ は **one-hot エンコーディング**：

* 正解クラスが2なら $y=[0,1,0,0]$  

Softmax 出力例 $\hat{y}=[0.1,0.5,0.2,0.2]$ とすると、  
正解クラスの確率を最大化するのが目標。

---

### ✅ 損失関数定義

$$
L = -\sum_{i=1}^K y_i \log \hat{y}_i
$$

one-hot のため、実質的に正解クラスの項のみ残る：

$$
L = -\log(\hat{y}_{\text{正解クラス}})
$$

---

### 📌 例

* 正解クラス2、$\hat{y}=[0.1,\mathbf{0.7},0.1,0.1]$  
  → $L=-\log(0.7)\approx0.357$  
* 間違った予測 $\hat{y}=[0.4,\mathbf{0.2},0.2,0.2]$  
  → $L=-\log(0.2)\approx1.609$  

正しいほど損失は小さく、間違いほど大きい。

![bg right:40% 90%](https://s.ar8.top/img/picgo/20250521185248423.webp)

---

## ✅ 3.4 なぜ交差エントロピーが適切か？

| 利点                 | 説明                                                     |
| ------------------ | ------------------------------------------------------ |
| 🎯 確率分布評価が自然   | Softmax 出力を確率とみなし、その分布差を直接測れる               |
| 📉 間違いへの単調ペナルティ | 真ラベルから外れるほど大きくペナルティを与える                      |
| ✅ 微分可能性が高い      | Softmax と組み合わせた際の勾配が単純で効率的に計算できる            |
| 🌐 最大尤度推定と等価    | 交差エントロピー最小化は尤度最大化に一致                         |

---

## ✅ 小まとめ

| 項目       | 内容                                                  |
| -------- | --------------------------------------------------- |
| 入力       | 特徴ベクトル $x\in\mathbb{R}^d$                         |
| 出力       | 確率ベクトル $\hat{y}\in\mathbb{R}^K$                    |
| モデル     | $\hat{y}=\mathrm{Softmax}(W x+b)$                   |
| ラベル     | one-hot エンコーディングベクトル                          |
| 損失関数   | 交差エントロピー：$L=-\sum_i y_i\log\hat{y}_i$        |

---

# 🎓 第4節：勾配導出とパラメータ更新

---

## ✅ 4.1 モデルおさらい

各サンプル $x_i$ に対し：

$$
\hat{y}_i=\mathrm{Softmax}(W x_i + b)
$$

交差エントロピー損失：

$$
L = -\frac{1}{n}\sum_{i=1}^n\sum_{j=1}^K y_{ij}\log\hat{y}_{ij}
$$

$Y$：真ラベル行列、$\hat{Y}$：予測行列。

---

## ✅ 4.2 重み $W$ の勾配

チェーンルールにより導出され：

$$
\nabla_W = \frac{1}{n} X^T (\hat{Y} - Y)
$$

- $X\in\mathbb{R}^{n\times d}$：入力特徴行列  
- $\hat{Y}-Y\in\mathbb{R}^{n\times K}$：予測誤差行列  

---

## ✅ 4.3 バイアス $b$ の勾配

同様に：

$$
\nabla_b = \frac{1}{n}\sum_{i=1}^n(\hat{y}_i - y_i)
\in\mathbb{R}^K
$$

---

## ✅ 4.4 勾配降下法の更新式

学習率 $\alpha$ を用い：

$$
W := W - \alpha\,\nabla_W
\quad,\quad
b := b - \alpha\,\nabla_b
$$

---

## ✅ 4.5 ロジスティック回帰との比較

| 項目       | ロジスティック回帰             | Softmax 回帰                 |
| -------- | ------------------------- | ------------------------ |
| 出力次元   | スカラー（1確率）              | ベクトル（K確率）            |
| ラベル形式 | スカラー（0/1）               | one-hot ベクトル            |
| 損失関数   | ログ損失（Log Loss）          | 交差エントロピー（Cross Entropy） |
| 勾配式     | $X^T(\hat{y}-y)$           | $X^T(\hat{Y}-Y)$          |

✅ 構造はほぼ同じで、出力と損失関数だけが拡張されている。

---

# 🎓 第5節：NumPy で実装する Softmax 多クラスモデルの学習プロセス

---


![](https://s.ar8.top/img/picgo/20250521192842701.webp)

---

### 📍 左図：Softmax 分類器の決定境界

* 背景を3色に分割：赤／青／薄色がそれぞれクラス0／1／2の予測領域  
* 点は訓練サンプル、色は真のクラス  
* 曲線は境界線、Softmax による多クラス処理を可視化

---

### 📉 右図：学習中の交差エントロピー損失の推移

* イテレーションごとに損失が減少  
* パラメータがデータにフィットしていく様子を示す

---

### ✅ 本節での実装内容

* 3クラス分離可能データの生成  
* Softmax と交差エントロピーの前向き計算  
* NumPy による勾配降下学習  
* 決定境界と損失推移の可視化

---

# 🎓 第6節：まとめと多クラス分類の拡張方向

---

## ✅ 6.1 知識回顧テーブル

| 項目         | ロジスティック回帰                  | Softmax 回帰                         |
| ------------ | ------------------------------- | --------------------------------- |
| 問題タイプ     | 二値分類                            | 多クラス分類（排他的クラス）               |
| 活性化関数     | Sigmoid                         | Softmax                           |
| 出力形式       | スカラー（確率）                      | K次元確率ベクトル                      |
| 出力範囲       | $(0,1)$                        | $(0,1)^K,\ \sum\hat{y}_i=1$         |
| 損失関数       | ログ損失（Log Loss）               | 交差エントロピー損失（Cross Entropy） |
| 予測方法       | $\hat{y}>0.5\Rightarrow1$    | $\arg\max_i\hat{y}_i$ を選択         |

---

## ✅ 6.2 Softmax の出力は「確率」と言えるか？

**はい、条件付きで**言えます。

### 🧠 理由

* 各要素が0～1  
* 全要素の和が1  

### 📌 留意点

* モデルの「相対的自信度」を示す  
* 統計的に厳密な確率ではないが、実務では確率として扱う  
* 主に最大値を取るクラスを予測し、自信度として利用

---

## ✅ 6.3 多ラベル分類への対応

ここまでの Softmax は**排他的クラス（multi-class）**向け。

* 一画像に犬か猫か、どちらかしか属せない  
* 🚫 同時に複数クラスを許容しない

### ❌ Softmax は不適

* 出力和が1になる制約がある  

### ✅ 代替手法

* 各出力ユニットに Sigmoid を適用  
* ラベルごとに二値交差エントロピーを計算（マルチラベルロス）

---

## ✅ 6.4 Softmax の応用例

| 分野            | 用途例                                 |
| -------------- | ------------------------------------ |
| 画像分類         | 複数の視覚カテゴリに属する確率を算出                |
| テキスト感情分析   | ポジティブ／ネガティブ／ニュートラル分類         |
| 音声コマンド認識   | 多種の意図を分類                           |
| OCR 文字認識     | a～z の文字ラベルを予測                     |

---

## 🧠 本講の総まとめ

* ロジスティック回帰と Softmax 回帰の違いが明確  
* Softmax の定義・性質を習得  
* 多クラス交差エントロピー損失を理解  
* NumPy による多クラス学習実装が可能  
* 多ラベル分類やニューラルネットワーク分類への拡張知見を獲得

---

**次回予告：**  
> 🔍 ニューラルネットワーク基礎（多層パーセプトロン，MLP）
