---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第19讲：循环神经网络（RNN）

---

## 一、什么是循环神经网络 (RNN)？

* **全称**：Recurrent Neural Network，循环神经网络。
* **核心目标**：处理 **序列数据**。

  * 普通神经网络（如全连接网络、卷积网络）输入和输出长度是固定的（例如一张图片就是固定大小）。
  * 但自然语言、语音、时间序列等输入长度不固定，而且有“前后依赖”的特点。
* **RNN 的思想**：在神经网络中引入 **“记忆”**，把前面步骤的信息传递给后面。

---

### 举个例子

* 假设我们要预测一句话的下一个词：

  * 输入序列："我 → 爱 → 自然 → 语言 → ..."
  * 预测下一个词。
* **普通网络**：只能看当前输入（例如“语言”），不知道前面有“自然”。
* **RNN**：会记住前面词的信息，把“自然”这个上下文传递下来，帮助预测“处理”。

所以，RNN 特别适合：

* **文本**（语言建模、机器翻译）
* **语音**（语音识别）
* **时间序列**（股价预测、传感器数据分析）

---

## 二、循环神经网络 (RNN) 的工作原理

### 2.1 和普通神经网络的区别

* 普通神经网络：输入 → 输出（一次性计算，没有记忆）。
* RNN：会 **把之前的输出“反馈”回来**，作为下一步计算的输入之一。

---

### 2.2 核心机制

* 在每一个时间步 $t$，RNN 接收：

  * 当前的输入 $x_t$（比如一个单词的向量），
  * 上一时刻的“隐藏状态” $h_{t-1}$（记忆）。
* 然后输出：

  * 当前的隐藏状态 $h_t$（新的记忆），
  * 当前的输出 $y_t$（可用于预测）。

![bg right:50% 90%](https://s.ar8.top/img/picgo/20251001182151853.webp)

---

用一句话总结：

$$
h_t = f(Wx_t + Uh_{t-1} + b)
$$

* 这里 $h_t$ 就是“记忆”，每一步都会更新。
* 就像在一条流水线上，每个节点都会带着“当前输入 + 之前的记忆”一起往下传。

---

### 2.3 直观比喻

* 想象你在读一句话：

  * 读到“我” → 记忆更新：主语是“我”。
  * 读到“爱” → 记忆更新：动作是“爱”。
  * 读到“自然” → 记忆更新：爱什么？自然。
  * 读到“语言” → 记忆更新：完整短语“自然语言”。
* 每个新词出现时，你都会结合“之前记住的内容”来理解当前词。

这就是 RNN 的精髓：**顺序处理 + 上下文记忆**。

---

## 三、RNN 结构与公式推导

### 3.1 结构直观理解

RNN 就像一条 **链条**：

* 每个时间步 $t$ 对应一个小“神经元单元”。
* 单元之间通过“隐藏状态”连接，像把前面学到的知识带到后面。

结构可以画成这样（时间展开图）：

```
x1 → [RNN cell] → h1 → y1
       ↑
x2 → [RNN cell] → h2 → y2
       ↑
x3 → [RNN cell] → h3 → y3
```

每个 cell 都是同一个结构，但输入不同（共享参数）。

---

![bg 80%](https://s.ar8.top/img/picgo/20251001182151853.webp)

---

### 3.2 公式推导

#### 输入定义

* $x_t$：时间步 $t$ 的输入向量（比如当前词的 embedding）。
* $h_t$：时间步 $t$ 的隐藏状态（记忆）。
* $y_t$：时间步 $t$ 的输出。

#### 更新规则

隐藏状态更新：

$$
h_t = \tanh(W_x x_t + W_h h_{t-1} + b_h)
$$

---

* $W_x$：输入权重矩阵
* $W_h$：隐藏层权重矩阵（处理记忆）
* $\tanh$：激活函数（非线性变换）
* $h_{t-1}$：上一时刻的隐藏状态

输出层：

$$
y_t = \text{softmax}(W_y h_t + b_y)
$$

---

#### 理解方式

* **第一步**（初始化）：
  $h_0$ 通常设为 0（没有记忆）。
* **第二步**：输入第一个词 $x_1$，算出 $h_1$。
* **第三步**：输入第二个词 $x_2$，结合 $h_1$ 算出 $h_2$。
* **第四步**：如此循环，把所有词处理完。

模型根据每个新词都会结合之前记住的信息来理解。

---

## 四、梯度消失与爆炸（RNN 最大的训练难题）

### 4.1 RNN 的特殊性

* 在普通前馈网络中，梯度传播层数有限。
* 在 RNN 中，序列可能很长，隐藏状态会不断“递归”使用同一个权重矩阵 $W_h$。
* 反向传播时，就会不断乘以这个矩阵的梯度项。

公式近似：

$$
\frac{\partial L}{\partial W_h} \propto (W_h)^t
$$

如果序列长度很长，就要连乘很多次。

---

### 4.2 什么是梯度消失？

* 如果 $|W_h| < 1$，反复相乘会让梯度趋近于 **0**。
* 结果：网络几乎学不到长期依赖，只能记住短期的上下文。
* 直观表现：模型训练时“前几个词”的影响很快就消失。

### 4.3 什么是梯度爆炸？

* 如果 $|W_h| > 1$，反复相乘会让梯度越来越大，趋向于 **无穷大**。
* 结果：训练时参数更新剧烈震荡，损失函数数值发散。

---

### 4.4 现实中的情况

* 在实际训练中，梯度消失比梯度爆炸更常见。
* 这也是为什么普通 RNN 很难捕捉长距离依赖关系（比如一篇文章开头和结尾的联系）。

### 4.5 常见解决办法

* **梯度裁剪（Gradient Clipping）**：防止爆炸，把梯度限制在一定范围。
* **更好的结构**：LSTM、GRU，通过“门机制”缓解消失问题。
* **更换激活函数**：ReLU 在某些情况下能缓解消失（但会带来稀疏性问题）。
* **正则化和初始化技巧**：合理初始化权重、批归一化。

---

## 五、常见激活函数

RNN 的隐藏状态更新需要非线性变换，否则网络的表达能力不足。常见激活函数有：

### 5.1 Sigmoid 函数

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

* 输出范围：$(0, 1)$
* 优点：平滑，输出可解释为概率。
* 缺点：容易出现梯度消失（导数最大只有 0.25，数值传播会越来越小）。

---

### 5.2 Tanh 函数

$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

* 输出范围：$(-1, 1)$
* 优点：比 Sigmoid 居中（零均值），梯度传播效果比 Sigmoid 稍好。
* 缺点：依然可能导致梯度消失。
* **RNN 默认常用的激活函数**。

---

### 5.3 ReLU 函数

$$
\text{ReLU}(x) = \max(0, x)
$$

* 输出范围：$[0, \infty)$
* 优点：不会饱和，能缓解梯度消失；计算简单。
* 缺点：可能导致“神经元死亡”（一旦输出 0，可能永远不会激活）。
* 在 RNN 里不常单独用，但在 LSTM/GRU 等变种中有时作为候选函数。

---

### 5.4 其他改进函数（简要）

* **Leaky ReLU**：解决 ReLU 死亡问题。
* **Swish/GELU**：在 Transformer/BERT 中常用，更平滑。

---

## 六、循环神经网络 (RNN) 的类型

虽然“标准 RNN”只有一种基本结构，但在不同任务里有一些常见变种和扩展：

### 6.1 单向 RNN (Unidirectional RNN)

* 信息流动方向：从左到右（或从前到后）。
* 每一步 $h_t$ 只依赖过去的输入（$x_1,...,x_t$）。
* 应用场景：语言建模（预测下一个词），时间序列预测（股价）。
* 局限：**未来信息不可见**，预测时只能依赖历史。

---

### 6.2 双向 RNN (Bidirectional RNN, Bi-RNN)

* 思路：一条 RNN 从左到右，另一条从右到左，最后拼接两个方向的隐藏状态。
* 优点：能同时利用“过去”和“未来”的上下文信息。
* 应用：

  * 命名实体识别（NER）
  * 词性标注（POS tagging）
  * 机器翻译的编码器部分
* 缺点：只能在**离线任务**中用（比如完整句子分析），不能用于实时预测。

---

### 6.3 多层 RNN (Stacked RNN)

* 思路：在时间维度展开后，再在层数上堆叠。
* 每一层的输出作为下一层的输入。
* 优点：更深的层数能捕捉更复杂的模式。
* 缺点：更容易过拟合，训练难度大。

---

### 6.4 编码器-解码器结构 (Encoder-Decoder with RNN)

* 结构：一个 RNN 作为编码器，把输入序列变成一个固定长度的向量；另一个 RNN 作为解码器，把向量解码为输出序列。
* 应用：神经机器翻译（早期的 Seq2Seq 模型）。
* 缺点：如果输入序列太长，压缩到一个固定向量会导致信息丢失。

---

## 七、循环神经网络 (RNN) 的局限性

虽然 RNN 在 2010 年代初期很流行，但它有一些重大问题，使得后续 LSTM、GRU、Transformer 等模型逐渐取代了它。

### 7.1 长期依赖问题

* RNN 理论上能记住无限长的序列，但实际上由于 **梯度消失**，只能记住短期上下文。

---

### 7.2 训练效率低

* RNN 必须 **按时间顺序逐步计算**，不能像 CNN/Transformer 那样并行处理。
* 对长序列，训练和推理速度慢。

### 7.3 难以捕捉远距离依赖

* 即使没有梯度消失问题，RNN 的隐藏状态容量有限，可能无法同时记住长句子中的所有重要信息。

---

### 7.4 计算开销大

* RNN 在序列较长时，显存占用和计算复杂度高。

### 7.5 替代者出现

* **LSTM (Long Short-Term Memory)**：通过“门机制”缓解梯度消失，能更好地捕捉长期依赖。
* **GRU (Gated Recurrent Unit)**：更简化的门控版本，计算更快。
* **Transformer**：完全放弃循环结构，靠自注意力机制并行建模长依赖，性能大幅超越 RNN。

---

## 八、实践：单词级语言模型

**目标**：给定前面的一串单词，预测下一个单词（next-word prediction）。
**做法**：

1. 文本 → 分词 → 构建词表（`stoi`/`itos`）。
2. 用 **Embedding**（可加载预训练词向量：Word2Vec/GloVe）把词ID映射为稠密向量。
3. 把序列喂给 **RNN**（或 LSTM/GRU），输出对下一个词的分布。
4. 训练：最小化交叉熵；评估：困惑度 Perplexity。
5. 推理：用模型逐词采样生成句子。
