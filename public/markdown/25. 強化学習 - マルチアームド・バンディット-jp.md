---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第25講：強化学習
# <!-- fit -->マルチアームド・バンディット（Multi-Armed Bandit）

---

# 1. マルチアームド・バンディットとは？

古典的なイメージ：

* **K 台のスロットマシン**があり、それぞれ当選確率は異なる。しかしあなたは**知らない**。
* 各ラウンドで、好きな台のレバーを 1 回引き、報酬（例：1 円または 0 円）を得る。
* 目的：限られた $T$ 回の中で、**総報酬を最大化する**こと。

重要な仮定（最も基本的なバージョン）：

* 各スロットの当選確率 $p_a$ は **固定（stationary）**
  時間や回数によって変化しない。
* これらの $p_a$ を知らないため、引いた結果（当たり/外れ）から“推定”するしかない。

---

すべての $p_a$ がわかっていれば、問題は単純：

> **最も当たりやすい台を選んで、ずっと引けばよい。**

しかし現実には、それを知らない。
「試しながら学び、学びながら稼ぐ」必要がある。
これが強化学習における最も簡略化された問題の 1 つである。

---

# 2. 数学的定式化：マルチアームド・バンディットの定義

* 腕（アクション）集合：
  $$
  \mathcal{A} = {1, 2, \dots, K}
  $$

* 各腕 $a$ は固定だが未知の当選確率 $p_a$ を持つ

* 第 $t$ ステップで腕 $a_t$ を選択し、報酬を得る：
  $$
  R_t \in {0,1},\quad R_t \sim \text{Bernoulli}(p_{a_t})
  $$

---

目的（期待総報酬の最大化）：
$$
\mathbb{E}\left[\sum_{t=1}^T R_t\right]
$$

別の言い方：**後悔（regret）**の最小化
「常に最良腕を選んだときの総報酬」−「実際の総報酬」

---

# 3. 核心的なジレンマ：探索 vs 活用（exploration vs exploitation）

この概念は RL 全体を理解するうえで極めて重要。

* **活用（Exploitation）**：
  現在の推定値に基づいて、最も良さそうな台を選ぶ → “目先の利益”

* **探索（Exploration）**：
  確信の持てない台を試すことで情報を得る → より良い台を見つける可能性

---

ジレンマ：

* 活用だけだと
  → 初期の誤判断をずっと引きずり、長期的損失が大きい。

* 探索だけだと
  → 情報は集まるが、報酬が増えない。

**マルチアームド・バンディット = 探索と活用のバランスをどう取るかの研究。**

---

# 4. 単純戦略：各腕を N 回ずつ試してから最良を選ぶ

自然に思いつくが“落とし穴”のある方法：

1. 最初に全ての腕を $N$ 回試す（純探索）
2. 各腕の平均当選率を計算
3. 残りの時間は最も平均値が高い腕だけを使う（純活用）

---

問題点：

### 問題 1：サンプルが少ないと誤判定しやすい

例えば真の確率：

* A：0.6
* B：0.4

しかし 10 回ずつ試すと：

* A：4 回成功（0.4）
* B：7 回成功（0.7）

のようなランダムな揺らぎは普通に起きる。
この誤判定をしたまま B に固定 → 長期的に大損。

---

### 問題 2：誤ったら修正できない

この戦略は「最初の一巡が終わったら探索をやめる」ため、
後から誤りを修正できない。

探索が**継続しない** → ロバスト性が低い。

---

# 5. ε-greedy：最も簡単な改良戦略

誤判定を防ぐための基本改善：

> **ε-greedy**
>
> * 大部分の時間は最良の腕（推定値が最大）を選ぶ（活用）
> * 小部分の時間はランダムに腕を選ぶ（探索）

---

具体的な方法：

各腕 $a$ の

* 選択回数 $N_a$
* 平均報酬推定 $\hat{Q}_a$

を更新しながら、

1. 確率 $\varepsilon$ で **探索（ランダムに選ぶ）**
2. 確率 $1-\varepsilon$ で **活用（最大 $\hat{Q}_a$ を選ぶ）**

$$
a_t =
\begin{cases}
\text{ランダムに選択} & \text{確率 }\varepsilon \
\arg\max_a \hat{Q}_a & \text{確率 }1-\varepsilon
\end{cases}
$$

---

平均値の更新（逐次更新）：

$$
\hat{Q}_a^{\text{new}}
= \hat{Q}_a^{\text{old}}

* \frac{1}{N_a^{\text{new}}}(R - \hat{Q}_a^{\text{old}})
  $$

直感：

* ε-greedy は **常に少しだけ探索を残す**
  → 初期誤判定を徐々に修正できる

*$\varepsilon$ が大きい → 探索多め
*$\varepsilon$ が小さい → 活用多め

---

# 6. ε-greedy の問題点：ランダム探索は“賢くない”

改善されたとはいえ、まだ幼稚な点がある。

### ① 探索が無差別

* 既に明らかに弱い腕（平均が極端に低い）も探索対象になる
* データが少なく非常に不確かな腕にも特別な優先探索がない

---

### ② “不確実性”を考慮できない

例：

* A：100 回試行、平均 0.6 → かなり確実
* B：3 回試行、平均 0.55 → ほぼ不明確

本来なら **B をより探索すべき** だが、
ε-greedy は A と B を同じ確率でランダム探索する。

---

### ③ 探索率が時間で変化しない

* 初期はもっと探索すべき
* 後期は確定情報が多いので探索を減らすべき

固定 ε では自動調整できない。

---

# 7. UCB（Upper Confidence Bound）：不確実性に基づく探索

UCB の核心思想：

> 各腕に対し、
> **平均報酬 ＋ “不確実性の補正項”**
> を計算し、これが最大の腕を選ぶ。

直感：

* 試行回数の少ない腕 → 不確実性大 → 補正が大きい → よく探索される
* 多く試した腕 → 不確実性小 → 補正が小さい → 平均がよほど良くない限り選ばれない

---

### UCB1 の公式

第 $t$ ステップにおける腕 $a$ の UCB 値：

$$
\text{UCB}_a(t)
= \hat{Q}_a(t)

* \sqrt{\frac{2\ln t}{N_a(t)}}
  $$

選択：
$$
a_t = \arg\max_a \text{UCB}_a(t)
$$

解釈：

* $\hat{Q}_a(t)$：平均報酬（活用）
* $\sqrt{\frac{2\ln t}{N_a(t)}}$：不確実性（探索）

---

### なぜ不確実性項は $\sqrt{1/N}$ の形なのか？

* 統計では、標本平均の標準偏差はだいたい $1/\sqrt{N}$
* 試すほど推定精度が上がり、不確実性が下がる
* UCB はこれを $\sqrt{\frac{\ln t}{N_a}}$ で近似

結論：

> UCB は
> “そこそこ良くて、まだあまり試されていない腕”
> を優先的に探索する。

探索が**優先度付き・目的志向**で、無駄が少ない。

---

# 8. Thompson Sampling：確率的に最も“人間らしい”方法

Thompson Sampling の発想：

> “平均値が最大の腕を決め打ちする”のではなく、
> 各腕の“**真の確率がどんな値か**”という分布を持ち、
> そこから**サンプリングして勝ちそうな腕を選ぶ**。

0/1 報酬の場合：

* 報酬モデル：$R \sim \text{Bernoulli}(p_a)$
* 信念：$p_a \sim \text{Beta}(\alpha_a, \beta_a)$

---
![](https://s.ar8.top/img/picgo/20251210193459345.webp)

---

### 1. 事前分布の初期化

各腕は最初：

$$
p_a \sim \text{Beta}(1,1)
$$

Beta(1,1) は一様分布 → “0〜1 のどこでも同じくらいあり得る”

---

### 2. 観測データで更新（事後分布）

腕 $a$ を $N_a$ 回試した結果：

* 成功回数：$S_a$
* 失敗回数：$F_a = N_a - S_a$

事後分布：

$$
p_a \mid \text{data} \sim \text{Beta}(\alpha_a, \beta_a)
$$

更新式：

$$
\alpha_a = 1 + S_a, \quad
\beta_a = 1 + F_a
$$

成功 → α を増やす
失敗 → β を増やす

---

### 3. 毎ラウンドの意思決定

第 $t$ ステップ：

1. 各腕でサンプリング：
   $$
   \tilde{p}_a^{(t)}
   \sim \text{Beta}(\alpha_a^{(t)}, \beta_a^{(t)})
   $$

2. 最大値の腕を選択：
   $$
   a_t = \arg\max_a \tilde{p}_a^{(t)}
   $$

3. 腕を引き、報酬 $R_t \in {0,1}$ を得る

4. 更新：

* $R_t = 1$ → $\alpha_{a_t} \leftarrow \alpha_{a_t} + 1$
* $R_t = 0$ → $\beta_{a_t} \leftarrow \beta_{a_t} + 1$

---

特徴：

* 不確実性の高い腕 → Beta 分布が“広く”、たまに高い値が出る → 探索されやすい
* データが増えると：

  * 分布が狭まり、不確実性が自然に減る
    → 探索量も自然に減少する

完全自動で探索と活用のバランスを調整できる。
ε の設定も UCB の式設計も不要。

