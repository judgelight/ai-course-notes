---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

## <!-- fit -->📘 第10講：サポートベクターマシン（SVM）

---

### 第1節：SVM の背景と線形分離可能な分類

#### 1.1 分類問題の振り返り

* **二値分類タスク**：特徴ベクトル $\mathbf{x}\in\mathbb R^d$ が与えられ，ラベル $y\in\{-1,+1\}$ を予測する。  
* よく使われる手法：パーセプトロン（Perceptron），ロジスティック回帰（Logistic Regression）  
  * パーセプトロンは分類の正誤のみを保証し，決定境界が不安定になりやすい。  
  * ロジスティック回帰は確率の対数損失（log-loss）を最適化する。  

---

#### 1.2 最大マージン超平面

* **問題設定**：正しく分類する線形超平面の中で，最も“頑健”なものはどれか？  
* **解答**：“幾何マージン”（geometric margin）を最大化する超平面を選ぶ。  

  * 超平面を $\mathbf w^\top \mathbf x + b = 0$ とすると，サンプル $(\mathbf x_i,y_i)$ に対する関数マージンは  
    $$y_i(\mathbf w^\top \mathbf x_i + b).$$  
  * 幾何マージンは  
    $$
      \hat\gamma_i = \frac{y_i(\mathbf w^\top \mathbf x_i + b)}{\|\mathbf w\|}.
    $$  
  * 最小の幾何マージンを最大化することで，モデルのノイズ耐性を高める。  

---

#### 1.3 サポートベクター

超平面から最も近いサンプル点を **サポートベクター** と呼ぶ。

![bg right:50%](https://s.ar8.top/img/picgo/20250709181950978.webp)

---

#### 1.4 最適マージン分類器（ハードマージン SVM）

* **最適化問題**：  
  $$
  \begin{aligned}
    &\max_{\mathbf w, b}\;\min_i\;\hat\gamma_i
    \quad\Longleftrightarrow\quad
    \min_{\mathbf w,b}\;\frac{1}{2}\|\mathbf w\|^2,\\
    &\text{s.t.}\quad y_i(\mathbf w^\top \mathbf x_i + b)\ge1,\;\forall i.
  \end{aligned}
  $$  
* 幾何的には，全サンプルが超平面から少なくとも距離 $1/\|\mathbf w\|$ を保つようにする。  

![bg right:50%](https://s.ar8.top/img/picgo/20250709181950978.webp)

---

#### 1.5 “ハードマージン SVM” 最適化目標の詳細

##### 1.5.1 「幾何マージン最大化」から「重みノルム最小化」へ

###### 1.5.1.1 幾何マージンの定義

* 超平面 $\mathbf w^\top \mathbf x + b = 0$ に対し，点 $\mathbf x_i$ からの距離は  
  $$
    d_i = \frac{\bigl|\mathbf w^\top \mathbf x_i + b\bigr|}{\|\mathbf w\|}.
  $$  
* ラベル付きサンプル $y_i\in\{+1,-1\}$ の場合，符号付き幾何マージンは  
  $$
    \hat\gamma_i 
    = y_i\;\frac{\mathbf w^\top \mathbf x_i + b}{\|\mathbf w\|}.
  $$  

---

###### 1.5.1.2 最小マージンの最大化

* 「全サンプルの幾何マージンの最小値を最大化する」ことを目指す：  
  $$
    \max_{\mathbf w,b}\;\min_{i}\;\hat\gamma_i.
  $$  
* これにより，最も境界に近い点ができるだけ離れるようになる。  

---

###### 1.5.1.3 マージンの正規化

* $(\mathbf w,b)$ を任意のスカラー $\kappa>0$ でスケーリングできる自由度を除去するため，  
  $$
    \min_i\,y_i(\mathbf w^\top \mathbf x_i + b) = 1
  $$  
  と規定する。  
* この下で，制約は  
  $$
    y_i(\mathbf w^\top \mathbf x_i + b)\ge1,\quad \forall i
  $$  
  となり，幾何マージンの最小値は $1/\|\mathbf w\|$ になる。  
* よって，$\min_i\hat\gamma_i$ の最大化は $\|\mathbf w\|^{-1}$ の最大化，すなわち $\|\mathbf w\|$ の最小化に等しい。  

---

##### 1.5.2 最終的なプライマル形式

* 計算を簡潔にするため，目的関数を $\tfrac12\|\mathbf w\|^2$ と書き直す：  
  $$
  \begin{aligned}
    &\min_{\mathbf w,b}
    \quad \frac{1}{2}\|\mathbf w\|^2,\\[6pt]
    &\text{s.t.}\quad
    y_i(\mathbf w^\top \mathbf x_i + b)\ge1,\;i=1,\dots,N.
  \end{aligned}
  $$  
* **目的**：$\|\mathbf w\|$ を小さくしてマージンを広げる。  
* **制約**：全サンプルを正しく分類し，関数マージンが 1 以上であることを保証。  

![bg right:50%](https://s.ar8.top/img/picgo/20250709181950978.webp)

---

##### 1.5.3 幾何的解釈

> **要点**：正規化後，正負両クラスの最も近いサンプルは，幅 $2/\|\mathbf w\|$ のマージン上にある。

1. **決定面**：$\mathbf w^\top \mathbf x + b = 0$  
2. **正クラス境界**：$\mathbf w^\top \mathbf x + b = +1$  
3. **負クラス境界**：$\mathbf w^\top \mathbf x + b = -1$  

* これらはすべて平行で，距離は  
  $$
    \frac{+1 - (-1)}{\|\mathbf w\|}
    = \frac{2}{\|\mathbf w\|}.
  $$  
* マージン幅はこの距離，半幅は $1/\|\mathbf w\|$。  

---

### 第2節：プライマル問題とラグランジュ双対

#### 2.1 プライマル形式

$$
\min_{\mathbf w,b}\;\frac{1}{2}\|\mathbf w\|^2
\quad
\text{s.t.}\;y_i(\mathbf w^\top \mathbf x_i + b)\ge1,\;i=1,\dots,N.
$$

#### 2.2 ラグランジュ関数の構築

* ラグランジュ乗数 $\alpha_i\ge0$ を導入して，  
  $$
  L(\mathbf w,b,\boldsymbol\alpha)
  = \frac{1}{2}\|\mathbf w\|^2 - \sum_{i=1}^N \alpha_i\bigl[y_i(\mathbf w^\top \mathbf x_i + b)-1\bigr].
  $$  

---

#### 2.3 KKT 条件と双対問題

* $\mathbf w,b$ で偏微分してゼロにすると：  
  $$
  \frac{\partial L}{\partial \mathbf w}
  = \mathbf w - \sum_i \alpha_i y_i\mathbf x_i = 0
  \;\Longrightarrow\;
  \mathbf w = \sum_i\alpha_i y_i\mathbf x_i;
  $$  
  $$
  \frac{\partial L}{\partial b}
  = -\sum_i \alpha_i y_i = 0.
  $$  

---

* これを代入して得られる双対問題：  
  $$
  \begin{aligned}
    &\max_{\boldsymbol\alpha}\;\sum_{i=1}^N \alpha_i - \frac12\sum_{i,j}\alpha_i\alpha_j y_i y_j \mathbf x_i^\top \mathbf x_j,\\
    &\text{s.t.}\;\sum_i\alpha_i y_i=0,\;\alpha_i\ge0.
  \end{aligned}
  $$  
* **サポートベクター**：$\alpha_i>0$ を持つサンプルはマージン境界上にある。  

---

#### 2.4 双対からモデル復元

* 重みは  
  $$
    \mathbf w = \sum_{i=1}^N \alpha_i\,y_i\,\mathbf x_i.
  $$  
* バイアス $b$ は，任意のサポートベクター $\mathbf x_k$ を用いて：  
  $$
  b = y_k - \sum_i\alpha_i y_i \mathbf x_i^\top \mathbf x_k.
  $$  

---

### 第3節：カーネルトリックと非線形分類

#### 3.1 非線形分離と特徴写像

* 線形分離不可能なデータには，写像 $\phi:\mathbb R^d\to\mathcal H$（高次元特徴空間）を導入し，その空間で線形分類を行う。  

#### 3.2 カーネル関数の定義

* **カーネル関数** $K(\mathbf x_i,\mathbf x_j)=\langle\phi(\mathbf x_i),\phi(\mathbf x_j)\rangle$  
* 明示的に $\phi$ を計算せずに類似度を得られるメリット。  

---

#### 3.3 代表的なカーネル

| カーネル          | 公式                                                  | 説明                         |
| ------------- | --------------------------------------------------- | -------------------------- |
| 線形核（Linear）  | $K(\mathbf x,\mathbf z)=\mathbf x^\top\mathbf z$    | 写像せず，線形 SVM に相当        |
| 多項式核（Poly）  | $(\mathbf x^\top\mathbf z + r)^d$                   | パラメータ：次数 $d$, 定数 $r$ |
| RBF（ガウス）核  | $\exp\bigl(-\gamma\|\mathbf x-\mathbf z\|^2\bigr)$  | パラメータ：$\gamma>0$        |
| シグモイド核（Sigmoid） | $\tanh(\kappa\,\mathbf x^\top\mathbf z + \theta)$ | ニューラルネット風活性化関数    |

---

### 第4節：ソフトマージン SVM（許容分類）

#### 4.1 緩和変数の導入

* 各サンプルに対し $\xi_i\ge0$ を導入し，違反を許容する。  
* 制約は  
  $$
  y_i(\mathbf w^\top \mathbf x_i + b)\ge1-\xi_i.
  $$  

#### 4.2 最適化問題

$$
\min_{\mathbf w,b,\boldsymbol\xi}
\;\frac12\|\mathbf w\|^2 + C\sum_{i=1}^N \xi_i
\quad
\text{s.t.}\;
y_i(\mathbf w^\top \mathbf x_i + b)\ge1-\xi_i,\;\xi_i\ge0.
$$

* **パラメータ $C$**：マージン幅と違反ペナルティのトレードオフ。  
  * $C$ が大きいほどハードマージンに近づき，  
  * 小さいほど誤分類を許容しやすくなる。  

---

#### 4.3 双対形式とカーネル化

* ハードマージンとの差は制約に上界 $\alpha_i\le C$ が加わる点のみ。  

$$
\boxed{
\begin{aligned}
&\max_{\boldsymbol\alpha}
\quad
\sum_{i=1}^N \alpha_i
- \tfrac12 \sum_{i,j} \alpha_i\alpha_j\,y_i y_j\,(x_i^\top x_j),\\
&\text{s.t.}\quad
\sum_i \alpha_i y_i = 0,
\quad
0 \le \alpha_i \le C,\quad i=1,\dots,N.
\end{aligned}
}
$$

---

### 第5節：多クラス SVM と実践
#### 5.1 多クラス戦略
一対他（One-vs-Rest）：各クラスに対して二値 SVM を訓練

一対一（One-vs-One）：クラス対ごとに SVM を訓練し，合計 $K(K-1)/2$ 個を構築

scikit-learn のデフォルトは One-vs-Rest を採用。

#### 5.2 scikit-learn 実装例

```python
from sklearn import datasets
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import classification_report, ConfusionMatrixDisplay

# データ読み込み
X, y = datasets.load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# ハイパーパラメータ探索
param_grid = [
    {'kernel': ['linear'], 'C': [0.1, 1, 10]},
    {'kernel': ['rbf'], 'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10]},
]
grid = GridSearchCV(SVC(), param_grid, cv=5)
grid.fit(X_train, y_train)
print("Best params:", grid.best_params_)

# 評価
y_pred = grid.predict(X_test)
print(classification_report(y_test, y_pred))
ConfusionMatrixDisplay.from_estimator(grid, X_test, y_test)
```