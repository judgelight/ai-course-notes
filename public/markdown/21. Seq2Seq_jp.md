---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第21講：Seq2Seq（エンコーダ–デコーダ フレームワーク）

---

## 一、Seq2Seqとは？

### 1.1 提案の背景

* 従来の RNN / LSTM は「系列 → 単一出力」しか扱えない。
  * 例：感情分類、音声認識ラベル。
* しかし多くのタスクは **入力も出力も系列** である：
  * **機械翻訳**：英語文 → 日本語文  
  * **対話生成**：質問 → 応答  
  * **音声認識**：音声特徴列 → 文字列
* 「入力系列の長さ ≠ 出力系列の長さ」という問題を解決するため、  
  2014年に Google が **Seq2Seq（Sequence to Sequence）** モデルを提案した。

---

### 1.2 コアアイデア

* **2つの RNN（または LSTM / GRU）** を使用：
  * **Encoder（エンコーダ）**：入力系列を読み込み、固定長の「意味ベクトル」に圧縮。
  * **Decoder（デコーダ）**：そのベクトルに基づき、出力系列を1語ずつ生成。
* 全体構造は次のように表される：
  $$
  X = (x_1, x_2, \dots, x_T) \Rightarrow Y = (y_1, y_2, \dots, y_{T'})
  $$
  入力と出力の長さは異なってよい。

---

## 二、Seq2Seq の構造と動作原理

### 2.1 モデル構造

1. **Encoder（エンコーダ）**  
   * 入力系列 $x_1, x_2, \dots, x_T$ を順に処理  
   * 各ステップで隠れ状態 $h_t$ を出力  
   * 最終隠れ状態 $h_T$ が文全体の「意味的要約」として用いられる。

2. **Decoder（デコーダ）**  
   * 初期入力はエンコーダの出力 $h_T$  
   * その後、逐次的に出力単語を予測：
     $$
     y_t = f(y_{t-1}, s_{t-1}, h_T)
     $$

---

### 2.2 Encoder の計算過程

各タイムステップで：

$$
h_t = f(h_{t-1}, x_t)
$$

LSTMの場合：

$$
h_t, C_t = \text{LSTM}(x_t, (h_{t-1}, C_{t-1}))
$$

最終的な出力：

* 最後の隠れ状態 $h_T$
* または全時刻の状態系列 $(h_1, h_2, ..., h_T)$

---

### 2.3 Decoder の計算過程

デコーダも RNN / LSTM であり、目的は：

* 前時刻の出力単語 $y_{t-1}$（またはその埋め込み）を入力し、
* 前の隠れ状態 $s_{t-1}$ と組み合わせ、
* 現在の出力 $y_t$ を生成すること。

$$
s_t = f(s_{t-1}, y_{t-1}, c)
$$
$$
P(y_t | y_{<t}, X) = \text{Softmax}(W \cdot s_t + b)
$$

ここで $c$ はエンコーダから渡される **コンテキストベクトル（context vector）**。

---

## 三、学習と推論のプロセス

### 3.1 学習段階（Teacher Forcing）

* **入力：**
  * ソース言語文 $(x_1, ..., x_T)$
  * ターゲット言語文 $(y_1, ..., y_{T'})$
* **目的：**
  * 各時刻で正しい次単語の確率を最大化：
    $$
    \max \sum_t \log P(y_t | y_{<t}, X)
    $$
* **Teacher Forcing 手法：**
  * 学習時、デコーダへの入力には「モデル生成語」ではなく **正解の前単語** を使う。

---

### 3.2 推論段階（Inference）

* 推論時は正解データが存在しない。
* したがって **自己回帰生成（autoregressive generation）** を行う：

  1. エンコーダでコンテキストベクトル $c$ を得る  
  2. 最初の入力として `<SOS>`（文開始トークン）を与える  
  3. デコーダが最初の語 $y_1$ を生成  
  4. $y_1$ を次の入力として $y_2$ を生成  
  5. `<EOS>`（文終了）を出力するまで繰り返す  

---

### 3.3 ビームサーチ（Beam Search）

* 各時刻で最も確率の高い語だけ選ぶ（貪欲法）では品質が悪化することがある。
* そこで、複数の候補文を同時に保持（ビーム幅）。
* 最終的に **全体確率が最大** の系列を選択する。

---

## 四、数理モデル化

### 4.1 全体確率モデル

Seq2Seq は次の条件付き確率を学習する：

$$
P(Y|X) = \prod_{t=1}^{T'} P(y_t | y_{<t}, X)
$$

各条件確率はデコーダの Softmax 出力で計算。

---

### 4.2 損失関数

* 一般的に **交差エントロピー損失（Cross Entropy Loss）** を使用：
  $$
  L = - \sum_{t=1}^{T'} \log P(y_t^{(true)} | y_{<t}, X)
  $$
* これは「正しい文の対数尤度の最小化」に相当。

---

## 五、Seq2Seq の改良と発展

### 5.1 問題①：情報圧縮のボトルネック

* エンコーダは全文を固定長ベクトル $c$ に圧縮。
* 長文では情報が失われやすい。
* 結果として、デコーダが前半の文脈を「忘れる」。

---

### 5.2 Attention 機構の導入（2015）

* Bahdanau らによる **Attention Seq2Seq** の提案。
* 改善点：
  * デコーダは最後の隠れ状態だけに依存しない。
  * 代わりにエンコーダの全隠れ状態の加重和を使用：
    $$
    c_t = \sum_i \alpha_{t,i} h_i
    $$
  * 重み $\alpha_{t,i}$ は注意スコア関数で計算：
    $$
    \alpha_{t,i} = \text{Softmax}(s_t^\top W_a h_i)
    $$

* 意味：モデルが各出力語を生成する際、「入力文の異なる部分」に注目できる。

---

### 5.3 双方向エンコーダ

* エンコーダに **双方向 LSTM（Bi-LSTM）** を用いる：
  * 一方は左から右（順方向）
  * もう一方は右から左（逆方向）
* 双方向の状態を結合：
  $$
  h_i = [\overrightarrow{h_i}; \overleftarrow{h_i}]
  $$

---

### 5.4 Copy & Coverage 機構（さらなる改良）

* **Copy 機構**：入力文中の単語を直接コピーして出力できる（未知語対策）。  
* **Coverage 機構**：既に注目した入力を追跡し、重複生成を防ぐ。

---

## 六、Seq2Seq の応用分野

### 6.1 機械翻訳（Neural Machine Translation）

* Seq2Seq の原点的応用。
* エンコーダがソース文（英語など）を読み、デコーダがターゲット文（日本語など）を生成。
* 例：  
  * 入力："I love you"  
  * 出力："私はあなたを愛している"

---

### 6.2 テキスト要約（Text Summarization）

* 入力：長文  
* 出力：要約文  
* モデルは主要情報を抽出するよう学習。  
* 注意機構（Attention）が非常に重要。

---

### 6.3 対話システム（Chatbot）

* 入力：ユーザの質問または前の発話  
* 出力：応答文  
* Encoder–Decoder 構造は初期のチャットボットの中核であった。

---

### 6.4 音声認識 / 音声→テキスト

* 入力：音声特徴（MFCCフレーム列）
* 出力：テキスト文字列
* Attention や CTC と組み合わせて、エンドツーエンド認識を実現。

---

## 七、Seq2Seq と Transformer の関係

### 7.1 Transformer の登場（2017）

* Transformer は本質的に **Seq2Seq の一形態**：
  * Encoder–Decoder 構造をそのまま維持しつつ、
  * RNN/LSTM を **Self-Attention** に置き換えた。
* 長所：
  * 並列計算が可能
  * 長距離依存を強くモデリング
  * 大規模モデル（BERT・GPT）へ拡張容易

---

### 7.2 比較まとめ

| 項目 | RNN/LSTM Seq2Seq | Transformer |
|------|------------------|--------------|
| 基盤構造 | 再帰（逐次） | 注意（並列） |
| 学習速度 | 遅い | 速い |
| 長距離依存 | 弱い | 強い |
| 可視化性 | ゲート構造の解釈 | 注意重みの可視化 |
| 主流時代 | 2014〜2017 | 2018以降 |

---

## 八、Seq2Seq の意義と影響

* **「入力も出力も系列」問題を統一的に解決**。
* **機械翻訳・要約・音声認識・対話生成** の基礎を築いた。
* **Attention 機構・Transformer** の誕生のきっかけとなった。
* 現在も軽量タスクや教育目的で広く利用されている。

---

## 九、まとめ

| モジュール | 機能 | コア概念 |
|-------------|------|-----------|
| Encoder | 入力系列を符号化 | 意味をコンテキストベクトルに圧縮 |
| Decoder | 出力系列を生成 | 逐次的に目標文を生成 |
| Attention | 長文情報損失を改善 | 動的に入力の異なる部分に注目 |
| Seq2Seq の意義 | 入出力系列の統一枠組み | 近代的生成モデルの基礎 |

---

## 十、実践の方向性

* **最小限の Seq2Seq モデル（PyTorch）を実装**
  * データ：英中文ペアまたは数値翻訳（例："one two" → "一 二"）
  * モデル：Encoder + Decoder（LSTM使用）
  * 損失関数：交差エントロピー
  * 学習戦略：Teacher Forcing + Beam Search
* 拡張案：
  * Attention 機構の追加
  * 双方向エンコーダの導入
  * Word2Vec / GloVe などの単語埋め込み使用
