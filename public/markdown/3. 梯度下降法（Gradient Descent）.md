---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

## <!-- fit -->📘 第3讲：梯度下降法（Gradient Descent）

---

## ✅ 1. 场景引入：从“下山”讲起

### 🧠 引导问题：

> “假设你站在一座山的山顶，天气太差看不到周围，你该怎么走才能尽快走到山脚下？”

---

### 🎯 引入类比：

- 你处在一个**复杂的地形函数**上，每个点都有一个“高度”。
- 你手里没有地图，但你能用脚感知“哪里更陡”。
- 所以你采取的策略是：
  - **感知当前位置的坡度方向（导数）**
  - **沿最陡的方向往下走一点**
  - **不断重复**

---

### 📌 类比转化：

| 登山问题 | 机器学习问题 |
|----------|----------------|
| 山的高度 | 损失函数值 |
| 当前所在点 | 当前模型参数（如 $w, b$）|
| 往下走 | 减少损失 |
| 坡度方向 | 梯度 |
| 每次迈出步 | 参数更新 |
| 下山成功 | 找到最优解（最小损失） |

![bg right:45% 100%](https://s.ar8.top/img/picgo/20250429225534445.webp)

---

### 🔑 小结：

> “这就是**梯度下降法**的基本思想：**让模型顺着损失函数的梯度不断‘下降’，直到找到最优解。**”

---

## ✅ 2. 什么是“梯度”？

---

### ✨ 2.1 在一维空间中（导数）

- 如果你在学微积分时学过导数：
   $$
    f'(x) = \frac{d}{dx} f(x)
   $$
  - 它表示函数在某一点的“斜率”。
- 梯度下降中的“梯度”就是导数：
  - 如果函数是 $f(x) = (x-3)^2$，那么：
   $$
    f'(x) = 2(x - 3)
   $$
    当前 $x$ 越大，斜率越大
- 你要做的就是 **沿着负斜率方向更新 $x$**，逐渐靠近最低点。

---

### 🧭 一维梯度下降过程：

- 初始值 $x_0 = 0$
- 学习率 $\alpha = 0.1$
- 更新公式：
 $$
  x_{t+1} = x_t - \alpha \cdot f'(x_t)
 $$

| t | $x_t$ | $f'(x_t)$ | $x_{t+1}$ |
|---|-----------|----------------|----------------|
| 0 | 0         | -6             | 0.6            |
| 1 | 0.6       | -4.8           | 1.08           |
| 2 | 1.08      | -3.84          | 1.464          |
| … | …         | …              | …              |

✅ 最终收敛到 $x = 3$（最小值点）

---

### 📐 2.2 在多维空间中（向量梯度）

如果我们有一个函数：

$$
f(w, b) = \frac{1}{n} \sum_{i=1}^{n} (y_i - (wx_i + b))^2
$$

- $w, b$ 是我们要优化的“位置”
- 梯度是一个**向量**，每个维度都告诉我们“这一方向上的最陡上升方向”

多维更新公式：

$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \alpha \cdot \nabla_{\boldsymbol{\theta}} f(\boldsymbol{\theta}_t)
$$

其中 $\boldsymbol{\theta} = [w, b]$

---

### 📌 梯度下降的直观理解


对$\theta$赋值，使得$f(\boldsymbol{\theta}_t)$按梯度下降最快方向进行，一直迭代下去，最终得到局部最小值。其中$\alpha$是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。
对于这个问题，求导的目的，基本上可以说取这个红点的切线，就是这样一条红色的直线，刚好与函数相切于这一点，让我们看看这条红色直线的斜率，就是这条刚好与函数曲线相切的这条直线，这条直线的斜率正好是这个三角形的高度除以这个水平长度，现在，这条线有一个正斜率，也就是说它有正导数，因此，我得到的新的$\theta_1$，$\theta_1$更新后等于$\theta_1$减去一个正数乘以$\alpha$。

![50% 90%](https://s.ar8.top/img/picgo/20250430000057416.webp)


---

### ✅ 小结：

> “梯度是告诉我们该往哪走的指南针，梯度下降是根据这个指南针，走向损失函数最小值的过程。”

---


# ✅ 3. 梯度下降的工作机制

### 🧭 3.1 梯度下降流程图（5 步）

我们用伪代码/图解方式展示完整流程：

```
1. 初始化参数（如 w, b）
2. 重复执行以下步骤，直到收敛：
   a. 使用当前参数计算预测值 y_hat
   b. 计算损失函数 L(w, b)
   c. 对参数求导，得到梯度 ∇L
   d. 按梯度的反方向更新参数
```

---

### 📉 3.2 数学更新规则（以线性回归为例）

#### 假设模型：

$$
\hat{y}_i = wx_i + b
$$

#### 损失函数（均方误差）：

$$
L(w, b) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
= \frac{1}{n} \sum_{i=1}^{n} (y_i - wx_i - b)^2
$$

---

#### 求导（偏导）：

$$
\frac{\partial L}{\partial w} = -\frac{2}{n} \sum x_i (y_i - \hat{y}_i)
$$
$$
\frac{\partial L}{\partial b} = -\frac{2}{n} \sum (y_i - \hat{y}_i)
$$

---

#### 更新规则：

$$
w := w - \alpha \cdot \frac{\partial L}{\partial w}
\quad\quad
b := b - \alpha \cdot \frac{\partial L}{\partial b}
$$

其中：

- $\alpha$：学习率（learning rate），控制每一步“走多少”
- $n$：样本数量

---

### 📌 3.3 超参数的作用

#### 🔧 学习率 $\alpha$

- 太小：收敛太慢，要走很多步
- 太大：可能震荡甚至发散（跳过最优点）

✅ 最佳做法：**适中，并动态调整或使用学习率衰减**

---

#### 🔁 迭代次数（Epoch）

- 每一个 epoch = 用所有样本完成一次参数更新
- 通常循环多次（几十、几百甚至几千次）直到：
  - 收敛（loss 变化很小）
  - 达到最大迭代次数

---

### 🧠 3.4 梯度下降的直觉总结

> 梯度下降就像“闭着眼睛沿斜坡走向谷底”的过程：

- 梯度告诉你方向（陡坡）
- 学习率决定你每次迈多远
- 多次走动之后，你越来越靠近最低点

---

# ✅ 4. 梯度下降在线性回归中的实例应用（手算一轮）

一元线性回归方程
$$
\hat{y}_i = wx_i + b
$$

给定初始：

- $w = 0$, $b = 0$
- 单个样本 $(x=2, y=10)$
- 学习率 $\alpha = 0.1$

计算预测值：

$$
\hat{y} = 0 \cdot 2 + 0 = 0
$$

---

计算误差：

$$
e = y - \hat{y} = 10 - 0 = 10
$$

计算梯度：

$$
\frac{\partial L}{\partial w} = -2x \cdot e = -2 \cdot 2 \cdot 10 = -40
\quad\Rightarrow\quad w := 0 - 0.1 \cdot (-40) = 4.0
$$

$$
\frac{\partial L}{\partial b} = -2 \cdot e = -20
\quad\Rightarrow\quad b := 0 - 0.1 \cdot (-20) = 2.0
$$

✅ 一次更新后，$w = 4, b = 2$，模型已逼近正确结果！

---


## 📌 小结

| 内容 | 要点 |
|------|------|
| 损失函数 | 最小化 $(y - \hat{y})^2$ |
| 梯度 | 是误差对参数的导数 |
| 更新规则 | $\theta := \theta - \alpha \cdot \nabla_\theta$ |
| 优势 | 不需要矩阵求逆，适合大数据、大特征量 |

---

# ✅ 第 5 部分：使用 NumPy 实现梯度下降求解线性回归

## 🧩 模拟数据（训练数据）

我们使用一个简单的线性数据集（面积 + 楼层 → 房价）：
外部参考

---

## 📌 总结

| 项目 | 说明 |
|------|------|
| 梯度计算 | 是关键步骤：通过误差反向传播 |
| 权重更新 | 根据梯度反方向、乘学习率调整 |
| 收敛判断 | 可通过损失是否趋于稳定来观察 |
| 可解释性 | 模型学出的每个权重都有明确物理意义 |

---

**下一节预告：**
> 🔍 逻辑回归问题