---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->ğŸ“˜ ç¬¬24è®²ï¼šTransformer æ¨¡å‹ï¼ˆTransformer Modelï¼‰

---

## ä¸€ã€èƒŒæ™¯ä¸åŠ¨æœº

### 1.1 ä» RNN åˆ° Transformer

* RNN / LSTM ç»“æ„çš„ç¼ºé™·ï¼š
  * **è®¡ç®—æ— æ³•å¹¶è¡Œ**ï¼ˆåºåˆ—ä¾èµ–ï¼‰ï¼›
  * **é•¿è·ç¦»ä¾èµ–éš¾æ•æ‰**ï¼›
  * **è®­ç»ƒé€Ÿåº¦æ…¢ï¼Œéš¾æ‰©å±•**ã€‚

* 2017 å¹´ï¼ŒGoogle æå‡ºäº†é©å‘½æ€§æ¶æ„ï¼š
  > **Transformer: Attention Is All You Need**

* æ ¸å¿ƒæ€æƒ³ï¼šå®Œå…¨æŠ›å¼ƒå¾ªç¯ç»“æ„ï¼Œä»…ä¾èµ– **Attention æœºåˆ¶**ã€‚

---

### 1.2 Transformer çš„æ ¸å¿ƒæ€æƒ³

* ä½¿ç”¨ **Self-Attention** å»ºæ¨¡åºåˆ—å†…éƒ¨çš„ä¾èµ–ï¼›
* ä½¿ç”¨ **å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰** å­¦ä¹ ä¸åŒè¯­ä¹‰å…³ç³»ï¼›
* ä½¿ç”¨ **ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰** è®©æ¨¡å‹å…·å¤‡é¡ºåºæ„Ÿï¼›
* é€šè¿‡å †å  Encoderâ€“Decoder å±‚ï¼Œæ„å»ºæ·±å±‚ç»“æ„ã€‚

---

## äºŒã€Transformer çš„æ•´ä½“æ¶æ„

### 2.1 æ¨¡å‹ç»„æˆ

Transformer ç”±ä¸¤å¤§éƒ¨åˆ†ç»„æˆï¼š

1. **Encoderï¼ˆç¼–ç å™¨ï¼‰**
   * è¾“å…¥å¥å­ â†’ ç”Ÿæˆä¸Šä¸‹æ–‡è¡¨ç¤ºï¼›
2. **Decoderï¼ˆè§£ç å™¨ï¼‰**
   * åˆ©ç”¨ç¼–ç å™¨è¾“å‡º â†’ é€æ­¥ç”Ÿæˆç›®æ ‡åºåˆ—ã€‚

---

![](https://s.ar8.top/img/picgo/20251126184516285.webp)

---

### 2.2 æ¯ä¸ªæ¨¡å—çš„ç»“æ„

| æ¨¡å— | ç»„æˆéƒ¨åˆ† | å…³é”®æœºåˆ¶ |
|------|-----------|----------|
| Encoder | Self-Attention + Feed Forward | å¹¶è¡Œå¤„ç†è¾“å…¥åºåˆ— |
| Decoder | Masked Self-Attention + Encoderâ€“Decoder Attention + Feed Forward | ç”Ÿæˆè¾“å‡ºåºåˆ— |

---

### 2.3 ç›´è§‚ç†è§£

* **Encoder**ï¼šè¯»å®Œæ•´ä¸ªå¥å­ï¼Œç†è§£æ•´ä½“è¯­ä¹‰ï¼›
* **Decoder**ï¼šåœ¨ç”Ÿæˆæ¯ä¸ªè¯æ—¶ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶â€œå‚è€ƒâ€è¾“å…¥ï¼›
* æ•´ä¸ªè¿‡ç¨‹ä¸å†ä¾èµ–æ—¶é—´é¡ºåºï¼Œè€Œæ˜¯ä¾èµ–æ³¨æ„åŠ›æƒé‡çŸ©é˜µã€‚

---

## ä¸‰ã€Self-Attentionï¼ˆè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼‰

### 3.1 åŸºæœ¬æ€æƒ³

> è®©åºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½èƒ½â€œçœ‹åˆ°â€å…¶ä»–æ‰€æœ‰å…ƒç´ ï¼Œ  
> å¹¶æ ¹æ®ç›¸å…³æ€§è°ƒæ•´è‡ªèº«è¡¨ç¤ºã€‚

---

### 3.2 è¾“å…¥ä¸ä¸‰ä¸ªå‘é‡

å¯¹æ¯ä¸ªè¾“å…¥å‘é‡ $x_i$ï¼Œç”Ÿæˆä¸‰ç»„æŠ•å½±ï¼š

$$
Q_i = W_Q x_i, \quad K_i = W_K x_i, \quad V_i = W_V x_i
$$

* **Query (Q)**ï¼šæŸ¥è¯¢å‘é‡  
* **Key (K)**ï¼šé”®å‘é‡  
* **Value (V)**ï¼šå€¼å‘é‡

---

### 3.3 æ³¨æ„åŠ›è®¡ç®—å…¬å¼

$$
\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{Q K^\top}{\sqrt{d_k}}\right)V
$$

è§£é‡Šï¼š

1. $QK^\top$ï¼šè®¡ç®—æ‰€æœ‰è¯é—´ç›¸ä¼¼åº¦ï¼›
2. $\sqrt{d_k}$ï¼šç¼©æ”¾å› å­ï¼Œé¿å…æ¢¯åº¦è¿‡å¤§ï¼›
3. Softmaxï¼šå½’ä¸€åŒ–ä¸ºæƒé‡ï¼›
4. åŠ æƒæ±‚å’Œå¾—åˆ°æ–°çš„è¡¨ç¤ºã€‚

---

### 3.4 ç›´è§‚ç†è§£

* $Q$ æ˜¯å½“å‰è¯çš„é—®é¢˜ï¼›  
* $K$ æ˜¯æ‰€æœ‰è¯çš„çº¿ç´¢ï¼›  
* $V$ æ˜¯è¦å–çš„ä¿¡æ¯ï¼›  
* æ³¨æ„åŠ›æœºåˆ¶è®©æ¨¡å‹ **â€œå†³å®šè¯¥çœ‹è°â€**ã€‚

---

### 3.5 ç¤ºä¾‹ï¼šå¥å­å†…éƒ¨çš„ä¾èµ–

å¥å­ï¼š
> "The animal didn't cross the street because it was too tired."

æ¨¡å‹å­¦ä¹ åˆ°ï¼š
* â€œitâ€ çš„æ³¨æ„åŠ›é‡ç‚¹è½åœ¨ â€œanimalâ€ï¼›
* è€Œä¸æ˜¯ â€œstreetâ€ï¼›
â†’ è¡¨æ˜æ¨¡å‹å…·å¤‡äº†**è¯­ä¹‰å¯¹é½**èƒ½åŠ›ã€‚

---

## å››ã€å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰

### 4.1 ä¸ºä»€ä¹ˆè¦å¤šå¤´ï¼Ÿ

* å•ä¸€æ³¨æ„åŠ›å¤´åªèƒ½å­¦ä¹ ä¸€ç§è¯­ä¹‰å…³ç³»ï¼›
* å¤šå¤´æœºåˆ¶å…è®¸æ¨¡å‹å¹¶è¡Œå­¦ä¹ ä¸åŒç±»å‹çš„ä¾èµ–ã€‚

---

### 4.2 å…¬å¼å®šä¹‰

$$
\text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)
$$

$$
\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O
$$

* $h$ï¼šæ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼ˆå¦‚ 8 ä¸ªï¼‰ï¼›
* æ¯ä¸ªå¤´æœ‰ç‹¬ç«‹å‚æ•°ï¼›
* æ‹¼æ¥åå†çº¿æ€§å˜æ¢ã€‚

---

### 4.3 å¤šå¤´çš„æ•ˆæœ

æ¯ä¸ªæ³¨æ„åŠ›å¤´å­¦ä¹ ä¸åŒçš„ä¿¡æ¯ï¼š
| å¤´ç¼–å· | å…³æ³¨æ–¹å‘ | å­¦ä¹ å…³ç³» |
|--------|-----------|-----------|
| Head 1 | åŠ¨è¯ä¸ä¸»è¯­ | è¯­æ³•ç»“æ„ |
| Head 2 | ä»£è¯ä¸å…ˆè¡Œè¯ | æŒ‡ä»£å…³ç³» |
| Head 3 | ä¿®é¥°è¯ä¸åè¯ | è¯­ä¹‰ä¿®é¥° |
| Head 4 | ä¸Šä¸‹æ–‡æ—¶æ€ | è¯­ä¹‰ä¸€è‡´æ€§ |

â†’ å¤šå¤´ = å¤šè§†è§’ç†è§£ã€‚

---

## äº”ã€ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰

### 5.1 ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ï¼Ÿ

* Transformer æ²¡æœ‰å¾ªç¯ç»“æ„ï¼Œæ— æ³•æ„ŸçŸ¥é¡ºåºï¼›
* æˆ‘ä»¬å¿…é¡»ä¸ºæ¯ä¸ªè¯æ³¨å…¥â€œä½ç½®ä¿¡æ¯â€ã€‚

---

### 5.2 æ­£å¼¦ä½ç½®ç¼–ç ï¼ˆSine & Cosineï¼‰

ä½¿ç”¨å›ºå®šå…¬å¼è®¡ç®—æ¯ä¸ªä½ç½®çš„å‘é‡ï¼š

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

* ä¸åŒç»´åº¦æœ‰ä¸åŒçš„å‘¨æœŸï¼›
* ä½¿æ¨¡å‹èƒ½å­¦åˆ°ç›¸å¯¹ä¸ç»å¯¹ä½ç½®å…³ç³»ã€‚

---

## å…­ã€Transformer çš„ç»“æ„ç»†èŠ‚

### 6.1 Encoder å±‚ç»“æ„

æ¯å±‚åŒ…å«ï¼š
1. Multi-Head Self-Attention  
2. Add & LayerNorm  
3. Feed Forwardï¼ˆä¸¤å±‚å…¨è¿æ¥ï¼‰

è¾“å‡ºï¼š
$$
H_l = \text{LayerNorm}(H_{l-1} + \text{SelfAttention}(H_{l-1}))
$$
$$
H_l = \text{LayerNorm}(H_l + \text{FFN}(H_l))
$$

---

### 6.2 Decoder å±‚ç»“æ„

åŒ…å«ä¸‰ä¸ªå­å±‚ï¼š
1. Masked Self-Attentionï¼ˆé˜²æ­¢çœ‹åˆ°æœªæ¥è¯ï¼‰  
2. Encoderâ€“Decoder Attention  
3. Feed Forward + æ®‹å·®è¿æ¥ + LayerNorm  

---

### 6.3 æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–

* **Residual Connection**ï¼š  
  é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼ŒåŠ é€Ÿæ”¶æ•›ã€‚
  $$
  x' = \text{LayerNorm}(x + \text{SubLayer}(x))
  $$
* **Layer Normalization**ï¼š  
  å¯¹æ¯ä¸ªæ ·æœ¬å†…éƒ¨çš„ç‰¹å¾ç»´åº¦åšå½’ä¸€åŒ–ï¼Œç¨³å®šè®­ç»ƒã€‚

---

## ä¸ƒã€å®è·µï¼šç”¨ Transformer è®­ç»ƒè¯­è¨€æ¨¡å‹

### 7.1 å®éªŒç›®æ ‡

* ä»»åŠ¡ï¼šé¢„æµ‹å¥å­ä¸­ä¸‹ä¸€ä¸ªè¯  
  â†’ å³ **è¯­è¨€å»ºæ¨¡ï¼ˆLanguage Modelingï¼‰**  
* æ•°æ®é›†ï¼šå°å‹è¯­æ–™ï¼ˆå¦‚ WikiText2ï¼‰  
* æ¨¡å‹ç»“æ„ï¼šç®€åŒ–ç‰ˆ Transformer Encoder

---

### 7.2 æ¨¡å‹ç»“æ„ç®€è¿°

1. **è¾“å…¥å±‚**ï¼šè¯åµŒå…¥ + ä½ç½®ç¼–ç   
2. **Encoder å±‚**ï¼šè‹¥å¹²å±‚ Multi-Head Self-Attention + FFN  
3. **è¾“å‡ºå±‚**ï¼šSoftmax é¢„æµ‹ä¸‹ä¸€ä¸ªè¯

---

### 7.3 PyTorch å®ç°ç¤ºä¾‹

```python
import torch
import torch.nn as nn
from torch.nn import TransformerEncoder, TransformerEncoderLayer

d_model = 256
nhead = 8
num_layers = 4
dim_feedforward = 512

encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward)
transformer = TransformerEncoder(encoder_layer, num_layers=num_layers)

x = torch.rand(20, 32, d_model)  # (seq_len, batch, dim)
out = transformer(x)
print(out.shape)  # [20, 32, 256]
````

---

### 7.4 è¯­è¨€å»ºæ¨¡æŸå¤±å‡½æ•°

$$
L = -\sum_t \log P(w_t | w_{<t})
$$

å³é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„äº¤å‰ç†µæŸå¤±ã€‚
Transformer å¯ä»¥åŒæ—¶è®¡ç®—æ‰€æœ‰ä½ç½®ï¼ˆå¹¶è¡ŒåŒ–è®­ç»ƒï¼‰ã€‚

---

### 7.5 å®è·µè¦ç‚¹

* è®­ç»ƒæ•°æ®éœ€æŒ‰åºæ’åˆ—ï¼›
* ä½¿ç”¨ Mask ç¡®ä¿é¢„æµ‹ä»…ä¾èµ–è¿‡å»è¯ï¼›
* å¯åŠ å…¥ Dropout é˜²æ­¢è¿‡æ‹Ÿåˆï¼›
* å­¦ä¹ ç‡è°ƒåº¦å™¨å¸¸é‡‡ç”¨ â€œWarmup + Cosine Decayâ€ã€‚

---

### 7.6 æ€§èƒ½æå‡ç‚¹

* å¢åŠ å±‚æ•°å’Œå¤´æ•° â†’ æ•æ‰æ›´å¤æ‚è¯­ä¹‰ï¼›
* ä½¿ç”¨æ›´å¤§è¯­æ–™ â†’ å­¦ä¹ è¯­è¨€è§„å¾‹ï¼›
* å¼•å…¥é¢„è®­ç»ƒï¼ˆå¦‚ GPT/BERTï¼‰ â†’ æ³›åŒ–æ›´å¼ºã€‚

---

## å…«ã€Transformer çš„ä¼˜åŠ¿ä¸å½±å“

### 8.1 ä¼˜ç‚¹

* å®Œå…¨å¹¶è¡ŒåŒ–è®­ç»ƒï¼›
* é•¿è·ç¦»ä¾èµ–æ•æ‰èƒ½åŠ›å¼ºï¼›
* å¯æ‰©å±•åˆ°è¶…å¤§è§„æ¨¡ï¼›
* å¯è§£é‡Šæ€§å¼ºï¼ˆå¯è§†åŒ–æ³¨æ„åŠ›ï¼‰ã€‚

---

### 8.2 å±€é™æ€§

* åºåˆ—é•¿åº¦é™åˆ¶ï¼ˆè®¡ç®—é‡ $O(n^2)$ï¼‰ï¼›
* ç¼ºä¹å¾ªç¯å¼æ—¶é—´æ„Ÿï¼›
* å‚æ•°é‡å·¨å¤§ï¼Œè®­ç»ƒæˆæœ¬é«˜ã€‚

---

### 8.3 åç»­æ”¹è¿›æ–¹å‘

| æ¨¡å‹                   | æ”¹è¿›ç‚¹                   |
| -------------------- | --------------------- |
| Transformer-XL       | å¼•å…¥å¾ªç¯è®°å¿†ï¼Œå»ºæ¨¡æ›´é•¿ä¸Šä¸‹æ–‡        |
| Longformer / BigBird | ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œé™ä½å¤æ‚åº¦         |
| Performer            | ä½¿ç”¨æ ¸å‡½æ•°è¿‘ä¼¼ï¼Œçº¿æ€§å¤æ‚åº¦         |
| GPT ç³»åˆ—               | ä»…ä¿ç•™ Decoder ç»“æ„ï¼Œç”¨äºç”Ÿæˆä»»åŠ¡ |

---

## ä¹ã€Transformer çš„æ„ä¹‰

* **å½»åº•æ”¹å˜äº† NLP ä¸ AI çš„ç ”ç©¶èŒƒå¼ã€‚**
* ä»æ­¤è¿›å…¥â€œå¤§æ¨¡å‹æ—¶ä»£â€ï¼š

  * BERTã€GPTã€T5ã€ViTã€Whisper ç­‰çš†åŸºäº Transformerã€‚
* æˆä¸ºè·¨é¢†åŸŸç»Ÿä¸€æ¶æ„ï¼ˆæ–‡æœ¬ã€å›¾åƒã€è¯­éŸ³ï¼‰ã€‚

---

## åã€æ€»ç»“

| æ¦‚å¿µ                   | è¯´æ˜           |
| -------------------- | ------------ |
| Self-Attention       | å»ºæ¨¡åºåˆ—å†…å…ƒç´ é—´å…³ç³»   |
| Multi-Head Attention | å¹¶è¡Œå­¦ä¹ å¤šç§ä¾èµ–æ¨¡å¼   |
| Positional Encoding  | æ³¨å…¥ä½ç½®ä¿¡æ¯ä»¥æ„ŸçŸ¥é¡ºåº  |
| Feed Forward         | éçº¿æ€§å˜æ¢å±‚       |
| Encoderâ€“Decoder      | å¹¶è¡Œè¾“å…¥ä¸è‡ªå›å½’è¾“å‡ºç»“æ„ |
| æ ¸å¿ƒä¼˜åŠ¿             | å¹¶è¡Œã€é«˜æ•ˆã€é•¿ç¨‹ä¾èµ–æ•æ‰ |

