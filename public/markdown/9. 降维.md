---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

## <!-- fit -->📘 第9讲：降维

---


## 降维技术

### 为什么要降维

* **维度诅咒**：高维空间距离失效、计算量大
* **可视化**：大于 3 维的数据只能靠降维可视化
* **加速 & 去噪**：去除冗余特征，提升后续建模效果

---

## 🔍 1. PCA 的直观目标

* **降维**：将原始的高维数据映射到一个低维子空间，同时尽量保留数据的“关键信息”——即样本的方差（变异性）。
* **可视化**：常用 2D、3D 投影观察数据结构、类别分布。
* **去噪**：丢弃小方差方向上的噪声成分。

---

## 📐 2. 数学原理与推导

1. **数据标准化**

   $$
   \tilde X = \frac{X - \mu}{\sigma}
   $$

   * 对每个特征减去均值 $\mu$，再除以标准差 $\sigma$，保证各维度同等尺度。

2. **计算协方差矩阵**
   对于标准化后样本矩阵 $\tilde X \in \mathbb{R}^{n\times d}$，协方差矩阵为

   $$
   C = \frac{1}{n-1}\,\tilde X^\top \tilde X 
   \in \mathbb{R}^{d\times d}.
   $$

---

3. **特征分解（Eigen decomposition）**
   求协方差矩阵的**特征值**$\lambda_1,\dots,\lambda_d$ 及对应的**特征向量**（单位向量）$\mathbf{v}_1,\dots,\mathbf{v}_d$，满足

   $$
   C\,\mathbf{v}_i = \lambda_i\,\mathbf{v}_i,
   \quad \lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_d \ge 0.
   $$

4. **选取主成分**

   * 第 $k$ 个主成分方向即对应 **第 $k$ 大特征值** $\lambda_k$ 的特征向量 $\mathbf{v}_k$。
   * 前 $m$ 个主成分构成映射矩阵

     $$
     V_m = [\mathbf{v}_1,\dots,\mathbf{v}_m] \in \mathbb{R}^{d\times m}.
     $$

---

5. **投影**
   将原始标准化数据 $\tilde X$ 投影到主成分子空间

   $$
   Z = \tilde X \;V_m \quad (\in \mathbb{R}^{n\times m}).
   $$

6. **方差贡献率**
   第 $k$ 个主成分保留的方差比例

   $$
   \text{ExplainedRatio}_k
   = \frac{\lambda_k}{\sum_{i=1}^d \lambda_i}.
   $$

   累计前 $m$ 个主成分的方差贡献率
   $\sum_{k=1}^m \text{ExplainedRatio}_k$，用于判断降维后信息保留程度。

---

**协方差矩阵（Covariance Matrix）**

1. **概念回顾**
   对于一个有 $d$ 个特征的样本集，我们常常希望知道各特征之间的“线性相关程度”。

   * **协方差** $\mathrm{Cov}(X_i, X_j)$ 描述的是第 $i$ 个和第 $j$ 个特征同时增大或减小的趋势：

     $$
       \mathrm{Cov}(X_i, X_j)
       = \mathbb{E}\bigl[(X_i - \mu_i)(X_j - \mu_j)\bigr]
       = \frac1{n-1}\sum_{k=1}^n (x_{k,i}-\mu_i)(x_{k,j}-\mu_j).
     $$
   * 当协方差为正时，两个特征“同向”变化；为负时，则“反向”变化；为零时，线性无关（不一定完全独立）。

---

2. **矩阵形式**
   把所有特征两两的协方差组织成一个 $d\times d$ 的方阵，就是**协方差矩阵**

   $$
     C =
     \begin{pmatrix}
       \mathrm{Cov}(X_1,X_1) & \mathrm{Cov}(X_1,X_2) & \cdots & \mathrm{Cov}(X_1,X_d) \\[6pt]
       \mathrm{Cov}(X_2,X_1) & \mathrm{Cov}(X_2,X_2) & \cdots & \mathrm{Cov}(X_2,X_d) \\[3pt]
       \vdots & \vdots & \ddots & \vdots \\[3pt]
       \mathrm{Cov}(X_d,X_1) & \mathrm{Cov}(X_d,X_2) & \cdots & \mathrm{Cov}(X_d,X_d)
     \end{pmatrix}.
   $$

   * 对角线元素是各特征自身的方差。
   * 对称矩阵：$\mathrm{Cov}(X_i,X_j)=\mathrm{Cov}(X_j,X_i)$。

---

**特征分解（Eigen Decomposition）**

1. **目标**
   我们希望找出协方差矩阵 $C$ 的“自然轴”（principal axes），即数据在什么方向上的变动最大，从而把高维数据投影到这些方向上，完成降维。

2. **数学形式**
   协方差矩阵 $C$ 可以被分解为

   $$
     C\,\mathbf{v}_i = \lambda_i\,\mathbf{v}_i,
     \quad i = 1,2,\dots,d
   $$

   其中

   * $\lambda_i$ 称为**特征值**（eigenvalue），标量。
   * $\mathbf{v}_i$ 称为对应的**特征向量**（eigenvector），是长度为 $d$ 的向量。
   * 特征向量是单位向量，且不同特征向量之间正交（相互垂直）。

---

3. **含义**

   * **特征向量 $\mathbf{v}_i$**：指出了数据在该方向上的主轴（某一维度组合方向）。
   * **特征值 $\lambda_i$**：表示在该方向上数据的“方差大小”（能量大小）。值越大，数据在这个方向上的投影分散得越开，信息量也就越多。

4. **排序与投影**

   * 按 $\lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_d$ 将特征向量排序，取前 $m$ 个最大的特征值对应的向量，形成投影矩阵

     $$
       V_m = [\mathbf{v}_1,\dots,\mathbf{v}_m].
     $$
   * 把原始数据 $X$ 投影到这些向量上：

     $$
       Z = X\,V_m,
     $$

     就得到了 $m$ 维的新表示，同时保留了最多的方差信息。

---


## 💻 3. 实战：Iris 数据集 PCA 代码示例

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# 1. 加载数据
data = load_iris()
X, y = data.data, data.target
target_names = data.target_names

# 2. 数据标准化
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

# 3. PCA 降到 2 维
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_std)

# 4. 查看方差贡献率
explained = pca.explained_variance_ratio_
print(f"PC1 explains {explained[0]*100:.1f}% variance")
print(f"PC2 explains {explained[1]*100:.1f}% variance")
print(f"Total explained: {(explained.sum())*100:.1f}%")

# 5. 可视化投影
plt.figure(figsize=(6,5))
for i, color in zip(range(3), ['r','g','b']):
    plt.scatter(X_pca[y==i, 0], X_pca[y==i, 1],
                label=target_names[i], alpha=0.7, edgecolor='k')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('Iris Dataset PCA Projection')
plt.legend()
plt.grid(True)
plt.show()

# 6. 方差贡献率柱状图
plt.figure(figsize=(6,4))
components = np.arange(1, len(pca.explained_variance_ratio_)+1)
plt.bar(components, pca.explained_variance_ratio_ * 100, alpha=0.7)
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance (%)')
plt.title('PCA Explained Variance Ratio')
plt.xticks(components)
plt.grid(axis='y')
plt.show()
```

---

## 📊 4. 结果解读

* **投影图**

  * 可以看到三种鸢尾花在前两个主成分空间中的分布：

    * Setosa（红）明显分离
    * Versicolor（绿）和 Virginica（蓝）有部分重叠
* **方差贡献率**

  * PC1 ≈ 72.9%、PC2 ≈ 23.0%，累计 95.9%
  * 前两维几乎保留了原数据 96% 的信息，可将数据从 4 维降到 2 维而丢失很少。

---

## ✅ 5. 小结与实践建议

1. **何时用 PCA**

   * 高维数据需可视化时
   * 特征冗余、想做预处理降噪时
   * 提前为聚类、分类、回归等建模降维

2. **注意事项**

   * PCA 假设方向上的方差最大即信息最重要，不适合方差小但信息关键的场景。
   * 强烈建议先**标准化**，否则数值大特征主导主成分。

---

3. **扩展**

   * **核PCA**：引入核函数处理非线性
   * **Sparse PCA**：对特征进行稀疏选择
   * **增量 PCA**：用于大规模数据

---

# 🎓  t-SNE 可视化技术详解

---

## 1. t-SNE 的目标与应用场景

### 1.1 什么是 t-SNE？

* **t-SNE**（t-分布随机邻域嵌入，t-Distributed Stochastic Neighbor Embedding）是一种非线性降维技术，常用于将高维数据映射到低维空间（2D/3D），以便可视化数据的**局部结构**。
* **目标**：通过在低维空间中尽量保留原始数据点的相对相似性，来发现数据中的簇结构。

### 1.2 适用场景

* **可视化**：在 2D 或 3D 平面上显示数据簇和类别，帮助理解数据的内在结构。
* **聚类**：对高维数据进行降维后，观察簇的分布情况。
* **流形学习**：揭示数据是否呈现流形结构，如手写数字、图像等。

---

## 2. t-SNE 算法核心原理

t-SNE 的目标是把高维数据中的局部结构映射到低维空间，通过最小化高维空间与低维空间的相似度差异来学习低维表示。

### 2.1 高维空间的相似度（条件概率）

首先，计算高维空间中每对点的“相似度”，使用**条件概率**来表示：

* 给定点 $x_i$，我们用条件概率 $p_{j|i}$ 表示 $x_j$ 在 $x_i$ 的邻域中的概率：

  $$
  p_{j|i} = \frac{\exp\big(-\|x_i - x_j\|^2 / (2\sigma_i^2)\big)}
             {\sum_{k\neq i} \exp\big(-\|x_i - x_k\|^2 / (2\sigma_i^2)\big)}.
  $$

  * $\|x_i - x_j\|^2$ 是欧氏距离的平方，表示样本之间的距离。
  * $\sigma_i$ 是由**Perplexity**计算得来的参数，用来控制点 $x_i$ 的邻域范围。

---

### 2.2 对称化联合概率

为了保证数据对称性，定义高维空间的**对称联合概率**：

$$
p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}.
$$

* 这样，$p_{ij}$ 满足 $p_{ij} = p_{ji}$，同时 $p_{ij}$ 约束为概率分布。

### 2.3 低维空间的相似度（学生 t 分布）

在低维空间中，t-SNE 使用**学生 t 分布**（自由度 1）来表示点 $y_i$ 和 $y_j$ 的相似度：

$$
q_{ij} = \frac{(1 + \|y_i - y_j\|^2)^{-1}}{\sum_{k \neq \ell} (1 + \|y_k - y_\ell\|^2)^{-1}}.
$$

* 使用 t 分布的原因是它具有更长的尾部，可以更好地处理数据点之间的“局部”拥挤问题。

---

### 2.4 最小化 KL 散度

为了将高维空间的相似度尽可能映射到低维空间，t-SNE 最小化高维空间与低维空间的**Kullback-Leibler（KL）散度**：

$$
C = \mathrm{KL}(P \parallel Q) = \sum_{i \neq j} p_{ij} \log\frac{p_{ij}}{q_{ij}}.
$$

* $C$ 越小，表示高维和低维空间的分布越相似，局部结构得到更好的保留。

### 2.5 梯度下降法

通过**梯度下降**来最小化 KL 散度，更新低维空间的点 $y_i$：

$$
\frac{\partial C}{\partial y_i} = 4 \sum_j (p_{ij} - q_{ij})(y_i - y_j) \big(1 + \|y_i - y_j\|^2 \big)^{-1}.
$$

* 使用这个公式来计算每个点的梯度，进而更新其位置，直到损失函数收敛。

---

## 3. t-SNE 参数与调优

### 3.1 Perplexity（困惑度）

* **Perplexity** 是控制每个点邻域大小的超参数。它控制的是每个点相似度的“信息量”。
* **较小的 Perplexity** 值意味着邻域较小，捕捉局部结构；**较大的 Perplexity** 会考虑更大范围的邻域，捕捉更广泛的信息。

### 3.2 学习率（Learning Rate）

* t-SNE 使用梯度下降来优化目标函数。学习率过大可能导致震荡，过小则可能收敛缓慢。
* 一般学习率初始设为 200，但根据数据集的不同，可能需要调节。

---

### 3.3 初始坐标

* t-SNE 会随机初始化低维坐标，或者使用 PCA 进行初始化，帮助加速收敛。

### 3.4 迭代次数

* 一般来说，t-SNE 在 1000 次迭代内即可获得不错的结果，但更多的迭代有助于细化聚类边界。

---

## 4. t-SNE 的优势与局限

### 4.1 优势

* **非线性降维**：能够捕捉高维数据中的复杂结构。
* **适用于可视化**：对类别分明或局部结构明显的数据，t-SNE 可以非常有效地进行 2D 或 3D 映射，便于观察和分析。

---

### 4.2 局限

* **计算复杂度高**：原始的 t-SNE 算法的时间复杂度为 $O(n^2)$，对大规模数据集计算较慢。
* **不保留全局结构**：t-SNE 更注重保留数据的局部结构，可能忽略全局距离或簇间关系。
* **随机性**：由于随机初始化和梯度下降的非确定性，t-SNE 结果可能在不同运行中有所不同。

---

## 5. 实战：MNIST 数据集 t-SNE 可视化

接下来，我们通过实际代码示例，使用 **MNIST** 手写数字数据集来演示 t-SNE 的使用和可视化。

### 5.1 加载 MNIST 数据

```python
from sklearn.datasets import fetch_openml
import numpy as np

# 加载 MNIST 数据集
mnist = fetch_openml('mnist_784', version=1)
X = mnist.data / 255.0  # 标准化数据
y = mnist.target.astype(int)

# 随机选择 1000 个样本
np.random.seed(42)
indices = np.random.choice(len(X), 1000, replace=False)
X_sample, y_sample = X.iloc[indices], y.iloc[indices]
```

### 5.2 t-SNE 降维与可视化

```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# 使用 t-SNE 降到 2 维
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X_sample)

# 可视化 t-SNE 结果
plt.figure(figsize=(8, 6))
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_sample, cmap='tab10', s=10, alpha=0.7)
plt.title("t-SNE Visualization of MNIST")
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.colorbar()
plt.show()
```
