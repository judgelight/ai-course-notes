---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第26讲：马尔可夫决策过程（MDP）

---

## 一句话理解马尔可夫决策过程（MDP）

**马尔可夫决策过程（Markov Decision Process，简称 MDP）**
是一种用来描述「**在不确定环境中，持续做决策以获得长期最大收益**」的数学模型。

它回答的是这样一类问题：

> “我现在该怎么做，才能让未来整体结果最好？”

---

## 一个直观的生活例子

想象你在玩一款**角色扮演游戏**：

* 你当前有一定 **血量、金币、等级**
* 每一回合你可以选择：
  * 打怪
  * 买装备
  * 休息回血
* 不同选择会：
  * 立刻带来收益或损失
  * 同时改变你**下一步的状态**

---

* 游戏中还存在随机性：
  * 打怪不一定成功
  * 掉落装备不确定

你的目标不是“这一回合分数最高”，而是：

> **尽可能活得久、变得强、通关游戏**

这个“**在当前状态下选择最优行动，以最大化长期收益**”的问题，本质上就是一个 **MDP**。

---

## MDP 由哪几部分组成？

一个完整的马尔可夫决策过程，只有 **5 个核心要素**。

### 1️⃣ 状态（State）

**状态 = 对当前情况的完整描述**

例如：

* 游戏中：血量、等级、金币
* 机器人中：位置、速度、电量
* 推荐系统中：用户历史行为、当前页面

关键点：

> **状态必须“足够完整”，只看当前状态，就能决定接下来会发生什么**

---

### 2️⃣ 动作（Action）

**动作 = 在某个状态下你可以做的选择**

例如：

* 向左走 / 向右走
* 买 / 不买
* 打怪 / 逃跑

在不同状态下，可选动作可能不同。

---

### 3️⃣ 状态转移（Transition）

**状态转移 = 你做了某个动作后，环境如何变化**

重要特点：

* 通常是**有概率的**
* 同一个动作，结果不一定相同

例如：

* 打怪成功概率 70%，失败 30%
* 下雨的概率取决于今天的天气

这就是“不确定性”。

---

### 4️⃣ 奖励（Reward）

**奖励 = 每一步得到的即时反馈**

可以是：

* 分数
* 金钱
* 正负数（惩罚）

例如：

* 打败怪物 +10
* 掉血 −5
* 撞墙 −100

关键点： **奖励是短期的，但我们关心的是长期累计奖励**

---

### 5️⃣ 策略（Policy）

**策略 = 告诉你“在某种状态下该做什么”的规则**

例如：

* 血量低于 30% 就回血
* 资金充足就投资
* 天气不好就不出门

策略本质上是一个映射：

> **状态 → 动作**

---

## 什么叫“马尔可夫”？

“马尔可夫”指的是一个非常重要的假设：

> **未来只和“现在的状态”有关，和更久以前的历史无关**

换句话说：

* 如果你已经知道“当前状态”
* 那么过去是怎么走到这里的，不再重要

这并不是说“过去没发生”，而是说：

> **当前状态已经包含了所有对未来有用的信息**

---

## MDP 真正想解决的核心问题

MDP 的目标不是：

* “下一步拿最多奖励”

而是：

> **找到一个策略，让长期累计奖励最大**

这也是为什么：

* 有时要“暂时吃亏”
* 才能换来未来更大的收益

---

## 一个反直觉但重要的点

举个例子：

* 现在可以拿 +10 分
* 或者现在拿 0 分，但未来每一步 +5 分

短视策略会选 +10
**最优策略**可能会选择暂时不拿分数

MDP 正是用来解决这种“**短期 vs 长期**”权衡的问题。

---

## 为什么 MDP 很重要？

因为现实世界的大量问题，都符合这个结构：

* 强化学习
* 自动驾驶
* 机器人控制
* 游戏 AI
* 资源调度
* 推荐系统
* 投资决策

---

只要问题满足：

* 有状态
* 有选择
* 有不确定性
* 追求长期目标

**它几乎一定可以被建模为 MDP。**

---

## 总结

> 马尔可夫决策过程是一种数学框架，用来描述智能体在不确定环境中，根据当前状态选择行动，并通过最大化长期累计奖励来做出最优决策。

---

## 0. 先把 MDP 场景固定下来

我们有：

* **状态** $(s)$：当前所处情况
* **动作** $(a)$：可选决策
* **转移**：做了 $(a)$ 后到下一个状态 $(s')$（可能随机）
* **奖励** $(r)$：每一步环境给你的即时反馈
* **策略** $(\pi(a|s))$：在状态 $(s)$ 下选择动作 $(a)$ 的规则（可带概率）

我们的任务是：**找到一个策略 $(\pi)$，让长期收益最大。**

---

## 1. 从“即时奖励”到“长期回报”

很多问题的关键在于：
**眼前奖励不重要，重要的是“长期累计”效果。**

因此我们定义“回报（Return）”：

> 从某一时刻开始，未来所有奖励的加总（通常会打折扣）。

---

常见写法：

$$
[
G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + ...
]
$$

这里的 $(\gamma)$ 是 **折扣因子**（0~1）：

* $(\gamma)$ 越小：越看重短期
* $(\gamma)$ 越大：越看重长期

直观理解：
**未来越远的奖励，影响越弱**（也能保证数学上不发散）。

---

## 2. “价值”到底是什么：用来衡量状态/动作的长期好坏

有了回报 $(G_t)$，下一步就自然要问：

> “如果我现在处于某个状态，它长期来看值不值得？”

于是定义 **状态价值函数**：

### 2.1 状态价值 $(V^\pi(s))$

$$
[
V^\pi(s) = \mathbb{E}_\pi[ G_t \mid s_t=s ]
]
$$

解释：
在策略 $(\pi)$ 下，如果当前在状态 $(s)$，未来回报的**期望值**是多少。

它是一个“好坏评分”：

* 值越大，说明这个状态在该策略下越“有前途”。

---

### 2.2 动作价值 $(Q^\pi(s,a))$

光知道“状态值”还不够，我们还需要知道：

> “在状态 $(s)$ 下，选哪个动作更好？”

于是定义 **动作价值函数**：

$$
[
Q^\pi(s,a) = \mathbb{E}_\pi[ G_t \mid s_t=s, a_t=a ]
]
$$

解释：
在策略 $(\pi)$ 下，处于 $(s)$ 并执行动作 $(a)$，未来回报期望是多少。

直觉上：

* $(V^\pi(s))$ 是“这个位置整体如何”
* $(Q^\pi(s,a))$ 是“在这个位置做某个选择如何”

---

## 3. 价值怎么“算出来”：贝尔曼方程（递推关系）

价值函数之所以可计算，是因为它满足一个非常关键的递推：

> **当前价值 = 当前奖励 + 未来价值（折扣后）的期望**

这就是 **贝尔曼期望方程（Bellman Expectation Equation）**。

---

### 3.1 对状态价值

$$
[
V^\pi(s)=\sum_a \pi(a|s)\sum_{s'} P(s'|s,a)\bigl[R(s,a,s')+\gamma V^\pi(s')\bigr]
]
$$

含义是：

* 在状态 $(s)$，你按策略 $(\pi)$ 选择动作 $(a)$
* 环境按概率转移到 $(s')$
* 你获得即时奖励 $(R)$
* 未来继续累计 $(V^\pi(s'))$，再乘折扣 $(\gamma)$

它给了我们一个“**用下一步的价值反推当前价值**”的方法。

---

## 4. 有了价值，如何得到更好的策略：贪心改进

如果我们已经知道 $(Q^\pi(s,a))$，那在每个状态下显然应该选：

$$
[
\pi'(s)=\arg\max_a Q^\pi(s,a)
]
$$

意思是：
**在每个状态都选让长期回报最大的动作。**

这一步叫：**策略改进（Policy Improvement）**。

关键结论（很重要）：

> 用 $(Q^\pi)$ 做贪心改进，得到的新策略 $(\pi')$ 不会比 $(\pi)$ 更差，通常更好。

---

## 5. 反复两步：策略评估 + 策略改进 → 逐步逼近最优

于是我们得到一个经典套路：

### 5.1 策略迭代（Policy Iteration）

循环执行：

1. **策略评估**：给定 $(\pi)$，求 $(V^\pi)$ 或 $(Q^\pi)$
2. **策略改进**：用 $(\arg\max_a Q^\pi(s,a))$ 得到更优策略 $(\pi')$

不断重复，直到策略不再变化。

---

直觉上像是：

* 先评估：这套“打法”到底长期收益如何
* 再改进：根据评估结果把打法升级
* 再评估，再升级……

最终收敛到最优策略 (\pi^*)。

---

## 6. 最优价值与最优策略：最终目标

定义最优状态价值：

$$
[
V^*(s) = \max_\pi V^\pi(s)
]
$$

最优动作价值：

$$
[
Q^*(s,a) = \max_\pi Q^\pi(s,a)
]
$$

最优策略就是：

$$
[
\pi^*(s)=\arg\max_a Q^*(s,a)
]
$$

这就是“最优策略”的形式：
**在每个状态选择最优动作价值最大的动作。**

---

## 7. 一个最重要的现实问题：我们通常不知道转移概率和奖励模型

上面推导看起来很“数学”，但它依赖一个前提：

* 你知道 $(P(s'|s,a))$ 和 $(R(s,a,s'))$

现实中通常不知道。怎么办？

答案就是强化学习常说的两条路线：

* **基于模型（Model-based）**：先学出 $(P,R)$，再做规划
* **无模型（Model-free）**：不学模型，直接从交互经验估计 $(V/Q)$

其中最经典的一条无模型路径就是：**用采样更新价值**（例如 TD、Q-learning、SARSA）。

---

## 8. 流程总结

在 MDP 中，我们每走一步会拿到即时奖励，但真正关心的是未来奖励的累计回报。为了衡量“一个状态或一个动作长期是否值得”，我们定义价值函数 $(V)$ 和 $(Q)$。价值函数满足贝尔曼递推关系，因此可被计算或估计。然后我们用“在每个状态选 $(Q)$ 最大的动作”来改进策略。不断重复“评估策略→改进策略”，就能一步步逼近最优策略。
