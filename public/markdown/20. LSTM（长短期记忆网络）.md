---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第20讲：LSTM（长短期记忆网络）

---

## 一、什么是 LSTM？

### 1.1 提出背景

* 我们已经知道 **普通 RNN** 在处理长序列时会出现 **梯度消失**：

  * 前面很久之前的信息，几乎无法影响后面。
  * 就像你读一段话，刚开始的句子到最后几乎记不住。
* 1997 年，Sepp Hochreiter 和 Jürgen Schmidhuber 提出了 **LSTM (Long Short-Term Memory)**，就是为了 **解决 RNN 难以捕捉长期依赖** 的问题。

---

### 1.2 为什么要发明 LSTM？

* 普通 RNN 只有一个隐藏状态 ( $h_t$ )，它要同时“存储信息”和“决定输出”，压力太大。
* LSTM 在 RNN 的基础上，增加了一个新的“存储器”：

  * **记忆单元（cell state）**：像一条传送带，可以长距离携带信息。
  * 通过“门”的机制来控制信息的进出。
* 这样，网络能保留需要的长期信息，而把无关信息遗忘掉。

---

### 1.3 直观理解（带阀门的记忆单元）

* 把 LSTM 想象成一个 **水管**，水流代表信息：

  * **遗忘门** = 阀门，决定“哪些旧水要放掉”。
  * **输入门** = 阀门，决定“哪些新水要注入”。
  * **输出门** = 阀门，决定“最后流出去多少水”。
* 整个过程就像一个“会选择性记忆和遗忘”的管道系统。
* 这就是 LSTM 相比普通 RNN 的最大改进：**有了长期记忆 + 有选择地读写信息**。

---

## 二、LSTM 的工作原理

### 2.1 核心结构：记忆单元 (Cell State)

* **Cell State**：像一条传送带，贯穿整个时间序列。
* 在传播过程中，可以携带关键信息，不容易丢失。
* 通过门的机制，LSTM 可以 **保留有用信息，丢弃无用信息**。

---

![bg 90%](https://s.ar8.top/img/picgo/20251007233132986.webp)

---

### 2.2 三个“门”的设计

在每一个时间步，LSTM 有三个主要的门（gate）：

1. **遗忘门 (Forget Gate)**

   * 决定“丢弃多少旧信息”。
   * 输入：当前输入 ( $x_t$ )，上一步隐藏状态 ( $h_{t-1}$ )。
   * 输出：0 到 1 之间的数（通过 **Sigmoid**）。
   * 例子：

     * “昨天去超市买水果，今天吃了一个___。”
     * 遗忘门会让“去超市”这种信息逐渐淡化。

---

2. **输入门 (Input Gate)**

   * 决定“加入多少新信息”。
   * 包括两部分：

     1. 输入门控制开关（Sigmoid）。
     2. 候选信息（通过 Tanh 产生）。
   * 例子：

     * “今天早上我吃了一个苹果。”
     * 输入门会让“苹果”这个新信息写进记忆单元。

---

3. **输出门 (Output Gate)**

   * 决定“从记忆单元输出多少信息，作为当前的隐藏状态”。
   * 例子：

     * 当我们预测下一个词时，输出门会决定让多少记忆影响预测。

---

## 三、LSTM 的结构与公式推导

### 3.1 每个时间步的计算流程

在时间步 ($t$)，LSTM 接收两个输入：

* 当前输入 ($x_t$)（比如当前词的 $embedding$）
* 上一时刻的隐藏状态 ($h_{t-1}$) 和记忆单元状态 ($C_{t-1}4$)

然后依次执行以下操作：

1. **遗忘门 (Forget Gate)**：决定丢弃多少旧信息
2. **输入门 (Input Gate)**：决定加入多少新信息
3. **候选记忆 (Candidate Memory)**：生成新的候选信息
4. **更新记忆 (Update Cell State)**：把遗忘和新信息结合，更新记忆单元
5. **输出门 (Output Gate)**：决定输出多少信息作为新的隐藏状态

---

### 3.2 公式逐步展开

#### ① 遗忘门（Forget Gate）

$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$

* 输入：前一隐藏状态 ($h_{t-1}$) + 当前输入 ($x_t$)
* 输出：一个 0 到 1 的向量（Sigmoid）
* 含义：1 = 完全保留，0 = 完全丢弃

---

#### ② 输入门（Input Gate）

$$
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
$$

* 控制“哪些新信息需要写入记忆单元”。
* 1 = 完全写入，0 = 不写入。

---

#### ③ 候选记忆（Candidate Memory）

$$
\tilde{C}*t = \tanh(W_C \cdot [h*{t-1}, x_t] + b_C)
$$

* 生成新的候选信息（范围在 -1 到 1）。
* 和输入门配合：只有输入门允许的部分才会被写入记忆单元。

---

#### ④ 更新记忆单元（Cell State）

$$
C_t = f_t \ast C_{t-1} + i_t \ast \tilde{C}_t
$$

* 把旧记忆 ($C_{t-1}$) 经过遗忘门过滤后留下部分。
* 再加上新的候选记忆（由输入门控制）。
* 最终得到更新后的记忆单元 ($C_t$)。
* **这是 LSTM 的核心：信息像“流水线”一样传递，既能保留过去，也能注入新知识。**

---

#### ⑤ 输出门（Output Gate）与隐藏状态

$$
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
$$
$$
h_t = o_t \ast \tanh(C_t)
$$

* 输出门决定要“露出”多少记忆内容。
* 最终隐藏状态 ($h_t$) = 输出门控制下的记忆内容。
* ($h_t$) 会传给下一个时间步，也可作为当前时刻的输出。

---

### 3.3 对比 RNN 的单一公式

* **普通 RNN**：
$$
  h_t = \tanh(W_x x_t + W_h h_{t-1} + b)
$$

  * 没有专门的机制来“遗忘”或“记忆”。
  * 信息只能隐含在 ($h_t$) 里，容易丢失。

* **LSTM**：

  * 额外引入了记忆单元 ($C_t$)。
  * 通过 **遗忘门**、**输入门**、**输出门** 精细地控制信息流动。
  * 能有效解决梯度消失问题，捕捉长期依赖。

---

## 四、LSTM 的直观理解与可视化

### 4.1 LSTM 的关键要素回顾

* **隐藏状态 $h_t$**：当前时刻的输出表示，用来和外部交互。
* **记忆单元 $C_t$**：长期记忆的“存储器”，可以跨越多个时间步。
* **门（Gate）**：通过 Sigmoid 函数输出 $0 \sim 1$，控制信息流量：

  * 遗忘门 $f_t$：要不要忘掉旧记忆
  * 输入门 $i_t$：要不要写入新信息
  * 输出门 $o_t$：要不要输出记忆

---

### 4.2 公式整体回顾（整合视角）

1. **遗忘门**
   $$
   f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
   $$

2. **输入门 + 候选记忆**
   $$
   i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
   $$
   $$
   \tilde{C}*t = \tanh(W_C \cdot [h*{t-1}, x_t] + b_C)
   $$

3. **更新记忆单元**
   $$
   C_t = f_t \ast C_{t-1} + i_t \ast \tilde{C}_t
   $$

4. **输出门与隐藏状态**
   $$
   o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
   $$
   $$
   h_t = o_t \ast \tanh(C_t)
   $$

---

### 4.3 直观比喻（管道与阀门模型）

把 **记忆单元 $C_t$** 想象成一条贯穿整条序列的“水管”：

* **遗忘门 $f_t$** = 阀门，决定旧水流要不要排掉（遗忘无关信息）。
* **输入门 $i_t$ + 候选记忆 $\tilde{C}_t$** = 阀门 + 新水，决定哪些新水注入水管（写入新知识）。
* **输出门 $o_t$** = 阀门，决定有多少水流出来，形成当前的隐藏状态 $h_t$（供下一步使用）。

这样，LSTM 就像一个“会记忆和遗忘的水管系统”：

* 不重要的信息逐渐被冲掉
* 重要的新信息注入
* 输出时选择性地释放部分信息

---

### 4.4 可视化结构（逻辑图）

LSTM 单元结构常见图示：
![120%](https://s.ar8.top/img/picgo/20251007233132986.webp)

---

### 4.5 总结

* LSTM 通过 **记忆单元 $C_t$** 来维持长期依赖。
* **三个门**（遗忘、输入、输出）像阀门一样控制信息流动：

  * 遗忘旧信息
  * 写入新信息
  * 输出有用信息
* 这种机制使 LSTM 能在长序列中保留关键信息，避免 RNN 的梯度消失问题。

---

## 五、LSTM 的变体

虽然标准 LSTM 已经比 RNN 强大很多，但在实践中，人们根据不同需求对它进行了多种改进。这里介绍几种常见的变体。

---

### 5.1 标准 LSTM

* 我们前面介绍的就是 **标准 LSTM**，包括 **遗忘门 $f_t$、输入门 $i_t$、候选记忆 $\tilde{C}_t$、输出门 $o_t$**。
* 特点：

  * 能捕捉长期依赖
  * 训练比普通 RNN 稳定
  * 但计算量和参数量比 RNN 大得多

---

### 5.2 Peephole LSTM（窥视孔 LSTM）

* 改进点：在 **门的计算**中，不仅考虑 $h_{t-1}$ 和 $x_t$，还引入 **前一时刻的记忆单元 $C_{t-1}$**。
* 公式示例（遗忘门）：
  $$
  f_t = \sigma(W_f \cdot [h_{t-1}, x_t, C_{t-1}] + b_f)
  $$
* 好处：

  * 门可以“窥视”记忆单元，更精细地控制遗忘和输入。
* 应用：需要更精细时间控制的任务（如时间序列预测）。

---

### 5.3 双向 LSTM（Bidirectional LSTM, Bi-LSTM）

* 思路：让模型既能看“过去”，也能看“未来”。
* 结构：

  * 一条 LSTM 从左到右（顺序）
  * 另一条 LSTM 从右到左（逆序）
  * 最终拼接两个方向的输出：
    $$
    h_t = [\overrightarrow{h_t}, \overleftarrow{h_t}]
    $$
* 应用场景：

  * 句子标注（词性标注、命名实体识别）
  * 文本分类（利用上下文完整信息）
* 缺点：不能用于实时预测（因为需要完整序列）。

---

### 5.4 堆叠 LSTM（Stacked LSTM）

* 思路：把多个 LSTM 层叠加起来。
* 第 1 层输出 $h_t^{(1)}$ 作为第 2 层输入，以此类推。
* 好处：

  * 更深的网络能提取更复杂的特征。
* 缺点：

  * 更容易过拟合
  * 训练更耗资源

---

### 5.5 GRU（门控循环单元，Gated Recurrent Unit）

* 虽然严格来说 GRU 不是 LSTM 的“变体”，但它是 LSTM 的**简化版本**。
* 关键改动：

  * 只有 **更新门** 和 **重置门**，没有单独的记忆单元 $C_t$，而是直接用隐藏状态 $h_t$。
* 优点：

  * 结构更简单，计算更快
  * 在很多任务上效果与 LSTM 相当
* 缺点：

  * 对特别复杂的长期依赖问题，可能不如 LSTM 精细

---

## 六、LSTM 的应用场景

LSTM 是 **深度学习序列建模的里程碑**，在 Transformer 出现之前，它是 NLP、语音、时间序列等领域的主力模型。常见应用有：

### 6.1 语言建模与机器翻译

* **语言建模**：给定前面的词，预测下一个词。

  * LSTM 能比 RNN 更好地捕捉句子中的长期依赖。
* **神经机器翻译（NMT）**：早期翻译系统（如 Google 2016 年的 NMT）大量使用 Encoder-Decoder 架构的 LSTM。

  * 编码器：用 LSTM 读取源语言句子，得到上下文表示。
  * 解码器：用 LSTM 生成目标语言句子。

---

### 6.2 文本分类、情感分析

* LSTM 能读完整个句子，把上下文都考虑进去，最后输出一个表示，用于分类。
* 应用：

  * 情感分析（正面/负面）
  * 新闻分类
  * 垃圾邮件检测

---

### 6.3 时间序列预测

* LSTM 擅长建模时间上的连续变化，常用于：

  * 股票价格预测
  * 传感器数据分析
  * 天气预测

---

### 6.4 语音识别

* 语音信号是典型的长序列。
* LSTM 可以逐帧接收语音特征（如 MFCC），逐步输出对应的文本。
* 在 Transformer 和 CTC 出现之前，LSTM 是语音识别的核心模型。

---

## 七、LSTM 的局限性

虽然 LSTM 比 RNN 强大得多，但它依然有一些局限，这也是为什么后来 Transformer 取而代之。

### 7.1 训练开销大

* LSTM 比 RNN 多了三套“门”，参数量更大。
* 计算更复杂，训练时间更长。

---

### 7.2 对超长序列依然有限

* LSTM 的记忆比普通 RNN 强，但不是无限：

  * 处理 10~50 个时间步还好，
  * 处理上千个时间步（如长文章、长语音），依然力不从心。

---

### 7.3 不能并行

* LSTM 必须按照时间顺序逐步计算：

  * 第 2 步依赖第 1 步的结果，第 3 步依赖第 2 步……
  * 无法像 Transformer 的自注意力那样并行处理整个序列。

---

### 7.4 被 Transformer 替代

* Transformer (2017) 提出后，几乎在所有序列任务上超越了 LSTM：

  * 更强的长期依赖建模能力
  * 完全并行计算 → 训练更快
  * 更容易扩展到大模型（BERT、GPT 系列）
* 现在 LSTM 更多出现在小型任务、资源受限的设备上，或作为教学示例。

---

## 八、实践——用 LSTM 做词级情感分类（IMDB）