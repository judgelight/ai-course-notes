---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第12講：半教師あり学習

---

## 第1章：半教師あり学習の概要

### 1.1 定義と動機

1. **教師あり学習（Supervised Learning）**

   * **定義**：ラベル付きデータ $(X, Y)$ を大量に用いて、写像関数 $f: X \to Y$ を学習する。  
   * **長所**：性能が安定しやすく、評価指標が明確。  
   * **短所**：専門領域（例：医用画像）のラベル取得コストが高く、ラベル収集が困難。

---

2. **教師なし学習（Unsupervised Learning）**

   * **定義**：ラベルなしデータ $X$ のみを用いて、クラスタリングや次元削減などデータ内部の構造を発見する。  
   * **長所**：人手によるラベル不要で、潜在的なデータ分布を探索できる。  
   * **短所**：直接予測タスクには利用しづらく、評価も目視や間接指標に依存しがち。

---

3. **半教師あり学習（Semi‑Supervised Learning, SSL）**

   * **定義**：少量のラベル付きデータ $(X_l, Y_l)$ と大量のラベルなしデータ $X_u$ を同時に活用し、両者を結びつけてモデル性能を向上させる学習手法。  
   * **コアアイデア**：ラベルなしデータを利用してラベル付きサンプルの情報量を拡張し、ラベル依存度を低減。  
   * **動機**：  
     * **ラベルのボトルネック**：高品質ラベルは希少かつ高価。  
     * **データ活用**：入手容易で大量のラベルなしデータを訓練に組み込むことで汎化性能を大幅向上。

---

4. **学習パラダイムの比較**

| 学習パラダイム | データ種類                        | 代表的手法                         | 長所                          | 短所                                 |
| ------------- | ------------------------------- | --------------------------------- | ----------------------------- | ------------------------------------ |
| 教師あり学習    | 全てラベル付き $(X_l, Y_l)$        | ロジスティック回帰、SVM、深層学習     | 高い予測精度、直接的な評価        | ラベル取得コストが高く、ラベル数に制限 |
| 教師なし学習    | 全てラベルなし $X_u$              | K‑means、PCA、孤立森                | ラベル不要、データ構造を探索可能    | 予測・分類タスクへの直接利用が困難     |
| 半教師あり学習  | 少量 $(X_l, Y_l)$ + 大量 $X_u$    | 自己学習、自訓練、グラフ伝播、TSVM 等 | ラベル依存度を下げつつ予測力も確保  | 分布仮定に依存、実装がやや複雑        |

---

### 1.2 代表的な応用例

1. **テキスト分類**  
   * **例**：感情分析、スパム検出  
   * **課題**：感情ラベルは人手判定が必要。大量のラベルなしテキストを自己学習やグラフ伝播に活用。

2. **画像分類・セグメンテーション**  
   * **例**：CIFAR‑10 少サンプル分類、医用画像（MRI/CT）セグメンテーション  
   * **課題**：医師によるラベル付けコストが高い。半教師あり手法で未ラベル画像を活用し、輪郭精度を向上。

---

3. **異常検知**  
   * **例**：金融詐欺検出、ネットワーク侵入検知  
   * **特徴**：異常サンプルは希少・多様。大量の正常サンプル（ラベルなし）で正常パターンを学習。

4. **音声認識**  
   * **例**：音声→テキスト転写  
   * **課題**：手動転写コストが高い。ASRにおいて大量の未ラベル音声と少量の転写テキストで学習。

5. **その他**  
   * 生物配列機能予測、リモートセンシング画像解析、推薦システムのコールドスタートなど。

---

## 第2章：核心仮定と評価指標

---

### 2.1 半教師あり学習の三大仮定

1. **滑らかさ仮定（Smoothness Assumption）**  
   * **内容**：特徴空間で近くにあるサンプルは同じラベルを持つ可能性が高い。  
   * **意義**：隣接サンプルに類似した予測を強制することで、ラベルなしデータを使って境界を滑らかに。  
   * **例**：グラフ伝播では高類似度のエッジ間でラベル差を大きく罰則。

---

2. **クラスタ仮定（Cluster Assumption）**  
   * **内容**：データは複数のクラスタに分かれ、同一クラスタ内のサンプルは同一ラベルを共有しやすい。  
   * **意義**：全データをクラスタリング後、クラスタ内でラベルを伝播、またはクラスタ間の空白領域に境界を置く。  
   * **例**：Transductive SVM はクラスタ間の疎な箇所で最大マージン境界を探す。

---

3. **多様体仮定（Manifold Assumption）**  
   * **内容**：高次元データ（画像、音声など）は低次元多様体上に分布している。  
   * **意義**：多様体構造をグラフ化や局所線型再構成で捉え、低次元空間でラベル伝播や分類を行う。  
   * **例**：Laplacian Eigenmaps でグラフラプラシアンを多様体近似に利用し、正則化項とする。

---

### 2.2 評価指標

| 指標                     | 意味                                                                 | 解説                                                            |
| ------------------------ | -------------------------------------------------------------------- | --------------------------------------------------------------- |
| Accuracy（正解率）       | 正しく予測したサンプル数 / 総サンプル数                               | 最も直感的だが、クラス不均衡に鈍感                               |
| Precision（適合率）      | TP / (TP + FP)                                                       | 予測正例の中で実際に正例である割合                              |
| Recall（再現率）         | TP / (TP + FN)                                                       | 実際の正例の中で検出できた割合                                  |
| F1値                     | $2 \times \frac{\mathrm{Precision}\times\mathrm{Recall}}{\mathrm{Precision} + \mathrm{Recall}}$ | PrecisionとRecallの調和平均                                    |
| AUC（ROC曲線下面積）     | 様々な閾値下でのTPRとFPRの曲線下面積                                 | モデルのソート性能を評価、閾値依存しない                         |
| 学習曲線（Learning Curve） | ラベル付きサンプル数の変化に伴うモデル性能（例：Accuracy）の曲線描画 | 半教師あり効果評価：少ラベル時に半教師ありが教師ありを上回るか確認 |

---

> **演習**：  
> 1. MNISTデータセットを例に、ラベル付き割合（1%、5%、10%、20%）ごとの教師あり／半教師ありAccuracy学習曲線を描き、差を分析。  

---

### 2.3 ラベルなしデータの「負の影響」と検証

* **誤った擬似ラベルの累積**  
  * 自訓練などで初期の擬似ラベル誤りが増幅し、モデルが偏る。  
  * **対策**：動的閾値設定、Top‑k高信頼度のみ選択、アンサンブル基学習器導入。

* **分布シフト（Distribution Shift）**  
  * ラベルなしデータとラベル付きデータの分布が異なる場合、誤った境界に誘導される。  
  * **対策**：分布マッチング（核密度推定）、ラベル付きと近いサブセットのみ使用。

---

* **検証戦略**  
  * **交差検証**：ラベル付きのみk折検証、ラベルなしは訓練にのみ利用。  
  * **ホールドアウト**：一部ラベル付きをテスト用に残し、訓練／テストを分離。

---

## 第3章：自己学習（Self‑Training）と擬似ラベリング

---

### 3.1 手法の原理

1. 少量のラベル付きデータで初期分類器（基学習器）を訓練。  
2. 無ラベルデータに分類器を適用し、高信頼度のサンプルに「擬似ラベル」を付与。  
3. 擬似ラベル付きサンプルをラベル付き集合に追加し、分類器を再訓練。  
4. サンプル消費または反復回数上限まで繰り返す。

---

* **長所**：実装が容易、任意の確信度出力可能な分類器と組み合わせ可能。  
* **短所**：誤った擬似ラベルが拡大し、confirmation biasを招く可能性。

---

### 3.2 アルゴリズムフロー

1. **初期化**  
   * ラベル付き集合 $L=\{(x_i,y_i)\}_{i=1}^l$  
   * 無ラベル集合 $U=\{x_j\}_{j=1}^u$  
   * 基学習器 $h$（確信度または確率出力可能）

2. **反復（停止条件まで）**  
   1. $L$ 上で $h$ を訓練  
   2. $U$ 内の各 $x$ に対し予測と確信度 $p(\hat y|x)$ を計算  
   3. 確信度閾値 $\tau$ 以上またはTop‑kでサブセット $S$ を選択  
   4. $(x,\hat y)\in S$ を $L$ に追加し、$U$ から除去  
   5. 新規追加なしまたは反復超過で終了  

3. **出力**：最終分類器 $h$

---

### 3.3 擬似ラベル戦略

* **固定閾値**：$\tau$（例：0.8）以上を採用。  
* **動的閾値**：反復ごとに閾値を調整（例：−0.02ずつ下げる）。  
* **Top‑k選択**：毎回信頼度上位kサンプルのみ。  
* **クラス制御サンプリング**：Top‑k内でクラス比を元のラベル付き集合に合わせる。

---

### 3.4 実装例（Scikit‑Learn）

```python
from sklearn.semi_supervised import SelfTrainingClassifier
from sklearn.svm import SVC
from sklearn.datasets import make_moons
from sklearn.metrics import accuracy_score
import numpy as np

# 1. データ準備
X, y = make_moons(n_samples=300, noise=0.1, random_state=42)
rng = np.random.RandomState(0)
y_partial = np.copy(y)
mask = rng.rand(len(y)) < 0.7
y_partial[mask] = -1  # 無ラベルを -1 で示す

# 2. Self‑Training分類器構築
base_clf = SVC(kernel='rbf', probability=True, gamma=10)
self_training = SelfTrainingClassifier(
    base_clf,
    threshold=0.8,       # 固定閾値
    criterion='probability',
    max_iter=10
)

# 3. 訓練
self_training.fit(X, y_partial)

# 4. 評価
y_pred = self_training.predict(X)
print("Self‑Training Accuracy:", accuracy_score(y, y_pred))
```

---

### 3.5 リスクと改善策

| 問題                          | 原因                   | 対策                                   |
| --------------------------- | -------------------- | ------------------------------------ |
| 誤擬似ラベル累積（confirmation bias） | 初期基学習器の誤りが反復で拡大      | 閾値厳格化、動的閾値、Top‑k、temperature scaling |
| クラス偏り（Class Imbalance）      | 高信頼度サンプルが主流クラスに偏る    | クラス制御サンプリング、各クラスTop‑k維持              |
| 分布シフト（Domain Shift）         | 無ラベルとラベル付きの分布不一致     | 分布整合（核密度推定、ドメイン適応）、類似サブセット使用         |
| 擬似ラベルへの過学習                  | 反復で擬似ラベル依存が強まり汎化能力低下 | 早期停止、訓練時に一定割合で元ラベル付きデータを混入           |

---

### 3.6 実践演習

1. **閾値比較**：固定閾値 {0.7,0.8,0.9} のAccuracyと擬似ラベル数増加曲線を比較。
2. **基学習器比較**：DecisionTree、SVM、MLPで半教師あり性能を比較。
3. **バランス vs 不均衡**：高度に不均衡データで固定閾値とクラス制御Top‑kの差異を分析。

---

## 第4章：協同訓練（Co‑Training）

---

### 4.1 マルチビュー構成（Multi‑View Framework）

* **概念**：サンプル特徴を複数の「ビュー」に分割（例：URL特徴／コンテンツ特徴、色ヒストグラム／テクスチャ特徴）。
* **ビュー条件**：

  1. **条件独立性**：ラベル条件下でビュー間が独立。
  2. **十分性**：各ビューがラベル予測に十分寄与。

---

### 4.2 アルゴリズムフロー

1. **初期化**

   * ラベル付き集合 $L$、無ラベル集合 $U$
   * 特徴をビュー $V\_1, V\_2$ に分割
   * $V\_1$ で分類器 $h\_1$, $V\_2$ で $h\_2$ を訓練

2. **反復相互ラベル付け**

   1. $U$ に対し $h\_1$ で予測、信頼度上位kを選択しラベル付きに追加
   2. 同様に $h\_2$ で別のkを選択し追加
   3. $U$ から該当サンプルを除去
   4. 新ラベル付き集合で $h\_1, h\_2$ を再訓練
   5. 停止条件（反復回数、$U$空）まで繰り返し

---

### 4.3 理論的背景

* ビュー条件独立性により、各分類器の誤りが相互に独立となり、confirmation bias抑制に寄与。
* 理想条件下で反復により誤差は収束的に低下。

---

### 4.4 実践例（擬似コード）

```python
import numpy as np
from sklearn.tree import DecisionTreeClassifier

# X1, X2: 二つのビュー特徴, y_partial: 部分ラベル (-1 無ラベル)
# L_idx: ラベル付きインデックス, U_idx: 無ラベルインデックス

h1 = DecisionTreeClassifier()
h2 = DecisionTreeClassifier()

for iter in range(max_iter):
    # 各ビューとラベル付きで訓練
    h1.fit(X1[L_idx], y[L_idx])
    h2.fit(X2[L_idx], y[L_idx])
    
    # 無ラベルで予測確率取得
    probs1 = h1.predict_proba(X1[U_idx])
    probs2 = h2.predict_proba(X2[U_idx])
    
    # Top‑kサンプル選択
    idx1 = np.argsort(probs1.max(axis=1))[-k:]
    idx2 = np.argsort(probs2.max(axis=1))[-k:]
    
    new_idx1 = U_idx[idx1]
    new_idx2 = U_idx[idx2]
    y[new_idx1] = probs1[idx1].argmax(axis=1)
    y[new_idx2] = probs2[idx2].argmax(axis=1)
    
    # インデックス更新
    L_idx = np.concatenate([L_idx, new_idx1, new_idx2])
    U_idx = np.setdiff1d(U_idx, np.concatenate([new_idx1, new_idx2]))
    
    if len(U_idx) == 0:
        break
```

---

### 4.5 注意点と対策

| 問題       | 説明                 | 対策                         |
| -------- | ------------------ | -------------------------- |
| ビュー独立性欠如 | 実データで条件独立が成り立ちにくい  | 直交する特徴選択、次元削減後にビュー分割       |
| ビュー十分性欠如 | 一部ビューで予測力不足        | ビュー数増加、事前学習モデルによる特徴抽出      |
| 擬似ラベル衝突  | 二分類器が異なるラベルを出す     | 一致ラベルのみ追加、衝突サンプルは次回以降까지保留  |
| 計算コスト    | 二つのモデルを交互訓練し、コスト増加 | 軽量基学習器採用、各反復の追加サンプル数 k を制限 |

---

## 第5章：グラフベース手法（Graph‑Based Methods）

---

### 5.1 サンプルグラフ構築

1. **頂点とエッジ**

   * 各サンプルを頂点とする。
   * 類似度に応じた重み付きエッジを張る。

2. **類似度計算**

   * **kNNグラフ**：各頂点にk近傍を結び、重みは定数またはガウス核。
   * **完全グラフ（RBF）**：

     $$
       W_{ij} = \exp\!\bigl(-\tfrac{\|x_i - x_j\|^2}{2\sigma^2}\bigr).
     $$

3. **次数行列とラプラシアン**

   * 次数行列 $D$, $D\_{ii}=\sum\_j W\_{ij}$.
   * ラプラシアン $L=D-W$, 正規化版 $L\_{\mathrm{sym}}=I-D^{-1/2}WD^{-1/2}$.

---

### 5.2 Label Propagation / Spreading

1. **Label Propagation (LP)**

   * ラベル付き頂点のラベルを固定し、無ラベル頂点はグラフ構造で反復更新。
   * 更新式：

     $$
       F^{(t+1)} = D^{-1}W F^{(t)},\quad F^{(0)}=Y.
     $$

2. **Label Spreading (LS)**

   * LPと同様だが、各反復後に正規化と退避項を追加：

     $$
       F^{(t+1)}=\alpha S F^{(t)}+(1-\alpha)Y,\quad S=D^{-1/2}WD^{-1/2}.
     $$

3. **ハイパーパラメータ**

   * ガウス核幅 $\sigma$ または kNNの k。
   * 反復回数／収束閾値、LSの $\alpha\in(0,1)$。

---

### 5.3 理論的導出

* **滑らかさエネルギー最小化**

  $$
    \mathcal{E}(F)=\tfrac12\sum_{i,j}W_{ij}\|F_i-F_j\|^2,
  $$

  制約下で最小化の閉形式解：

  $$
    F=(I-\alpha S)^{-1}Y.
  $$

* **ラプラシアン正則化との関係**

  $$
    \min_f\sum_{i\in L}\!\ell(f(x_i),y_i)
    +\lambda\sum_{i,j}W_{ij}\|f(x_i)-f(x_j)\|^2.
  $$

---

### 5.4 実践例（Scikit‑Learn）

```python
from sklearn import datasets
from sklearn.semi_supervised import LabelPropagation, LabelSpreading
from sklearn.metrics import accuracy_score
import numpy as np

# データ読み込みとラベル隠蔽
X, y = datasets.make_circles(n_samples=300, noise=0.1, factor=0.4)
rng = np.random.RandomState(0)
y_partial = np.copy(y)
mask = rng.rand(len(y)) < 0.7
y_partial[mask] = -1

# Label Propagation
lp = LabelPropagation(kernel='rbf', gamma=20, max_iter=1000)
lp.fit(X, y_partial)
pred_lp = lp.transduction_

# Label Spreading
ls = LabelSpreading(kernel='knn', n_neighbors=10, alpha=0.8, max_iter=1000)
ls.fit(X, y_partial)
pred_ls = ls.transduction_

print("LP Accuracy:", accuracy_score(y, pred_lp))
print("LS Accuracy:", accuracy_score(y, pred_ls))
```

---

### 5.5 リスクと改善策

| 問題                   | 説明                                  | 対策                         |
| -------------------- | ----------------------------------- | -------------------------- |
| グラフが密かつ大規模           | 完全グラフは計算量 $\mathcal{O}(n^2)$      | kNNで疎グラフ構築、FAISS等で近似最近傍高速化 |
| ハイパーパラメータ敏感          | $\sigma$, k, $\alpha$ の選択が結果に影響 | グリッドサーチ、検証セットで適応的選択        |
| ラベル漏洩（Label Leakage） | LPはラベル固定により過度依存の可能性                 | LSで正規化と退避項を追加              |
| 分布不均                 | ラベルなし分布とラベル付きクラスタが不一致               | サブ空間クラスタリング、分割伝播           |

---

## 第6章：生成モデルベース手法

---

### 6.1 生成的半教師ありフレームワーク

1. **基本思想**

   * 潜在変数 $z$ とラベル $y$ が共同でデータ生成：

     $$
       p(x,y,z)=p(y)p(z)p(x\mid y,z).
     $$
   * ラベル付き・なし両方の周辺尤度を最大化し、生成モデルと分類器を共同学習。

2. **EMアルゴリズム**

   * **Eステップ**：無ラベルデータの後验 $q(y,z\mid x)$ を推定。
   * **Mステップ**：固定した $q$ で完全データ対数尤度を最大化。
   * 収束まで反復。

---

### 6.2 深層生成モデル：半教師ありVAE（M1+M2）

1. **M1：無教師ありVAE**

   * **構成**：エンコーダ $q\_\phi(z\mid x)$、デコーダ $p\_\theta(x\mid z)$。
   * **ELBO**：

     $$
       \mathcal{L}_{\mathrm{VAE}}(x) 
       =\mathbb{E}_{q_\phi(z\mid x)}[\log p_\theta(x\mid z)]
       -\mathrm{KL}(q_\phi(z\mid x)\|p(z)).
     $$

2. **M2：半教師あり拡張**

   * **構成**：M1にクラス変数 $y$ を追加。

     * エンコーダ $q\_\phi(z\mid x,y)$、予測器 $q\_\phi(y\mid x)$、デコーダ $p\_\theta(x\mid z,y)$.
   * **目的関数**：

     * ラベル付き：

       $$
         \mathcal{L}(x,y)
         =\mathbb{E}_{q(z\mid x,y)}[\log p(x\mid z,y)]
         -\mathrm{KL}[q(z\mid x,y)\|p(z)].
       $$
     * ラベルなし：

       $$
         \mathcal{U}(x)
         =\sum_y q(y\mid x)\,\mathcal{L}(x,y)
         +\mathcal{H}(q(y\mid x)).
       $$

3. **総合目的**：

   $$
     \max_{\theta,\phi}
     \sum_{(x,y)\in L}\mathcal{L}(x,y)
     +\sum_{x\in U}\mathcal{U}(x)
     +\alpha\,\mathbb{E}_{(x,y)\in L}[\log q_\phi(y\mid x)].
   $$

---

### 6.3 Ladder Network

1. **構造概観**

   * 各層にノイズを加え、ジャンプ接続で無ノイズ活性化を復元。
   * 最上位に分類ブランチを追加し、分類損失＋復元損失を同時最小化。

2. **損失関数**

   $$
     \mathcal{L}
     =\mathcal{L}_{\mathrm{sup}}(y,\hat y)
     +\sum_{l=0}^L\lambda_l\|h^{(l)}-\tilde h^{(l)}\|^2.
   $$

---

### 6.4 実践例（PyTorch：半教師ありVAE骨格）

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# エンコーダ q(z|x,y) と q(y|x)
class Encoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, z_dim, n_classes):
        super().__init__()
        self.fc_x = nn.Linear(input_dim, hidden_dim)
        self.fc_y = nn.Linear(n_classes, hidden_dim)
        self.fc_mu = nn.Linear(hidden_dim, z_dim)
        self.fc_logvar = nn.Linear(hidden_dim, z_dim)
        self.fc_qy = nn.Linear(input_dim, n_classes)
    def forward(self, x, y=None):
        logits = self.fc_qy(x)
        qy = F.softmax(logits, dim=-1)
        y_onehot = y if y is not None else qy
        h = F.relu(self.fc_x(x) + self.fc_y(y_onehot))
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return qy, mu, logvar

# デコーダ p(x|z,y)
class Decoder(nn.Module):
    def __init__(self, z_dim, hidden_dim, output_dim, n_classes):
        super().__init__()
        self.fc_z = nn.Linear(z_dim, hidden_dim)
        self.fc_y = nn.Linear(n_classes, hidden_dim)
        self.fc_out = nn.Linear(hidden_dim, output_dim)
    def forward(self, z, y_onehot):
        h = F.relu(self.fc_z(z) + self.fc_y(y_onehot))
        return torch.sigmoid(self.fc_out(h))

# 再パラメータ化サンプリング
def reparameterize(mu, logvar):
    std = torch.exp(0.5 * logvar)
    eps = torch.randn_like(std)
    return mu + eps * std

# 単ステップ訓練例
def train_step(x, y, encoder, decoder, optimizer, n_classes, alpha=0.1):
    qy, mu, logvar = encoder(x, y)
    z = reparameterize(mu, logvar)
    x_recon = decoder(z, F.one_hot(y, n_classes).float() if y is not None else qy)
    recon_loss = F.binary_cross_entropy(x_recon, x, reduction='sum')
    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    if y is None:
        loss = torch.sum(qy * (recon_loss + kl)) + torch.sum(qy * torch.log(qy + 1e-8))
    else:
        ce = F.cross_entropy(encoder.fc_qy(x), y)
        loss = recon_loss + kl + alpha * ce
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return loss.item()
```

> **注**：バッチの混合や重み正規化などは省略。

---

## 第7章：Transductive SVM とその他の進化手法

---

### 7.1 Transductive SVM（TSVM）

1. **基本思想**

   * ラベル付き $(X\_l, Y\_l)$ とラベルなし $X\_u$ を同時に考慮し、ラベル付き正解＋無ラベル高密度領域から離れたマージンを最大化。

2. **最適化問題**

   $$
   \min_{w,b,\hat y_u}
   \tfrac12\|w\|^2
   +C_l\sum_{i\in L}\ell(y_i,w^\top x_i+b)
   +C_u\sum_{j\in U}\ell(\hat y_j,w^\top x_j+b).
   $$

---

### 7.2 GANベース半教師あり（GAN‑Based SSL）

1. **核心**

   * 判別器が「真偽」区別に加え、前Kクラス＋生成クラス(K+1)を分類。

2. **代表例**

   * CatGAN、TripleGAN など。

---

### 7.3 メタラーニング応用（Meta‑Learning）

1. **背景**：少数タスクで迅速適応可能な初期パラメータを学習。
2. **MAML‑SSL**：MAML枠組み内で半教師あり損失を使いタスク内更新、タスク間で初期化最適化。

---

### 7.4 実践例と演習

* **TSVM vs SVM**、**GAN‑SSL**、**MAML‑SSL** の比較演習。

---

## 附録：まとめと選定ガイド

| 手法カテゴリ           | 代表アルゴリズム                            | 長所             | 適用シーン            |
| ---------------- | ----------------------------------- | -------------- | ---------------- |
| 自己学習             | Self‑Training, Pseudo‑Label         | 実装容易、多モデル対応    | 基学習器信頼度高、分布類似時   |
| 協同訓練             | Co‑Training                         | 多ビューでバイアス低減    | 自然多ビューが存在する場合    |
| グラフベース           | Label Propagation/Spreading         | グローバル構造滑らか化    | クラスタ構造や多様体が明確な場合 |
| 生成モデル            | Semi‑Supervised VAE, Ladder Network | 深層表現学習、明示的モデル化 | 高次元複雑データ（画像・音声等） |
| Transductive SVM | Transductive SVM                    | 密度回避マージン最大化    | 小規模無ラベル、二値分類     |
| GAN‑SSL          | CatGAN, TripleGAN                   | 強力な分布学習        | 豊富な計算資源がある場合     |
| メタラーニング拡張        | MAML‑SSL                            | 少ラベルで高速適応      | 少サンプル・マルチタスク     |

---

> **選定ポイント**：
>
> * 小規模・単純構造 → 自己学習 or TSVM
> * 自然多ビュー → Co‑Training
> * 大規模高次元 → 生成モデル or GAN‑SSL
> * 少ラベル新タスク → MAML‑SSL

