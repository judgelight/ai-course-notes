---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第17讲：自然语言处理基础

---

# 自然语言处理（NLP）基础介绍

## 一、NLP 的定义与目标

* **定义**：自然语言处理（Natural Language Processing，NLP）是人工智能和计算语言学的一个重要分支，旨在让计算机理解、生成和处理人类语言。
* **目标**：

  1. **理解**：让机器“读懂”语言（如情感分析、问答系统）。
  2. **生成**：让机器“写/说”语言（如机器翻译、文本生成）。
  3. **交互**：支持人机自然交互（如语音助手、聊天机器人）。

---

## 二、NLP 的挑战

* **语言的多样性**：不同语言差异巨大（中文 vs 英文 vs 日语）。
* **歧义性**：同一个词、句子可能有多种解释。

  * 例：中文“苹果” → 水果 or 公司。
* **上下文依赖**：词语意义随上下文而变化。

  * 例：“他打败了对手”，“他打了篮球”。
* **世界知识**：语言中常常隐含常识。

  * 例：“他喝了三杯水，但还是渴” → 需要理解“水不能立刻缓解口渴”。

---

## 三、NLP 的主要任务

1. **分类类任务**

   * 文本分类（垃圾邮件检测、情感分析）
   * 主题建模：从大量文本中自动发现潜在主题
2. **理解类任务**

   * 命名实体识别（NER）：识别文本中出现的人名、地名、组织名、日期等
   * 依存句法分析：分析句子内部的语法关系
   * 阅读理解（QA）：给定一段文本，回答相关问题

---

3. **生成类任务**

   * 机器翻译
   * 文本摘要
   * 对话系统
4. **检索类任务**

   * 信息检索（搜索引擎）
   * 知识抽取: 从非结构化文本中 抽取结构化的知识（实体、关系、事件）

---

## 四、方法论演进：统计方法 vs 深度学习方法

### **统计方法**

* **基于规则**：人工制定规则（早期 NLP 系统，如 ELIZA）
* **基于概率模型**：n-gram、隐马尔可夫模型（HMM）、最大熵模型
* 优点：解释性强
* 缺点：难以处理复杂语义，上限受限

---

### **深度学习方法**

* 2010 年后快速发展
* 核心思想：用神经网络从数据中自动学习特征
* 代表方法：

  * Word2Vec（2013） → 词向量
  * Seq2Seq（2014） → 神经机器翻译
  * Transformer（2017） → BERT、GPT、LLM
* 优点：性能大幅提升，尤其在大数据场景
* 缺点：可解释性差、依赖算力与数据

---

## 五、常用工具与框架

1. **传统 NLP 工具**

   * **NLTK**：学术界常用的教学和研究工具，功能全面。
   * **spaCy**：工业级 NLP 库，速度快，API 友好。
2. **现代深度学习工具**

   * **PyTorch / TensorFlow**：深度学习框架
   * **Hugging Face Transformers**：预训练模型库（BERT、GPT 等）
   * **datasets 库**：开源 NLP 数据集集合
3. **综合平台**

   * Google Colab / Kaggle Notebook：快速实验环境
   * Jupyter Notebook：交互式代码 + 文档

---

## 六、课堂小练习

1. **安装工具**

   ```bash
   pip install nltk spacy torch transformers
   ```
2. **用 NLTK 做简单分词**

   ```python
   import nltk
   nltk.download("punkt")
   nltk.download("punkt_tab")
   text = "I love studying Natural Language Processing."
   print(nltk.word_tokenize(text))
   ```
3. **用 spaCy 做词性标注**

   ```python
   import spacy
   nlp = spacy.load("en_core_web_sm")
   doc = nlp("Apple is looking at buying a startup in Japan")
   for token in doc:
       print(token.text, token.pos_)
   ```

---

# 文本预处理

## 一、为什么需要文本预处理？

* **文本是非结构化数据**，不能直接输入模型。
* 预处理的目标是把 **自然语言 → 数字化表示**。
* 不同的预处理方法会直接影响模型的性能。

---

## 二、文本清洗与标准化

1. **大小写统一**

   * 将 "Apple" 和 "apple" 统一 → "apple"
   * 防止模型把大小写当成不同词。

2. **去除特殊符号**

   * 标点、表情符号、HTML 标签等
   * 是否保留要看任务需求（情感分析可能需要保留表情）。

---

3. **去停用词（Stopwords）**

   * 停用词是高频但信息量小的词，如 *is, the, of*
   * NLTK 提供常见停用词表。

4. **词干化（Stemming）与词形还原（Lemmatization）**

   * Stemming：词根化（running → runn）
   * Lemmatization：语义化还原（running → run）
   * **区别**：Lemmatization 更精确，但速度慢。

---

## 三、分词（Tokenization）

* **英文**：空格分词 + 标点切分。
* **中文/日文**：没有天然空格，需要额外的分词器。

  * 中文常用工具：jieba、HanLP
  * 日文常用工具：MeCab、Sudachi

### 示例

```python
import nltk
nltk.download("punkt")
nltk.download("punkt_tab")
text = "I love studying Natural Language Processing."
print(nltk.word_tokenize(text))
# 输出: ['I', 'love', 'studying', 'Natural', 'Language', 'Processing', '.']
```

---

## 四、N-gram 模型

* **定义**：把连续的 n 个词作为一个单元。
* **例子**：

  * Sentence: "I love NLP"
  * Unigram: \[I], \[love], \[NLP]
  * Bigram: \[I love], \[love NLP]
  * Trigram: \[I love NLP]
* **用途**：语言模型、特征工程。

---

## 五、向量化表示方法

1. **One-hot Encoding**

   * 每个词表示为一个高维稀疏向量。
   * 缺点：无法表达词义相似度，维度高。

2. **Bag of Words (BoW)**

   * 统计词频，忽略顺序。
   * 例子：

     * 文档1："I love NLP" → {I:1, love:1, NLP:1}
     * 文档2："I love AI" → {I:1, love:1, AI:1}

---

3. **TF-IDF (Term Frequency – Inverse Document Frequency)**

   * 解决高频词无信息量的问题。
   * 公式：

     $$
     \text{TF-IDF}(t, d) = \text{TF}(t, d) \times \log \frac{N}{df(t)}
     $$

     * TF：词在文档中出现的频率
     * df：包含该词的文档数
     * N：总文档数
   * **直观理解**：如果一个词在某篇文章里很常见，但在整个语料库中很少见 → 该词对区分文章很重要。

---

## 六、代码实践：IMDB 影评情感分析（预处理部分）

```python
import nltk
import string
from sklearn.feature_extraction.text import TfidfVectorizer

# 示例文本
corpus = [
    "I love this movie, it was amazing!",
    "I hate this film, it was terrible..."
]

# 1. 分词 + 去标点
def preprocess(text):
    tokens = nltk.word_tokenize(text.lower())  # 小写化 + 分词
    tokens = [t for t in tokens if t not in string.punctuation]  # 去标点
    return tokens

print(preprocess("I love this movie, it was amazing!"))

# 2. TF-IDF 向量化
vectorizer = TfidfVectorizer(tokenizer=preprocess)
X = vectorizer.fit_transform(corpus)

print("词表:", vectorizer.get_feature_names_out())
print("TF-IDF 矩阵:\n", X.toarray())
```

输出示例：

```
词表: ['amazing' 'film' 'hate' 'it' 'love' 'movie' 'terrible' 'this' 'was']
TF-IDF 矩阵:
[[0.46 0.    0.    0.33 0.46 0.46 0.    0.33 0.33]
 [0.    0.46 0.46 0.33 0.    0.    0.46 0.33 0.33]]
```

---

# 词表示方法

## 一、为什么需要词表示？

* 计算机无法直接理解自然语言文本，必须将其转化为数值。
* **词表示方法**的目标：把词语映射到向量空间，便于计算机进行计算。
* 理想的词表示应能体现 **语义相似性**（例如 *king* 与 *queen* 应该比 *king* 与 *apple* 更相似）。

---

## 二、One-hot Encoding（独热编码）

### 原理

* 给定词表（Vocabulary）大小为 $V$，每个词用 $V$ 维向量表示。
* 向量中只有一个位置为 1，其他全是 0。

### 示例

词表：\["I", "love", "NLP"]

* "I" → \[1, 0, 0]
* "love" → \[0, 1, 0]
* "NLP" → \[0, 0, 1]

---

### 优缺点

* ✅ 简单直观，容易实现
* ❌ 高维稀疏，浪费存储
* ❌ 无法体现词义相似性（"dog" 与 "cat" 完全独立）

### 代码示例

```python
from sklearn.preprocessing import OneHotEncoder
import numpy as np

words = np.array(["I", "love", "NLP"]).reshape(-1, 1)
encoder = OneHotEncoder()
onehot = encoder.fit_transform(words)

print("词表:", encoder.categories_)
print("One-hot 编码:\n", onehot)
```

---

## 三、Bag of Words（词袋模型，BoW）

### 原理

* 不考虑词序，把文档看作“词的集合”。
* 统计每个词在文档中出现的次数。

### 示例

文档1："I love NLP"
文档2："I love AI"
词表：\["I", "love", "NLP", "AI"]

* 文档1 → \[1, 1, 1, 0]
* 文档2 → \[1, 1, 0, 1]

---

### 优缺点

* ✅ 简单有效，适合小规模任务（如垃圾邮件分类）
* ❌ 忽略词序
* ❌ 语义缺失（"good" 和 "bad" 可能都高频）

### 代码示例

```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    "I love NLP",
    "I love AI"
]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)

print("词表:", vectorizer.get_feature_names_out())
print("BoW 矩阵:\n", X.toarray())
```

---

## 四、TF-IDF（词频-逆文档频率）

### 原理

* **TF (Term Frequency)**：某词在文档中的出现频率。
* **IDF (Inverse Document Frequency)**：某词在整个语料库中的稀有程度。
* 公式：

  $$
  \text{TF-IDF}(t, d) = \text{TF}(t, d) \times \log \frac{N}{df(t)}
  $$

  * $N$ = 总文档数
  * $df(t)$ = 包含该词的文档数

---

### 示例

语料库：

* 文档1："I love NLP"
* 文档2："I love AI"
* 文档3："I love love love AI"

结果：

* "love" 在所有文档都出现 → IDF 低 → 权重下降
* "NLP" 只在一篇文档出现 → IDF 高 → 权重上升

---

### 优缺点

* ✅ 相比 BoW，更能突出重要词汇
* ❌ 仍然忽略词序与上下文
* ❌ 对长文档和新词表现有限

### 代码示例

```python
from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
    "I love NLP",
    "I love AI",
    "I love love love AI"
]

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)

print("词表:", vectorizer.get_feature_names_out())
print("TF-IDF 矩阵:\n", X.toarray())
```

---

## 五、三者对比

| 方法      | 特点 | 优点           | 缺点             |
| ------- | -- | ------------ | -------------- |
| One-hot | 离散 | 简单直观         | 无语义信息，高维稀疏     |
| BoW     | 统计 | 实现简单，适合小任务   | 忽略顺序，语义缺失      |
| TF-IDF  | 加权 | 突出关键词，常用文本表示 | 仍无语义信息，不能处理同义词 |

