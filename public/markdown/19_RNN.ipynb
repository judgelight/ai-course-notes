{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "with open(\"corpus.txt\", \"w\") as f:\n",
        "    f.write(\"I love natural language processing\\n\")\n",
        "    f.write(\"natural language processing is fun\\n\")\n",
        "    f.write(\"I love deep learning\\n\")\n",
        "    f.write(\"deep learning powers artificial intelligence\\n\")\n",
        "    f.write(\"language models can generate text\\n\")\n",
        "    f.write(\"text generation is a task in natural language processing\\n\")"
      ],
      "metadata": {
        "id": "mZW7O_v9SqQI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "id": "M6W-flWETh7B",
        "outputId": "20161b02-dc6e-4fee-dac7-e87af576a08e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.2\n",
            "    Uninstalling scipy-1.16.2:\n",
            "      Successfully uninstalled scipy-1.16.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "62233e919a624a8389bfce8b3d375fb6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i5CWrjm4OyQT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb99dd5f-1e26-49d0-bd36-7a7595fa18e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "词向量维度: (100,)\n",
            "相似词: [('languages', 0.8260655403137207), ('word', 0.7464082837104797), ('spoken', 0.7381494045257568), ('arabic', 0.7318817377090454), ('english', 0.7214903831481934)]\n",
            "king - man + woman ≈ [('queen', 0.7698540687561035)]\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# 选择预训练词向量，例如 GloVe 100维\n",
        "wv = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "print(\"词向量维度:\", wv[\"computer\"].shape)\n",
        "print(\"相似词:\", wv.most_similar(\"language\", topn=5))\n",
        "print(\"king - man + woman ≈\", wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 环境依赖: pip install torch gensim tqdm\n",
        "# 文件: word_rnn_lm.py\n",
        "# ================================\n",
        "\n",
        "import io, random, math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gensim.downloader as api\n",
        "from tqdm import tqdm\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# -------------------------------\n",
        "# 1. 读取语料 & 构建词表\n",
        "# -------------------------------\n",
        "def read_corpus(path=\"corpus.txt\"):\n",
        "    tokens = []\n",
        "    with io.open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            words = line.strip().split()\n",
        "            if words:\n",
        "                tokens.extend(words + [\"<eos>\"])  # 每句加一个结束标记\n",
        "    return tokens\n",
        "\n",
        "def build_vocab(tokens, min_freq=1, specials=(\"<pad>\", \"<unk>\", \"<eos>\")):\n",
        "    counter = Counter(tokens)\n",
        "    stoi = {sp: i for i, sp in enumerate(specials)}\n",
        "    idx = len(stoi)\n",
        "    for w, c in counter.items():\n",
        "        if w in stoi:\n",
        "            continue\n",
        "        if c >= min_freq:\n",
        "            stoi[w] = idx\n",
        "            idx += 1\n",
        "    itos = {i: w for w, i in stoi.items()}\n",
        "    return stoi, itos\n",
        "\n",
        "# -------------------------------\n",
        "# 2. 数据集 (滑动窗口: 输入→下一个词)\n",
        "# -------------------------------\n",
        "class WordLMDataset(Dataset):\n",
        "    def __init__(self, tokens, stoi, seq_len=5):\n",
        "        ids = [stoi.get(t, stoi[\"<unk>\"]) for t in tokens]\n",
        "        self.inputs, self.targets = [], []\n",
        "        for i in range(len(ids) - seq_len):\n",
        "            self.inputs.append(ids[i:i+seq_len])\n",
        "            self.targets.append(ids[i+1:i+seq_len+1])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.inputs[idx]), torch.tensor(self.targets[idx])\n",
        "\n",
        "# -------------------------------\n",
        "# 3. 加载预训练 GloVe\n",
        "# -------------------------------\n",
        "def load_glove(stoi, embed_dim=100):\n",
        "    print(\"正在下载并加载 GloVe 预训练词向量...\")\n",
        "    wv = api.load(f\"glove-wiki-gigaword-{embed_dim}\")\n",
        "    matrix = np.random.normal(0, 0.1, (len(stoi), embed_dim)).astype(np.float32)\n",
        "    hit = 0\n",
        "    for w, i in stoi.items():\n",
        "        if w in wv:\n",
        "            matrix[i] = wv[w]\n",
        "            hit += 1\n",
        "    print(f\"GloVe 覆盖了 {hit}/{len(stoi)} 个词 ({hit/len(stoi):.2%})\")\n",
        "    return torch.tensor(matrix)\n",
        "\n",
        "# -------------------------------\n",
        "# 4. 模型定义\n",
        "# -------------------------------\n",
        "class WordRNNLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_size, batch_first=True)\n",
        "        self.proj = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, h0=None):\n",
        "        emb = self.embedding(x)            # (B,T,E)\n",
        "        out, hN = self.rnn(emb, h0)        # (B,T,H)\n",
        "        logits = self.proj(out)            # (B,T,V)\n",
        "        return logits, hN\n",
        "\n",
        "# -------------------------------\n",
        "# 5. 训练函数\n",
        "# -------------------------------\n",
        "def train_model(model, train_loader, stoi, itos, epochs=10, lr=0.01):\n",
        "    pad_idx = stoi[\"<pad>\"]\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            logits, _ = model(x)\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # 防止梯度爆炸\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch} 平均loss: {avg_loss:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. 文本生成\n",
        "# -------------------------------\n",
        "@torch.no_grad()\n",
        "def generate_text(model, stoi, itos, prefix=[\"I\"], max_len=10, temperature=1.0):\n",
        "    model.eval()\n",
        "    ids = [stoi.get(w, stoi[\"<unk>\"]) for w in prefix]\n",
        "    x = torch.tensor(ids, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        logits, _ = model(x)\n",
        "        next_logit = logits[0, -1] / temperature\n",
        "        probs = torch.softmax(next_logit, dim=-1)\n",
        "        next_id = torch.multinomial(probs, 1).item()\n",
        "        x = torch.cat([x, torch.tensor([[next_id]], device=DEVICE)], dim=1)\n",
        "    return [itos[i] for i in x[0].tolist()]\n",
        "\n",
        "# -------------------------------\n",
        "# 主流程\n",
        "# -------------------------------\n",
        "def main():\n",
        "    # Step1: 读语料\n",
        "    tokens = read_corpus(\"corpus.txt\")\n",
        "    stoi, itos = build_vocab(tokens, min_freq=1)\n",
        "    print(\"词表大小:\", len(stoi))\n",
        "\n",
        "    # Step2: 数据加载\n",
        "    dataset = WordLMDataset(tokens, stoi, seq_len=5)\n",
        "    train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "    # Step3: 构建模型\n",
        "    embed_dim = 100\n",
        "    hidden_size = 100\n",
        "    model = WordRNNLM(len(stoi), embed_dim, hidden_size, pad_idx=stoi[\"<pad>\"]).to(DEVICE)\n",
        "\n",
        "    # Step4: 加载预训练词向量\n",
        "    embedding_matrix = load_glove(stoi, embed_dim)\n",
        "    model.embedding.weight.data.copy_(embedding_matrix)\n",
        "\n",
        "    # Step5: 训练\n",
        "    train_model(model, train_loader, stoi, itos, epochs=10, lr=0.01)\n",
        "\n",
        "    # Step6: 文本生成\n",
        "    out = generate_text(model, stoi, itos, prefix=[\"I\"], max_len=8)\n",
        "    print(\"生成文本:\", \" \".join(out))\n"
      ],
      "metadata": {
        "id": "BTznxzgpT562"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTHJullaUJob",
        "outputId": "039514d5-5a37-412d-c6af-b71f334655cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "词表大小: 23\n",
            "正在下载并加载 GloVe 预训练词向量...\n",
            "GloVe 覆盖了 19/23 个词 (82.61%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 9/9 [00:00<00:00, 62.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 平均loss: 2.2167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 9/9 [00:00<00:00, 333.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 平均loss: 0.6362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 9/9 [00:00<00:00, 320.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 平均loss: 0.2986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 9/9 [00:00<00:00, 340.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 平均loss: 0.3173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 9/9 [00:00<00:00, 327.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 平均loss: 0.2121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 9/9 [00:00<00:00, 270.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 平均loss: 0.2349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 9/9 [00:00<00:00, 313.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 平均loss: 0.2228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 9/9 [00:00<00:00, 350.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 平均loss: 0.2392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 9/9 [00:00<00:00, 325.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 平均loss: 0.2168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 9/9 [00:00<00:00, 330.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 平均loss: 0.2018\n",
            "生成文本: I love natural language processing is fun <eos> I\n"
          ]
        }
      ]
    }
  ]
}