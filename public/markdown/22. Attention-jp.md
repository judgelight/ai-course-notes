---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第22講：注意機構（Attention Mechanism）

---

## 一、注意機構とは？

### 1.1 提案の背景

* **Seq2Seq モデル** では、エンコーダが全入力文を1つの固定長ベクトルに圧縮する。
  * 長文の場合、このベクトルはすべての情報を完全には保持できない。
  * その結果、デコーダが文頭の情報を「忘れてしまう」。
* この「情報ボトルネック」を解決するために、  
  → 2015年 Bahdanau らによって **Attention 機構** が提案された。

---

### 1.2 コアアイデア

> 「文全体を一度に記憶しよう」とせずに、  
> 各出力語を生成する際に **入力文の最も関連する部分に動的に注目する**。

* モデルが第 $t$ 番目の出力を生成する際：
  * もはや最後の隠れ状態だけに依存せず、
  * 入力系列中の各単語と現在のデコード状態との「関連度重み」を計算する。

---

### 1.3 類比（人間の注意）

* 人が翻訳をするとき：
  * 文全体を覚えるのではなく、**今の単語に対応する部分に注意**する。
* 例：
```

I love playing the piano → 私はピアノを弾くのが好きです

```
「ピアノ」を出力するとき、自然に “piano” に注目する。

* 注意機構により、モデルは「どこを見るか」を学び、  
  **選択的に焦点を当てる能力** を得る。

---

## 二、注意機構の基本原理

### 2.1 入出力構造

各タイムステップ $t$ において：

1. **入力：**
   * デコーダの現在の隠れ状態 $s_t$
   * エンコーダの全隠れ状態 $h_1, h_2, ..., h_T$
2. **出力：**
   * 加重平均によって得られた「コンテキストベクトル」 $c_t$
   * これが当該時刻の外部情報としてデコーダに入力される。

---

![bg 80%](https://s.ar8.top/img/picgo/20251029193706920.webp)

---
### 2.2 計算手順

#### 第1段階：類似度（score）の計算

$s_t$ と各 $h_i$ の一致度を求める：

$$
e_{t,i} = \text{score}(s_t, h_i)
$$

主なスコア関数：

| 名称 | 公式 | 説明 |
|------|------|------|
| Dot | $s_t^\top h_i$ | ベクトルの内積 |
| General | $s_t^\top W_a h_i$ | 重み付き内積 |
| Additive (Bahdanau) | $v_a^\top \tanh(W_s s_t + W_h h_i)$ | 非線形結合 |

---

#### 第2段階：Softmax による正規化

スコアを確率分布に変換：

$$
\alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_{k=1}^{T} \exp(e_{t,k})}
$$

* $\alpha_{t,i}$ は、現在の出力生成時に第 $i$ 入力へどれだけ「注意」しているかを示す。
* 全ての $\alpha_{t,i}$ の和は 1 になる。

---

#### 第3段階：コンテキストベクトルの算出

$$
c_t = \sum_{i=1}^{T} \alpha_{t,i} h_i
$$

* $c_t$ は入力情報の加重平均。
* 現在時刻のデコーダが「参照している内容」を表す。

---

#### 第4段階：コンテキストと状態を結合して出力を生成

$$
\tilde{s_t} = \tanh(W_c [s_t; c_t])
$$

そして Softmax を通して最終出力確率を得る：

$$
P(y_t | y_{<t}, X) = \text{Softmax}(W_o \tilde{s_t})
$$

---

## 三、直感的理解と可視化

### 3.1 イメージ比喩

* **エンコーダ隠れ状態 $h_i$**：入力文の各単語の「記憶」。
* **デコーダ状態 $s_t$**：現在生成している「意味的焦点」。
* **注意重み $\alpha_{t,i}$**：スポットライトのように、最も関連する入力を照らす。

---

### 3.2 機械翻訳の例

入力：
```

X = ["I", "love", "playing", "piano"]

```
出力：
```

Y = ["私", "好き", "弾く", "ピアノ"]

```
---

「ピアノ」を生成するとき：

| 入力語 | 重み ($\alpha_{t,i}$) |
|--------|----------------|
| I | 0.01 |
| love | 0.03 |
| playing | 0.12 |
| piano | **0.84** |

モデルは自動的に “piano” に焦点を当てる。

---

### 3.3 注意マップ（Attention Map）

文全体の生成過程で、  
2次元のマトリクスが得られる：

| 出力語 | I | love | playing | piano |
|--------|--|------|----------|--------|
| 私 | 0.7 | 0.2 | 0.1 | 0.0 |
| 好き | 0.1 | 0.8 | 0.1 | 0.0 |
| 弾く | 0.0 | 0.2 | 0.6 | 0.2 |
| ピアノ | 0.0 | 0.0 | 0.1 | 0.9 |

これが **Attention Heatmap**（注意ヒートマップ）であり、モデルが「どこを見ているか」を可視化できる。

---

## 四、注意の種類

### 4.1 Soft Attention と Hard Attention

| 種類 | 説明 | 特徴 |
|------|------|------|
| Soft Attention | 全入力に対して重み付け平均（微分可能） | 学習可能・主流 |
| Hard Attention | 特定の入力のみ選択（サンプリング） | 微分不可・強化学習が必要 |

現代のモデルはほぼすべて **Soft Attention** を採用。

---

### 4.2 Global Attention と Local Attention

| 種類 | 説明 | 長所 | 短所 |
|------|------|------|------|
| Global Attention | 全入力系列を考慮 | 広い文脈を取得 | 計算コスト大 |
| Local Attention | 一部のウィンドウに限定 | 高速 | 長距離依存を見落とす可能性 |

---

### 4.3 Self-Attention（自己注意）

* 注意は「デコーダがエンコーダを見る」だけでなく、
* **同一系列内の要素同士が互いに注目する** こともできる。

例：
> "The animal didn't cross the street because it was too tired."

* “it” は “street” ではなく “animal” に注意すべき。  
→ これが **Self-Attention（自己注意）** の考え方。

---

## 五、Self-Attention の動作原理

### 5.1 3つの基本ベクトル

各入力 $x_i$ から3つの表現を生成：

$$
Q_i = W_Q x_i \\
K_i = W_K x_i \\
V_i = W_V x_i
$$

それぞれ：

* **Q（Query）**：問い合わせベクトル  
* **K（Key）**：キー（検索対象）  
* **V（Value）**：値（情報本体）

---

![](https://s.ar8.top/img/picgo/20251029194631019.webp)

---
### 5.2 注意計算式

$$
\text{Attention}(Q, K, V) = \text{Softmax}\left( \frac{Q K^\top}{\sqrt{d_k}} \right) V
$$

解釈：

1. $QK^\top$：各 Query と全 Key の類似度を計算。  
2. $\sqrt{d_k}$ でスケーリング（数値の安定化）。  
3. Softmax で確率分布化。  
4. 加重平均をとって Value を集約。

---

### 5.3 マルチヘッド注意（Multi-Head Attention）

* 単一の注意ヘッドは一種類の関係（例：主語–動詞）しか学びにくい。
* Transformer では複数のヘッドを並行利用：
  $$
  \text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
  $$
* 各ヘッドが異なる関係（構文・意味・文脈など）を学習する。

---

## 六、Attention から Transformer へ

### 6.1 Transformer の核心

> Self-Attention によって RNN/LSTM の逐次構造を完全に置き換える。

構造自体は **Encoder–Decoder** のまま：
* Encoder：多層 Self-Attention + FeedForward
* Decoder：Self-Attention + Encoder–Decoder Attention

---

### 6.2 位置エンコーディング（Positional Encoding）

* 注意機構自体には「順序」の概念がない。
* Transformer は位置情報を正弦・余弦関数で付与：
  $$
  PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}}) \\
  PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})
  $$
* これにより語順を認識できるようにする。

---

## 七、注意機構の利点と意義

### 7.1 利点

* 長距離依存を捕捉できる（固定長ベクトル不要）。
* 翻訳・要約・対話などの精度向上。
* 重み可視化による高い可解釈性。
* Self-Attention では並列化が容易。

---

### 7.2 意義

* **Attention は RNN と Transformer をつなぐ架け橋**。
* 「逐次記憶」から「全局的注目」への転換点。
* BERT・GPT など大規模モデルの核心技術の基礎。

---

## 八、代表的な注意のバリエーション

| 名称 | コアアイデア | 主な用途 |
|------|---------------|-----------|
| Additive (Bahdanau) | $\tanh$ と学習可能パラメータを利用 | 古典的 Seq2Seq |
| Dot-Product (Luong) | ベクトル内積で類似度算出 | 高速翻訳モデル |
| Scaled Dot-Product | $\sqrt{d_k}$ によるスケーリング | Transformer |
| Multi-Head | 並列に複数の関係を学習 | Transformer |
| Self-Attention | 要素間の相互注目 | NLP・CV共通 |
| Cross-Attention | デコーダがエンコーダ出力を見る | 生成タスク |
| Sparse / Local | 注目範囲を限定 | 長文・高速モデル |

---

## 九、応用例

### 9.1 機械翻訳
* 初期の Attention Seq2Seq は BLEU スコアを大幅に向上。
* 入出力単語の整合性を高精度で実現。

### 9.2 テキスト要約
* モデルが主要文脈に焦点を当て、より自然な要約を生成。

### 9.3 画像キャプション生成（Image Captioning）
* 画像の異なる領域に「視線」を向ける仕組みを実現。

### 9.4 音声認識 / 映像理解
* 時系列データ内の重要部分に選択的に注目。

---

## 十、まとめ

| モジュール | 役割 | 公式 |
|-------------|------|------|
| スコア関数 | Query と Key の関連度を計算 | $e_{t,i} = \text{score}(Q,K)$ |
| 重み分布 | Softmax で正規化 | $\alpha_{t,i} = \text{Softmax}(e_{t,i})$ |
| 加重和 | コンテキストベクトル生成 | $c_t = \sum_i \alpha_{t,i} V_i$ |
| Self-Attention | 系列内相互注目 | $Q=K=V=X$ |
| Multi-Head | 複数視点の同時学習 | $\text{Concat}(\text{head}_i)$ |

---

# ✅ 授業後の思考課題

1. Attention はなぜ Seq2Seq の「情報ボトルネック」を解決できるのか？  
2. Additive Attention と Dot-Product Attention の違いは？  
3. Self-Attention はどのように並列計算を実現するのか？  
4. Transformer はなぜ RNN を完全に置き換えることができたのか？
