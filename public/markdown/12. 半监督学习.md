---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第12讲：半监督学习

---

## 第1章：半监督学习概述

### 1.1 定义与动机

1. **监督学习（Supervised Learning）**

   * **定义**：利用大量带标签数据 $(X, Y)$ 学习映射函数 $f: X \to Y$。
   * **优点**：性能通常较稳定，评价指标明确；
   * **缺点**：标签获取成本高，尤其在专业领域（如医学影像）标签难以获得。

2. **无监督学习（Unsupervised Learning）**

   * **定义**：仅用无标签数据 $X$，发现数据内部结构，如聚类、降维；
   * **优点**：无需人工标注，能挖掘潜在数据分布；
   * **缺点**：往往无法直接用于具体预测任务，质量评估依赖人眼或间接指标。

---

3. **半监督学习（Semi‑Supervised Learning，SSL）**

   * **定义**：同时利用少量带标签数据 $(X_l, Y_l)$ 和大量无标签数据 $X_u$，在两者之间建立联系，提升模型性能。
   * **核心思想**：借助无标签数据“增强”带标签样本的信息量，降低标签需求。
   * **动机**：

     * **标注瓶颈**：真实世界中，高质量标签稀缺且昂贵；
     * **数据利用**：无标签数据易获、量大，将其纳入训练可显著提升泛化能力。

---

4. **关键对比**

| 学习范式  | 数据类型                      | 典型方法                | 优势                   | 局限                 |
| ----- | ------------------------- | ------------------- | -------------------- | ------------------ |
| 监督学习  | 全部带标签 $(X_l,Y_l)$         | 逻辑回归、SVM、深度学习       | 预测精度高，评价直接           | 标注代价大，标签量受限        |
| 无监督学习 | 全部无标签 $X_u$               | K‑means、PCA、孤立森林    | 无需标注，可探索数据结构         | 难以直接用于预测或分类        |
| 半监督学习 | 少量 $(X_l,Y_l)$ + 大量 $X_u$ | 自训练、协同训练、图传播、TSVM 等 | 降低标注需求，兼具预测能力与数据挖掘优势 | 方法选择依赖数据分布假设；实现较复杂 |

---

### 1.2 经典应用场景

1. **文本分类**

   * **例子**：情感分析、垃圾邮件检测
   * **挑战**：情感标签需要人工判读，无标签的海量文本可用于模型自训练或图传播。

2. **图像分类与分割**

   * **例子**：CIFAR‑10 少样本分类、医学影像（MRI/CT）分割
   * **挑战**：医生标注成本高，半监督方法能利用未标注图像提升分割边缘精准度。

3. **异常检测**

   * **例子**：金融欺诈识别、网络入侵检测
   * **特点**：异常样本稀少且多样，无标签海量正常样本可帮助模型学习正常模式。
---

4. **语音识别**

   * **例子**：语音到文本转录
   * **挑战**：手动转录昂贵，半监督 ASR（自动语音识别）利用大量无标注音频与少量转录文本进行联合训练。

5. **其他场景**

   * 生物序列功能预测、遥感影像分析、推荐系统中的冷启动等。

---

## 第2章：核心假设与评价指标

---

### 2.1 半监督学习的三大假设

1. **平滑假设（Smoothness Assumption）**

   * **描述**：若两个样本在特征空间中足够接近，则它们很可能具有相同的标签。
   * **意义**：半监督方法通过对相邻样本“强制”输出相似预测，利用无标签数据增强决策边界的平滑性。
   * **应用示例**：图传播中，权重大的边（高相似度）之间标签差距惩罚更高。

---

2. **聚类假设（Cluster Assumption）**

   * **描述**：数据通常分布成若干簇，同一簇内的样本大多共享标签；不同簇之间标签差异较大。
   * **意义**：可先对全量数据做聚类，然后在簇内部传播标签；或鼓励决策边界落在“空白”簇间处。
   * **应用示例**：Transductive SVM 在无标签数据点稀疏区（簇间）寻找最大间隔边界。

---

3. **流形假设（Manifold Assumption）**

   * **描述**：高维数据（如图像、语音）实际分布在一个低维流形（多样本的连续曲面）上。
   * **意义**：算法可先对流形结构建图或局部线性重构，再在此低维空间中进行标签传播或分类。
   * **应用示例**：Laplacian Eigenmaps 用图拉普拉斯算子近似流形结构，作为半监督正则项。

---

### 2.2 评价指标

| 指标                    | 含义                                                                                       | 说明                            |
| --------------------- | ---------------------------------------------------------------------------------------- | ----------------------------- |
| 准确率 (Accuracy)        | 正确预测样本数 / 总样本数                                                                           | 最直观，但对类别不平衡不敏感                |
| 精确率 (Precision)       | TP / (TP + FP)                                                                           | 关注预测为正例中的准确性                  |
| 召回率 (Recall)          | TP / (TP + FN)                                                                           | 关注实际正例中被捕获的比例                 |
| F1 值                  | $2 \times \frac{\text{Precision}\times \text{Recall}}{\text{Precision} + \text{Recall}}$ | Precision 与 Recall 的调和平均      |
| AUC (ROC 曲线下面积)       | 不同阈值下 TPR 与 FPR 的变化曲线下面积                                                                 | 对模型排序能力评估，不依赖特定阈值             |
| 学习曲线 (Learning Curve) | 随带标签样本量变化时，模型性能（如 Accuracy）曲线                                                            | 用于评估半监督效果：带标签样本少时，半监督应明显优于仅监督 |

---

> **练习**：
>
> 1. 以 MNIST 数据集为例，画出不同带标签比例（1%、5%、10%、20%）下，仅用监督学习与半监督学习的 Accuracy 学习曲线，并分析差异。

---

### 2.3 无标签数据的“负面效应”与验证

* **错误伪标签累积**

  * 在自训练等方法中，早期伪标签错误会被放大，可能导致模型偏移。
  * **对策**：动态阈值、只选 Top‑k 高置信样本、引入混合基学习器。

* **分布偏移（Distribution Shift）**

  * 若无标签数据与带标签数据分布不一致，半监督方法可能误导决策边界。
  * **对策**：先做分布匹配（如核密度估计），或只使用与带标签分布相近的子集。

* **验证策略**

  * **交叉验证（Cross‑Validation）**：只在带标签上做 k 折验证；无标签仅用于训练。
  * **留一法（Hold‑out）**：预留部分带标签样本测试，保持训练/测试集分离。

---

## 第3章：自训练（Self‑Training）与伪标签

---

### 3.1 方法原理

* **核心思想**：

  1. 利用少量带标签数据训练一个初始分类器（基学习器）；
  2. 将该分类器应用于无标签数据，对其预测置信度最高的若干样本打上“伪标签”；
  3. 将这些伪标签样本并入有标签集，重新训练分类器；
  4. 重复上述过程，直到满足停止条件（无标签样本耗尽、迭代次数达上限、伪标签数不再增加等）。

---

* **优点**：简单易实现，可与任意能输出置信度的分类器（如树模型、SVM、神经网络）结合。

* **缺点**：伪标签错误会被“放大”——若早期决策有偏差，后续迭代可能陷入错误循环（confirmation bias）。

---

### 3.2 算法流程

1. **初始化**

   * 训练集 $L = \{(x_i,y_i)\}_{i=1}^{l}$（带标签）
   * 无标签集 $U = \{x_j\}_{j=1}^{u}$
   * 选择基学习器 $h$（需能输出类别置信度或概率）

---

2. **迭代步骤**（直到满足停止条件）

   1. 在当前带标签集 $L$ 上训练分类器 $h$；
   2. 对所有 $x \in U$ 预测类别及置信度 $\hat{y},\,p(\hat{y}|x)$；
   3. 按置信度排序，选出满足策略（如 $p(\hat{y}|x)\geq \tau$ 或 Top‑k）的样本子集 $S$；
   4. 将 $(x, \hat{y})$ 加入 $L$，并从 $U$ 中移除；
   5. 如无新样本被选入或迭代次数超过阈值，则停止。

3. **输出**：最终分类器 $h$。

---

### 3.3 伪标签策略

* **固定阈值（Fixed Threshold）**：只采信置信度 ≥ $\tau$ 的预测，常见取 $\tau=0.8$ 或更高。
* **动态阈值（Dynamic Threshold）**：随迭代逐步放宽或收紧阈值，如每迭代一次阈值下调 0.02。
* **Top‑k 选择**：每次只取置信度最高的 k 个样本。
* **类别平衡（Class‑Balanced Sampling）**：在 Top‑k 中保持各类样本数量比例与原始带标签集一致，防止类别偏移。

---

### 3.4 算法实现（Scikit‑Learn 示例）

```python
from sklearn.semi_supervised import SelfTrainingClassifier
from sklearn.svm import SVC
from sklearn.datasets import make_moons
from sklearn.metrics import accuracy_score
import numpy as np

# 1. 数据准备
X, y = make_moons(n_samples=300, noise=0.1, random_state=42)
rng = np.random.RandomState(0)
# 随机隐藏 70% 标签，用 −1 表示无标签
y_partial = np.copy(y)
mask = rng.rand(len(y)) < 0.7
y_partial[mask] = -1

# 2. 构造 Self‑Training 分类器
base_clf = SVC(kernel='rbf', probability=True, gamma=10)
self_training = SelfTrainingClassifier(
    base_clf,
    threshold=0.8,       # 固定阈值
    criterion='probability',  
    max_iter=10
)

# 3. 训练
self_training.fit(X, y_partial)

# 4. 评估
y_pred = self_training.predict(X)
print("Self‑Training 准确率：", accuracy_score(y, y_pred))
```

---

### 3.5 风险与改进

| 问题                         | 原因                      | 对策                                               |
| -------------------------- | ----------------------- | ------------------------------------------------ |
| 错误伪标签累积（confirmation bias） | 基学习器早期预测错误，伪标签被持续放大     | 使用更严格的阈值；动态阈值；只选 Top‑k；引入校正（temperature scaling） |
| 类别偏移（Class Imbalance）      | 置信度高样本往往集中在主流类别         | 类别平衡采样；在每轮只选各类 Top‑k                             |
| 分布偏移（Domain Shift）         | 无标签数据与带标签数据分布不一致        | 先做分布对齐（如核密度估计、领域对抗）；只用与带标签分布相似的子集                |
| 过拟合于伪标签                    | 随轮数增多，模型越来越依赖伪标签，泛化能力下降 | 加入早停（early stopping）；在训练中混入一定比例的原始带标签样本          |

---

### 3.6 实战练习

1. **阈值对比实验**：在同一数据集上，比较不同固定阈值 $\{0.7,\,0.8,\,0.9\}$ 下的 Self‑Training 准确率与伪标签数量增长曲线。
2. **基学习器对比**：分别使用决策树（DecisionTreeClassifier）、SVM、简单的多层感知机（MLPClassifier）作为基学习器，比较半监督效果。
3. **平衡与不平衡**：构造一个类别高度不平衡的二分类数据集，比较“固定阈值”与“类别平衡 Top‑k”策略的效果差异。

---

## 第4章：协同训练（Co‑Training）

---

### 4.1 多视图架构（Multi‑View Framework）

* **概念**：将样本的特征分为两组或多组“视图”（views），例如网页数据可分为“URL 特征”与“内容特征”；图像可分为“颜色直方图”与“纹理特征”。
* **视图条件**：理想情况下，两个视图应满足

  1. **条件独立性**：在给定标签的条件下，视图间相互独立；
  2. **充分性**：每个视图都足够预测标签。

---

### 4.2 算法流程

1. **初始化**

   * 带标签集 $L$，无标签集 $U$。
   * 将特征集拆分为视图 $V_1$ 与 $V_2$。
   * 在 $V_1$ 上训练分类器 $h_1$，在 $V_2$ 上训练分类器 $h_2$。

---

2. **迭代互标**

   * 对所有 $x \in U$，使用 $h_1$ 在视图 $V_1$ 上预测，并选出置信度最高的 $k$ 个样本及其预测标签，加入带标签集；
   * 同时，使用 $h_2$ 在视图 $V_2$ 上选出另一组 $k$ 个高置信样本及伪标签，加入带标签集；
   * 从 $U$ 中移除这两批样本；
   * 在新带标签集上分别重训练 $h_1$ 与 $h_2$。
   * 重复直到满足停止条件（迭代轮数、无标签集耗尽等）。

---

### 4.3 理论基础

* **视图条件独立性** 使两个分类器在不同特征子空间上的错误不相关，互标可减少 confirmation bias。
* **性能保证**：在理想视图条件下，Co‑Training 的错误率可随着迭代下降，并在一定轮数后收敛。

---

### 4.4 实战示例（Python 伪代码）

```python
import numpy as np
from sklearn.tree import DecisionTreeClassifier

# 假定 X1, X2 是两视图特征； y_partial 为部分标签，-1 表示无标签
# L_idx: 当前带标签样本索引； U_idx: 无标签样本索引

h1 = DecisionTreeClassifier()  # 视图1分类器
h2 = DecisionTreeClassifier()  # 视图2分类器

for iter in range(max_iter):
    # 1. 在各自视图与带标签集上训练
    h1.fit(X1[L_idx], y[L_idx])
    h2.fit(X2[L_idx], y[L_idx])
    
    # 2. 预测无标签集置信度
    probs1 = h1.predict_proba(X1[U_idx])
    probs2 = h2.predict_proba(X2[U_idx])
    
    # 3. 选取各自视图 Top‑k 样本
    idx1 = np.argsort(probs1.max(axis=1))[-k:]
    idx2 = np.argsort(probs2.max(axis=1))[-k:]
    
    # 4. 将选中样本及伪标签加入带标签集
    new_idx1 = U_idx[idx1]
    new_idx2 = U_idx[idx2]
    y[new_idx1] = probs1[idx1].argmax(axis=1)
    y[new_idx2] = probs2[idx2].argmax(axis=1)
    
    # 5. 更新 L_idx 与 U_idx
    L_idx = np.concatenate([L_idx, new_idx1, new_idx2])
    U_idx = np.setdiff1d(U_idx, np.concatenate([new_idx1, new_idx2]))
    
    if len(U_idx) == 0:
        break
```

---

### 4.5 局限与注意事项

| 问题    | 说明                 | 对策                            |
| ----- | ------------------ | ----------------------------- |
| 视图不独立 | 实际数据中常难满足条件独立性     | 选择尽量正交的特征集；或降维后再拆分视图          |
| 视图不充分 | 某一视图信息不足以支撑准确预测    | 增加视图数量；引入更多特征或使用预训练模型提取视图     |
| 伪标签冲突 | 两个分类器可能对同一样本给出不同标签 | 只取相互一致的伪标签；或为冲突样本保留为无标签直到后续轮次 |
| 计算成本  | 两个模型交替训练，训练时间接近两倍  | 可使用轻量级基学习器；控制每轮新增样本数 k        |

---

## 第5章：图‑基方法（Graph‑Based Methods）

---

### 5.1 样本图构建

1. **顶点与边**

   * 将每个样本点视为图中的一个顶点。
   * 顶点间的相似度决定边的权重。

2. **相似度计算**

   * **kNN 图**：对于每个点，连接其 k 个最近邻；权重可设为常数或高斯核。
   * **全连接图（RBF）**：

     $$
       W_{ij} = \exp\!\bigl(-\tfrac{\|x_i - x_j\|^2}{2\sigma^2}\bigr)
     $$

     所有样本两两之间都有一条边（稠密、重计算）。

---

3. **度矩阵与拉普拉斯矩阵**

   * 度矩阵 $D$：对角元素 $D_{ii} = \sum_j W_{ij}$。
   * 无向图拉普拉斯 $L = D - W$。
   * 归一化拉普拉斯：

     $$
       L_{\text{sym}} = I - D^{-\tfrac12} W D^{-\tfrac12},\quad
       L_{\text{rw}} = I - D^{-1}W.
     $$

---

### 5.2 Label Propagation / Label Spreading

1. **Label Propagation（LP）**

   * 固定带标签点的标签，不在迭代中改变。
   * 无标签点的标签通过图结构迭代更新，直至收敛。
   * 更新公式：

     $$
       F^{(t+1)} = D^{-1}W\,F^{(t)},\quad
       F^{(0)} = Y
     $$

     其中 $Y$ 包含带标签点的 one‐hot 向量，且对标记点 $Y_i$ 固定。

---

2. **Label Spreading（LS）**

   * 核心思想同 LP，但在每次迭代后，对整条 $F$ 向量做归一化，防止数值发散。
   * 引入平滑参数 $\alpha$：

     $$
       F^{(t+1)} = \alpha S\,F^{(t)} + (1-\alpha)\,Y,\quad S = D^{-\tfrac12} W D^{-\tfrac12}.
     $$

3. **超参数**

   * 核函数带宽 $\sigma$（或 kNN 的 k）。
   * 传播迭代次数或收敛阈值。
   * Label Spreading 中的 $\alpha\in(0,1)$。

---

### 5.3 理论推导

* **平滑能量最小化**

  $$
    \mathcal{E}(F) = \tfrac12 \sum_{i,j} W_{ij}\,\|F_i - F_j\|^2
  $$

  在约束带标签点固定的条件下，求解使 $\mathcal{E}(F)$ 最小的 $F$，其闭式解为：

  $$
    F = (I - \alpha S)^{-1} Y.
  $$

* **与拉普拉斯正则化**
  将半监督视为在监督损失上加入图正则项：

  $$
    \min_f \sum_{i\in L}\! \ell\bigl(f(x_i),y_i\bigr)
    + \lambda \sum_{i,j}W_{ij}\|f(x_i)-f(x_j)\|^2.
  $$

---

### 5.4 实战示例（Scikit‑Learn）

```python
from sklearn import datasets
from sklearn.semi_supervised import LabelPropagation, LabelSpreading
from sklearn.metrics import accuracy_score
import numpy as np

# 数据加载与标签隐藏
X, y = datasets.make_circles(n_samples=300, noise=0.1, factor=0.4)
rng = np.random.RandomState(0)
y_partial = np.copy(y)
mask = rng.rand(len(y)) < 0.7
y_partial[mask] = -1  # 隐藏标签

# Label Propagation
lp = LabelPropagation(kernel='rbf', gamma=20, max_iter=1000)
lp.fit(X, y_partial)
pred_lp = lp.transduction_

# Label Spreading
ls = LabelSpreading(kernel='knn', n_neighbors=10, alpha=0.8, max_iter=1000)
ls.fit(X, y_partial)
pred_ls = ls.transduction_

print("LP Accuracy:", accuracy_score(y, pred_lp))
print("LS Accuracy:", accuracy_score(y, pred_ls))
```

---

### 5.5 风险与改进

| 问题                  | 说明                             | 对策                           |
| ------------------- | ------------------------------ | ---------------------------- |
| 图稠密且规模大             | 全连接图存储与计算开销 $\mathcal{O}(n^2)$ | 用 kNN 构建稀疏图；近似最近邻加速（如 FAISS） |
| 超参数敏感               | $\sigma$、k、$\alpha$ 参数选择影响结果   | 网格搜索；基于验证集的自适应选择             |
| 标签泄露（Label Leakage） | LP 固定带标签点，可能使无标签点过度依赖这些节点      | Label Spreading 加入归一化与回退项    |
| 分布不均                | 如果无标签数据分布与带标签集中簇结构不匹配，传播效果会受损  | 先做子空间聚类或分块传播；或在传播前剔除分布偏离点    |

---

## 第6章：基于生成模型的方法

---

### 6.1 生成式半监督框架

1. **基本思路**

   * 假设数据由潜在变量 $z$ 和标签 $y$ 共同生成：

     $$
       p(x, y, z) = p(y)\,p(z)\,p(x \mid y, z).
     $$
   * 通过最大化带标签与无标签数据的边缘似然，联合学习生成模型与分类器。

2. **EM 算法简介**

   * **E 步**：对无标签数据 $x$ 估计后验 $q(y,z\mid x)$。
   * **M 步**：固定 $q$ 后，最大化完整数据对数似然：
     $\mathbb{E}_{q(y,z\mid x)}\bigl[\log p(x,y,z)\bigr]$。
   * 迭代至收敛。

---

### 6.2 深度生成模型：半监督 VAE（Model M1+M2）

1. **M1：纯无监督 VAE**

   * **结构**：编码器 $q_\phi(z\mid x)$，解码器 $p_\theta(x\mid z)$。
   * **ELBO**：

     $$
       \mathcal{L}_{\text{VAE}}(x) = 
       \mathbb{E}_{q_\phi(z\mid x)}[\log p_\theta(x\mid z)]
       - \mathrm{KL}\bigl(q_\phi(z\mid x)\,\|\,p(z)\bigr).
     $$

---

2. **M2：半监督扩展**

   * **结构**：在 M1 基础上加入类别变量 $y$：

     * 编码器 $q_\phi(z\mid x,y)$，
     * 预测器 $q_\phi(y\mid x)$，
     * 解码器 $p_\theta(x\mid z,y)$。
   * **标签分支**：对带标签数据用联合 ELBO；对无标签数据对所有可能 $y$ 做加权：

     $$
       \mathcal{L}(x,y) = 
       \mathbb{E}_{q(z\mid x,y)}[\log p(x\mid z,y)]
       - \mathrm{KL}[\,q(z\mid x,y)\,\|\,p(z)\,],  
     $$

     $$
       \mathcal{U}(x) = 
       \sum_y q(y\mid x)\,\mathcal{L}(x,y)
       + \mathcal{H}\bigl(q(y\mid x)\bigr),
     $$

     其中 $\mathcal{H}$ 是熵项。

---

3. **总目标**：

   $$
     \max_{\theta,\phi}\;\sum_{(x,y)\in L}\mathcal{L}(x,y)
     + \sum_{x\in U}\mathcal{U}(x)
     + \alpha\,\mathbb{E}_{(x,y)\in L}[\log q_\phi(y\mid x)],
   $$

   最后一项为监督分类项，$\alpha$ 控制分类器权重。

---

### 6.3 Ladder Network

1. **结构概览**

   * 在普通自动编码器基础上，每一层都添加噪声输入，并构造“跳跃连接”（lateral connection）将无噪声层激活引导去噪。
   * 同时在顶层加分类分支，联合最小化分类损失与去噪重构损失。

---

2. **损失函数**

   $$
     \mathcal{L} = \mathcal{L}_{\text{sup}}(y,\hat y)
       + \sum_{l=0}^L \lambda_l\,\bigl\|h^{(l)} - \tilde h^{(l)}\bigr\|^2,
   $$

   * $\mathcal{L}_{\text{sup}}$：带标签分类交叉熵
   * $\tilde h^{(l)}$：带噪声编码器第 $l$ 层激活，$h^{(l)}$ 为干净数据对应激活
   * $\lambda_l$：各层重构损失权重

3. **核心优势**：

   * 同时利用有标签与无标签数据学习更鲁棒的特征表示；
   * 去噪机制对流形结构有隐式正则化效果。

---

### 6.4 实战示例（PyTorch：半监督 VAE M1+M2 骨架）

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# 编码器 q(z|x,y) 与 q(y|x)
class Encoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, z_dim, n_classes):
        super().__init__()
        self.fc_x = nn.Linear(input_dim, hidden_dim)
        self.fc_y = nn.Linear(n_classes, hidden_dim)
        self.fc_mu = nn.Linear(hidden_dim, z_dim)
        self.fc_logvar = nn.Linear(hidden_dim, z_dim)
        self.fc_qy = nn.Linear(input_dim, n_classes)
    def forward(self, x, y=None):
        # 预测类别概率 q(y|x)
        logits = self.fc_qy(x)
        qy = F.softmax(logits, dim=-1)
        # 若有标签 y，则使用 one-hot 否则采样或使用 qy
        y_onehot = y if y is not None else qy
        h = F.relu(self.fc_x(x) + self.fc_y(y_onehot))
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return qy, mu, logvar

# 解码器 p(x|z,y)
class Decoder(nn.Module):
    def __init__(self, z_dim, hidden_dim, output_dim, n_classes):
        super().__init__()
        self.fc_z = nn.Linear(z_dim, hidden_dim)
        self.fc_y = nn.Linear(n_classes, hidden_dim)
        self.fc_out = nn.Linear(hidden_dim, output_dim)
    def forward(self, z, y_onehot):
        h = F.relu(self.fc_z(z) + self.fc_y(y_onehot))
        return torch.sigmoid(self.fc_out(h))

# 重参数化采样
def reparameterize(mu, logvar):
    std = torch.exp(0.5 * logvar)
    eps = torch.randn_like(std)
    return mu + eps * std

# 单步训练示例
def train_step(x, y, encoder, decoder, optimizer, n_classes, alpha=0.1):
    # 编码器前向
    qy, mu, logvar = encoder(x, y)
    # 对无标签分支，qy 用于后续计算
    # 采 z
    z = reparameterize(mu, logvar)
    # 解码
    x_recon = decoder(z, F.one_hot(y, n_classes).float() if y is not None else qy)
    # 计算重构与 KL
    recon_loss = F.binary_cross_entropy(x_recon, x, reduction='sum')
    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    # 对无标签数据额外加熵项
    if y is None:
        loss = torch.sum(qy * (recon_loss + kl)) + torch.sum(qy * torch.log(qy + 1e-8))
    else:
        # 带标签：ELBO + 分类交叉熵
        ce = F.cross_entropy(encoder.fc_qy(x), y)
        loss = recon_loss + kl + alpha * ce
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return loss.item()
```

> **注**：以上仅为核心骨架，完整实现需构造带标签与无标签的混合批次、权重归一化等。

---

## 第7章：Transductive SVM 与其它进阶方法

---

### 7.1 Transductive SVM（TSVM）

1. **基本思想**

   * 与标准 SVM 不同，TSVM 同时考虑带标签样本 $(X_l,Y_l)$ 与无标签样本 $X_u$，试图找到一个决策边界，使得带标签数据分类正确，同时边界尽量远离无标签数据点的高密度区域。

2. **优化目标**

   $$
   \min_{w,b,\hat{y}_u}\;\frac{1}{2}\|w\|^2 + C_l \sum_{i\in L} \ell\bigl(y_i, w^\top x_i + b\bigr)
     + C_u \sum_{j\in U} \ell\bigl(\hat{y}_j, w^\top x_j + b\bigr),
   $$

   * 其中 $\hat{y}_j\in\{\pm1\}$ 为对无标签样本的伪标签；
   * $\ell$ 通常为铰链损失；
   * $C_l, C_u$ 分别控制带标签与无标签惩罚权重。

---

3. **求解策略**

   * 由于目标对 $\hat{y}_u$ 二元变量非凸，可采用交替优化（CCCP 或交替最小化）：

     1. 固定 $\hat{y}_u$，解凸问题求 $w,b$；
     2. 固定 $w,b$，对每个 $x_j\in U$ 取 $\hat{y}_j = \mathrm{sign}(w^\top x_j + b)$；
     3. 迭代至收敛。

4. **优缺点**

   * **优点**：能显式利用无标签数据分布，提升二分类边界质量；
   * **缺点**：求解成本高，容易陷入局部最优，对初始伪标签敏感。

---

### 7.2 对抗式半监督（GAN‑Based SSL）

1. **核心思想**

   * 将生成对抗网络（GAN）框架扩展到半监督场景：判别器不仅区分“真/假”，还分类真实样本的具体类别。
   * 判别器输出 $K+1$ 类：前 $K$ 类对应真实标签，最后一类对应生成样本。

2. **常见模型**

   * **CatGAN**：通过最大化真实数据在真实类别上的不确定性与生成数据在伪“假”类别上的确定性。
   * **TripleGAN**：同时训练生成器、判别器与分类器，分类器与判别器共享部分参数；联合优化分类与对抗目标。

---

3. **判别器损失**（以 $K$-类+生成类为例）

   $$
     \mathcal{L}_D = -\mathbb{E}_{(x,y)\in L}\!\log p_D(y\mid x)
                 -\mathbb{E}_{x\in U}\!\log p_D(y_{\text{real}}\le K\mid x)
                 -\mathbb{E}_{z\sim p_z}\!\log p_D(y=K+1\mid G(z)),
   $$

   * $\;y_{\text{real}}\le K$ 表示真实数据为前 $K$ 类；
   * 生成器则通过对抗目标学习生成难以被判别器识别的样本。

4. **优缺点**

   * **优点**：可从无标签数据中学习数据分布，提升判别器的表示能力；
   * **缺点**：训练不稳定，超参数多，对网络结构敏感。

---

### 7.3 元学习（Meta‑Learning）在半监督中的应用

1. **背景**

   * 元学习旨在学习“如何学习”，通过多任务训练获得快速适应新任务的能力。

2. **MAML‑SSL**

   * 在 MAML（Model‑Agnostic Meta‑Learning）框架下，将每个任务的少量带标签与大量无标签数据一起用于任务内更新：

     1. **任务内**：使用带标签和无标签数据的半监督损失（如伪标签或图正则）更新模型；
     2. **任务间**：在多个类似任务上元优化，学习初始参数 $\theta$。

---

3. **优势**

   * 对新任务仅需少量带标签数据即可通过半监督快速适应；
   * 元学习优化的初始化对半监督迭代更稳健，减少伪标签偏差。

---

### 7.4 实战演示

1. **TSVM 训练（SciKit‑Learn–LibSVM 接口示例）**

   ```python
   from sklearn.datasets import make_circles
   from sklearn.svm import SVC

   # 带标签与无标签合并：无标签用标签 0 或 1 任选，C_u 小
   X, y = make_circles(n_samples=300, noise=0.1)
   # 仅示意：实际需使用专门的 TSVM 库（如 transductive SVM 的 SVMlight 接口）
   clf = SVC(kernel='rbf', C=1.0, probability=True)
   clf.fit(X, y)  # TSVM 实际训练需替换为专用算法
   ```

2. **GAN‑SSL 案例框架（PyTorch 伪代码）**

   ```python
   # 判别器输出维度为 K+1
   logits = D(x)            # 对真实带标签数据
   loss_sup = CE(logits[:K], y)
   loss_unsup_real = -logsumexp(logits[:K], dim=1).mean()  # 真实数据应非生成类
   loss_unsup_fake = F.cross_entropy(logits, K)           # 生成数据为生成类
   loss_D = loss_sup + loss_unsup_real + loss_unsup_fake
   ```

3. **MAML‑SSL 流程概览**

   * **外环**：采样多个少样本任务；
   * **内环**：对每个任务，用半监督损失（监督+伪标签或图正则）更新几个梯度步骤；
   * **元更新**：聚合任务内更新后的模型，优化初始参数。

---

### 7.5 实战练习

1. **TSVM vs SVM 对比**：在一个二分类数据集上，分别训练 SVM 与 TSVM，比较决策边界与分类精度。
2. **GAN‑SSL 实验**：使用 CIFAR‑10 子集，构建一个简单的指向式 GAN，观察半监督判别器在带标签比例 10% 时的性能提升。
3. **MAML‑SSL 探索**：在 Omniglot 或 Mini‑ImageNet 任务上实现 MAML‑SSL，比较纯 MAML、纯半监督与 MAML‑SSL 的适应效果。

---

## 附录：课程总结与选型指南

| 方法类别  | 典型算法                        | 优势             | 适用场景               |
| ----- | --------------------------- | -------------- | ------------------ |
| 自训练   | Self‑Training, Pseudo‑Label | 简单易实现，多模型可用    | 基学习器可靠、无标签分布与带标签相似 |
| 协同训练  | Co‑Training                 | 利用多视图减少偏差      | 数据有自然多视图，如多模态      |
| 图‑基方法 | Label Propagation/Spreading | 平滑全局结构，显式图正则   | 数据簇明显，流形结构清晰       |
| 生成模型  | Semi‑Supervised VAE, Ladder | 深度特征学习，显式建模    | 高维复杂数据，如图像、语音      |
| 边际优化  | Transductive SVM            | 最大化间隔避密度，理论优雅  | 二分类、小规模无标签数据       |
| 对抗式方法 | CatGAN, TripleGAN           | 强分布学习能力，提升表示   | 对抗训练可行且算力充足        |
| 元学习扩展 | MAML‑SSL                    | 快速适应新任务，少标签效果好 | 少样本多任务场景，任务间差异不大   |

---

> **选型建议**：
>
> * **小规模、结构简单**：优先自训练或 TSVM；
> * **自然多视图**：协同训练；
> * **大规模高维**：生成模型或对抗式半监督；
> * **少样本新任务**：元学习结合半监督。
