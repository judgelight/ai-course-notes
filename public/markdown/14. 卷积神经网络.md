---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第14讲：卷积神经网络 第 1 课

---

## 一、什么是卷积神经网络（CNN）

卷积神经网络（Convolutional Neural Network, CNN）是一类专门用于处理具有**网格拓扑结构数据**（如图像、语音）的深度学习模型。

* **网格拓扑数据**：图像是二维像素格子，语音是时间序列的采样点。
* CNN 的核心思想：利用 **局部感受野（local receptive field）** 和 **权值共享（weight sharing）** 来提取特征。

---

### 传统神经网络的局限性

* 多层感知机（MLP）每个神经元与上一层的所有节点全连接，参数数量庞大。
* 假设输入是一张 $32\times32\times3$ 的彩色图像，展平成 3072 个像素点，若第一层有 1000 个神经元，参数量就是 $3{,}072 \times 1{,}000 = 3$ 百万。
* 这种连接方式既浪费计算，又难以捕捉图像的空间局部结构。

### CNN 的优势

* **局部连接**：只对局部区域进行卷积运算。
* **权值共享**：同一个卷积核应用于整张图像，参数量大幅减少。
* **平移不变性**：即使物体在图像中位置发生变化，CNN 仍能识别。

---

## 二、卷积层（Convolutional Layer）

卷积层是 CNN 的核心，用卷积核对输入图像做滑动窗口运算，从而提取局部特征。

### 1. 卷积核（Filter）

* 一个卷积核大小为 $k \times k \times C$，其中 $C$ 是输入的通道数。
* 例如：输入是一张 RGB 图像 $32\times32\times3$，卷积核大小 $3\times3$，则卷积核的参数量是 $3\times3\times3=27$。

---

### 2. 卷积运算公式

对输入特征图 $X$，卷积核权重 $W$，偏置 $b$，输出特征图 $Y$ 的计算为：

$$
Y(i,j) = \sum_{m=1}^{k}\sum_{n=1}^{k}\sum_{c=1}^{C} X(i+m,j+n,c)\cdot W(m,n,c) + b
$$

### 3. 超参数

* **卷积核大小 $k$**：常用 $3\times3$、$5\times5$。
* **步长 Stride**：每次卷积核移动的像素数，影响输出特征图大小。
* **填充 Padding**：是否在输入边缘补零，保持尺寸不变。

---

### 4. 特征提取

* 前层卷积核可能学习边缘、线条；
* 深层卷积核可能学习复杂形状、物体结构。

![](https://s.ar8.top/img/picgo/20250820194752003.webp)

---

![](https://s.ar8.top/img/picgo/20250820194905230.webp)

---

## 三、池化层（Pooling Layer）

池化层用于**降低特征图的空间维度**，减少计算量并提高模型的平移不变性。

### 1. 常见池化方式

* **最大池化（Max Pooling）**：取窗口内的最大值。
* **平均池化（Average Pooling）**：取窗口内的平均值。

### 2. 公式

对 $p\times p$ 的池化窗口：

$$
Y(i,j) = \max_{0 \le m,n < p} X(i+m,j+n)
$$

---

### 3. 示例

输入 $4\times4$ 特征图，使用 $2\times2$ 最大池化，输出变为 $2\times2$，同时保留显著特征。

![](https://s.ar8.top/img/picgo/20250820195152581.webp)

---

## 四、归一化层（Normalization Layer）

深度网络训练时可能出现梯度消失或分布偏移问题。归一化层可以稳定训练，加快收敛。

### 1. 批归一化（Batch Normalization, BN）

在小批量训练时，对每一层的激活值 $x$ 做标准化处理：

$$
\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}
$$

再进行缩放和平移：

$$
y = \gamma \hat{x} + \beta
$$

其中 $\mu, \sigma^2$ 来自小批量数据，$\gamma, \beta$ 是可学习参数。

---

### 2. 其他归一化

* **Layer Normalization**：在样本维度做归一化。
* **Group Normalization**：在通道分组内做归一化。

---

## 五、全连接层（Fully Connected Layer）

CNN 的后半部分一般是若干个全连接层，用于综合所有特征，完成分类或回归任务。

### 计算公式

$$
y = f(Wx + b)
$$

其中 $W$ 是权重矩阵，$f$ 是激活函数。
全连接层打平特征图，把局部信息整合为全局特征。

---

## 六、激活函数（Activation Function）

非线性激活函数使神经网络能够拟合复杂函数。

* **Sigmoid**：$f(x)=\frac{1}{1+e^{-x}}$，但可能导致梯度消失。
* **Tanh**：值域 \[-1,1]，比 Sigmoid 居中，但也有梯度问题。
* **ReLU**：$f(x)=\max(0,x)$，计算简单，缓解梯度消失，是 CNN 的主流选择。
* **Leaky ReLU / ELU / GELU**：ReLU 的改进，解决 “死亡神经元” 问题。

---

## 七、CNN 的反向传播（Backpropagation）

CNN 的训练依赖反向传播算法（Backpropagation, BP），核心是链式法则。

### 1. 卷积层的梯度计算

* **对权重的梯度**：输入与误差的卷积。
* **对输入的梯度**：误差与翻转后的卷积核做卷积。

### 2. 池化层的梯度

* **最大池化**：梯度只传给最大值所在位置。
* **平均池化**：梯度均分给池化窗口内所有位置。

---

### 3. 更新公式

$$
W \leftarrow W - \eta \frac{\partial L}{\partial W}
$$

其中 $\eta$ 是学习率，$L$ 是损失函数。

---

## 八、总结：CNN 的典型工作流程

1. 输入图像（如 $32\times32\times3$）
2. **卷积层** 提取局部特征
3. **池化层** 降低空间分辨率
4. 重复多次卷积+池化，得到高层语义特征
5. **全连接层** 综合特征
6. **Softmax 层** 输出分类概率

