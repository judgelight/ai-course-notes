---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第14講：畳み込みニューラルネットワーク 第1課

---

## 一、畳み込みニューラルネットワーク（CNN）とは

畳み込みニューラルネットワーク（Convolutional Neural Network, CNN）は、**格子状のトポロジー構造を持つデータ**（画像や音声など）を処理するために特化した深層学習モデルです。

* **格子状トポロジーデータ**：画像は2次元の画素格子、音声は時系列のサンプリング点。
* CNNの核心的なアイデア：**局所受容野（local receptive field）** と **重み共有（weight sharing）** を利用して特徴を抽出する。

---

### 従来のニューラルネットワークの限界

* 多層パーセプトロン（MLP）は各ニューロンが前層の全ノードと全結合し、膨大なパラメータが必要。
* 入力が $32\times32\times3$ のカラー画像の場合、フラット化すると 3072 次元。第1層に1000個のニューロンがあると、パラメータ数は $3{,}072 \times 1{,}000 = 300万$。
* この接続方式は計算の無駄が多く、画像の空間的局所構造を捉えるのが難しい。

### CNNの利点

* **局所接続**：局所領域に対してのみ畳み込み演算を行う。
* **重み共有**：同じフィルタを画像全体に適用し、パラメータ数を大幅に削減。
* **平行移動不変性**：対象が画像内で位置を変えても認識可能。

---

## 二、畳み込み層（Convolutional Layer）

畳み込み層はCNNの核心であり、フィルタを用いたスライディングウィンドウ演算で局所特徴を抽出する。

### 1. フィルタ（カーネル）

* フィルタの大きさは $k \times k \times C$、ここで $C$ は入力チャネル数。
* 例：入力が $32\times32\times3$ のRGB画像、フィルタが $3\times3$ の場合、パラメータ数は $3\times3\times3=27$。

---

### 2. 畳み込み演算の数式

入力特徴マップ $X$、フィルタ重み $W$、バイアス $b$ に対し、出力特徴マップ $Y$ は以下のように計算される：

$$
Y(i,j) = \sum_{m=1}^{k}\sum_{n=1}^{k}\sum_{c=1}^{C} X(i+m,j+n,c)\cdot W(m,n,c) + b
$$

### 3. ハイパーパラメータ

* **フィルタサイズ $k$**：$3\times3$、$5\times5$ がよく使われる。
* **ストライド（Stride）**：フィルタを移動させるピクセル数。出力サイズに影響。
* **パディング（Padding）**：入力の端をゼロで埋め、サイズを保持するかどうか。

---

### 4. 特徴抽出

* 浅い層のフィルタはエッジや線を学習。
* 深い層のフィルタは複雑な形状や物体構造を学習。

![](https://s.ar8.top/img/picgo/20250820194752003.webp)

---

![](https://s.ar8.top/img/picgo/20250820194905230.webp)

---

## 三、プーリング層（Pooling Layer）

プーリング層は**特徴マップの空間次元を縮小**し、計算量削減と平行移動不変性の向上に役立つ。

### 1. よく使われるプーリング方法

* **最大プーリング（Max Pooling）**：ウィンドウ内の最大値を取る。
* **平均プーリング（Average Pooling）**：ウィンドウ内の平均値を取る。

### 2. 数式

プーリングウィンドウが $p\times p$ の場合：

$$
Y(i,j) = \max_{0 \le m,n < p} X(i+m,j+n)
$$

---

### 3. 例

入力 $4\times4$ 特徴マップに $2\times2$ 最大プーリングを適用すると、出力は $2\times2$ になり、重要な特徴を保持する。

![](https://s.ar8.top/img/picgo/20250820195152581.webp)

---

## 四、正規化層（Normalization Layer）

深いネットワークの学習では勾配消失や分布シフトが起きる可能性がある。正規化層は学習を安定化し、収束を速める。

### 1. バッチ正規化（Batch Normalization, BN）

ミニバッチ学習時、各層の活性値 $x$ を標準化：

$$
\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}
$$

その後、スケーリングとシフト：

$$
y = \gamma \hat{x} + \beta
$$

ここで $\mu, \sigma^2$ はミニバッチから算出、$\gamma, \beta$ は学習可能パラメータ。

---

### 2. その他の正規化

* **Layer Normalization**：サンプル次元で正規化。
* **Group Normalization**：チャネルをグループに分けて正規化。

---

## 五、全結合層（Fully Connected Layer）

CNNの後半では通常、いくつかの全結合層を用いて全特徴を統合し、分類や回帰を行う。

### 計算式

$$
y = f(Wx + b)
$$

ここで $W$ は重み行列、$f$ は活性化関数。
全結合層は特徴マップをフラット化し、局所情報をグローバル特徴に統合する。

---

## 六、活性化関数（Activation Function）

非線形活性化関数により、ニューラルネットワークは複雑な関数を近似できる。

* **Sigmoid**：$f(x)=\frac{1}{1+e^{-x}}$、勾配消失が起きやすい。
* **Tanh**：値域 \[-1,1]、Sigmoid より中心化されるが勾配問題あり。
* **ReLU**：$f(x)=\max(0,x)$、計算が簡単で勾配消失を緩和。CNNで主流。
* **Leaky ReLU / ELU / GELU**：ReLUの改良型で「死んだニューロン」問題を軽減。

---

## 七、CNNの誤差逆伝播（Backpropagation）

CNNの学習は誤差逆伝播法（Backpropagation, BP）に依存し、鎖律則が核心。

### 1. 畳み込み層の勾配計算

* **重みに対する勾配**：入力と誤差の畳み込み。
* **入力に対する勾配**：誤差と反転したフィルタとの畳み込み。

### 2. プーリング層の勾配

* **最大プーリング**：勾配は最大値の位置にのみ伝達。
* **平均プーリング**：勾配はウィンドウ内の全位置に均等に分配。

---

### 3. 更新式

$$
W \leftarrow W - \eta \frac{\partial L}{\partial W}
$$

ここで $\eta$ は学習率、$L$ は損失関数。

---

## 八、まとめ：CNNの典型的な処理フロー

1. 入力画像（例：$32\times32\times3$）
2. **畳み込み層** で局所特徴を抽出
3. **プーリング層** で空間分解能を縮小
4. 畳み込み＋プーリングを繰り返し、高次特徴を獲得
5. **全結合層** で特徴を統合
6. **Softmax層** で分類確率を出力

