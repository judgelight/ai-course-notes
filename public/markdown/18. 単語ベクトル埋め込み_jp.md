---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第18講：単語ベクトル埋め込み

---

## 一、なぜ単語ベクトル埋め込みが必要か？

* **One-hot / TF-IDF の限界**

  * ベクトルが高次元疎（次元 = 語彙サイズ）
  * 単語間に意味的関係がない（“dog” と “cat” のベクトルは完全に独立）

* **単語ベクトル埋め込みの核心思想**

  * 各単語を **低次元の密ベクトル**（例：100 次元、300 次元）で表す
  * ベクトル空間で意味的関係を捉えることができる：

    $$
    \text{king} - \text{man} + \text{woman} \approx \text{queen}
    $$

---

![bg 70%](https://s.ar8.top/img/picgo/20250924170742451.webp)

---

## 二、分布仮説（Distributional Hypothesis）

* **核心的見解**：単語の意味は文脈によって決まる
* 「You shall know a word by the company it keeps.」
* 例：

  * “dog” → “bark, pet, animal” と共起
  * “cat” → “meow, pet, animal” と共起  
    → これにより意味的に近いとわかる

---

## 三、Word2Vec（2013，Google）

* Word2Vec は Google が 2013 年に提案した方法
* 目的：**各単語**を**低次元の数値ベクトル**に変換し、意味の近い単語同士をベクトル空間でも近づける
* 例：

  * 「猫」と「犬」→ ベクトル類似度が高い
  * 「猫」と「車」→ ベクトル類似度が低い
* Word2Vec の重要な考え方：**単語の意味は文脈から決まる**

---

## 四、Word2Vec の原理

* Word2Vec はルールベースではなく、小さなニューラルネットで「単語と文脈の関係」を学習
* 訓練方法：

  * 文を単語列に分割
  * **スライディングウィンドウ**で対象単語の文脈を観察
  * 学習モード：

    * **CBOW**：文脈から中心語を予測
    * **Skip-gram**：中心語から文脈を予測

---

例文：There is an apple on the table

処理後：["there","is","an","apple","on","the","table"]

---

### 1. 入力層（Input layer）

* 入力は **one-hot ベクトル**（長さ = 語彙サイズ $V$）
* 例：語彙 10,000 の場合、単語 *apple* のベクトルはある位置のみ 1、他は 0

---

### 2. 隠れ層（Hidden layer）

* 重み行列 $W_{V \times N}$

  * $V$ = 語彙サイズ
  * $N$ = 埋め込み次元（例：100、300）

* 計算：

  $$
  h = X \cdot W
  $$

  $X$ が one-hot のため、結果は $W$ のある 1 行 → 単語の **ベクトル表現**

---

### 3. 出力層（Output layer）

* 重み行列 $W'_{N \times V}$
* 計算：

  $$
  y = h \cdot W'
  $$

* 出力は長さ $V$ のベクトル → 各単語のスコア（その文脈で現れる確率）
* Softmax で正規化し確率分布に変換

---

![bg 70%](https://s.ar8.top/img/picgo/20250924171846139.webp)

---

### 2 つの重み行列の意味

* **$W$（入力層重み，$V \times N$）**

  * 各行が単語ベクトル
  * 学習後、通常この行列が **Embedding 表**として利用される

* **$W'$（出力層重み，$N \times V$）**

  * 学習時に必要、隠れ層から語彙空間への写像に利用
  * 学習後も意味を含むが、多くの応用では $W$ のみを使用
  * 一部の論文・ツール（gensim など）は **$W$ と $W'^T$ を平均**して最終ベクトルとする

---

## 五、CBOW モデル

* **タスク**：文脈から中心語を予測
* 例：

  * 文："There is an apple on the table"
  * 予測目標：中心語 "on"
  * 入力文脈：\["apple", "the"]

* 特徴：

  * 訓練が速く、高頻度語に有効
  * 低頻度語にはやや弱い

---
* 入力層はコンテキスト単語のワンホットベクトル → $W$ を通じて複数の単語ベクトルを取得 → 平均を取る → 隠れ層表現 $h$ を得る。  
* 出力層は中心単語の確率分布を予測する。  
* **目標**：実際の中心単語の確率が最も高くなるようにする。

![bg right:50% 90%](https://s.ar8.top/img/picgo/20250924174405533.webp)

---
## 六、Skip-gram モデル

* **タスク**：中心語から文脈を予測
* 例：

  * 文："There is an apple on the table"
  * 中心語："on"
  * 予測目標：\["apple", "the"]

* 特徴：

  * 低頻度語に強い
  * 実務でより多く利用される

---
  * 入力層は中心語であり、$W$ を通じてそのベクトル $h$ を得る。
  * 出力層は複数の文脈語を予測する。
  * **目標**：実際の文脈語の確率を最も高くすること。

![bg right:50% 90%](https://s.ar8.top/img/picgo/20250924174737514.webp)

---
## 七、GloVe とは？

* GloVe = **Global Vectors for Word Representation**
* 2014 年、スタンフォード大学が提案
* Word2Vec：**予測型**（文脈から単語を予測）
* GloVe：**統計型**（コーパス全体の共起を利用）

---

## 八、GloVe の原理

* コーパスから **単語共起頻度**を数える
* 「ice」は「cold」と共起、「steam」は「hot」と共起
* 共起行列を作り、ベクトルがこれを近似するよう学習

---

公式：

$$
\text{Loss} = \sum_{i,j} f(X_{ij}) (w_i^T \tilde{w_j} + b_i + \tilde{b_j} - \log X_{ij})^2
$$

* $X_{ij}$：単語 $i$ と $j$ の共起回数
* $w_i$：単語ベクトル

---

✅ **比較まとめ**

* **Word2Vec**：予測型 → 局所文脈に強い、訓練が速い
* **GloVe**：統計型 → 全体的な分布を反映、安定性が高い

---

### コード例：Word2Vec

```python
# pip install gensim
from gensim.models import Word2Vec

# ===== 1) Prepare data =====
sentences = [
    "there is an apple on the table",
    "an orange is on the desk",
    "i love natural language processing",
    "word embeddings capture semantics",
]
sentences = [s.split() for s in sentences]


# ===== 2) train Word2Vec：CBOW（sg=0） =====
model = Word2Vec(
    sentences,
    vector_size=100,
    window=5,
    min_count=1,
    sg=0,
    negative=10,
    epochs=20,
    workers=4
)

# ===== 3) Use：vector / most similar  =====
wv = model.wv  # KeyedVectors
print("vector(dim=100) of 'apple':", wv["apple"][:8])

print("most similar to 'apple':", wv.most_similar("apple", topn=5))

# ===== 4) sentence vector Sample =====
import numpy as np
def sent_vec(tokens, wv):
    vecs = [wv[w] for w in tokens if w in wv]
    return np.mean(vecs, axis=0) if vecs else np.zeros(wv.vector_size)

v1 = sent_vec("there is an apple".split(), wv)
v2 = sent_vec("an orange is on the desk".split(), wv)
cos = float(np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2)+1e-9))
print("cosine(sent1, sent2) =", cos)

# ===== 5) Save / reload =====
wv.save_word2vec_format("cbow_vectors.txt")
# from gensim.models import KeyedVectors
# wv2 = KeyedVectors.load_word2vec_format("cbow_vectors.txt")

```

### 事前学習済み GloVe の利用

```python
import gensim.downloader as api

# 事前学習済み GloVe (50次元) をロード
glove = api.load("glove-wiki-gigaword-50")

print("ベクトル次元:", glove["cat"].shape)
print("cat vs dog 類似度:", glove.similarity("cat", "dog"))
print("king - man + woman ≈", glove.most_similar(positive=["king","woman"], negative=["man"], topn=1))

```