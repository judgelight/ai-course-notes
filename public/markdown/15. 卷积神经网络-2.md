---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第15讲：卷积神经网络 第 2 课

---

## 一、卷积层的变体

### 1. 深度可分离卷积（Depthwise Separable Convolution）

**思想**：把标准卷积分解为两步：

1. **Depthwise Convolution（逐通道卷积）**：每个输入通道用一个独立的卷积核，只做空间卷积，不混合通道；
2. **Pointwise Convolution（逐点卷积，1×1）**：用 $1\times1$ 卷积对通道做线性组合，实现通道融合。

---

![](https://s.ar8.top/img/picgo/20250827185623703.webp)

---

**参数量对比**：

* 标准卷积：

$$
\#params = K_h \cdot K_w \cdot C_{in} \cdot C_{out}
$$

* 深度可分离卷积：

$$
\#params = K_h \cdot K_w \cdot C_{in} \;+\; C_{in} \cdot C_{out}
$$

---

**例子**：

* 输入 $224\times224\times32$，卷积核 $3\times3$，输出通道 $64$：

  * 标准卷积参数量 ≈ $3\times3\times32\times64 = 18,432$
  * 深度可分离卷积参数量 ≈ $3\times3\times32 + 32\times64 = 288 + 2048 = 2336$
    → 节省近 **8 倍** 参数量。

**应用**：

* 移动端轻量化网络（MobileNet、Xception）。

---

### 2. 空洞卷积（Dilated Convolution / Atrous Convolution）

**定义**：在卷积核中“插入空洞”（dilation），让卷积核在输入特征图上“跳步”采样。


* 等效卷积核大小：

$$
K^{eff} = D\cdot(K-1) + 1
$$

其中 $D$ 为 dilation rate。

![bg right:40% 100%](https://s.ar8.top/img/picgo/20250827190216155.webp)

---

**效果**：

* 感受野变大（RF 增大），无需增加参数量或计算量。
* 适合需要全局信息但分辨率又不能降低的任务。

**应用**：

* 语义分割（DeepLab 系列），能在不丢失分辨率的情况下捕捉全局上下文。

---

### 3. 转置卷积（Transposed Convolution / Deconvolution）

**动机**：在生成模型或分割任务中，我们需要“上采样”，从小分辨率特征恢复到大分辨率特征。

**定义**：转置卷积是卷积运算的“逆向传播形式”，它通过在输入之间插入零，再进行卷积，实现特征图放大。

![](https://s.ar8.top/img/picgo/20250827191156988.webp)

---

**输出尺寸公式**（单维）：

$$
H_{out} = (H_{in}-1)\cdot S - 2P + D\cdot (K-1) + \text{output\_padding} + 1
$$

> * $H_{\text{in}}$：输入特征图在该维度的长度（例如输入高）。
> * $S$：stride（步幅）。
> * $P$：padding（对**正向卷积**而言的 padding），转置时会以相反的方式影响尺寸。
> * $K$：kernel size（卷积核尺寸）。
> * $D$：dilation（膨胀率，空洞参数）。
> * $\text{output\_padding}$：**仅在转置卷积**里出现，用来打破歧义、补齐“差 1 个像素”的情况，范围通常在 $[0, S-1]$。
> * 末尾的 **+1** 是由离散卷积的索引边界决定的常见项（与正向卷积公式的 “-1/+1” 是对偶关系）。

---

**问题**：容易出现“棋盘格伪影”（checkerboard artifacts）。

**替代方案**：上采样（最近邻 / 双线性） + 普通卷积，更稳定。

**应用**：

* 生成对抗网络（GAN），图像分割（FCN、U-Net）。

---

# 替代方案详解：不直接用转置卷积也能上采样

很多任务（分割、生成）需要放大特征图，但转置卷积**容易产生“棋盘格伪影”**（checkerboard artifacts），原因在于 stride>1 时卷积核覆盖的不均匀重叠。下面是工程里非常常用、且更稳定的替代做法。

---

## 方案 A：上采样（插值） ➜ 普通卷积（Resize-Conv / UpSample-Conv）

**思路**：先用插值法把特征图放大到目标尺寸，再用标准卷积“清理、重建、提特征”。

* 插值方式：`nearest`（最快）、`bilinear`/`bicubic`（更平滑）、`area`（等效平均池化反向）。
* 优点：**几乎不出伪影**、实现简单、数值稳定，是很多分割/生成网络的默认选择。
* PyTorch 例：

  ```python
  import torch.nn.functional as F
  up = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)  # or 'nearest'
  y  = conv3x3(up)  # 标准卷积
  ```
---

## 方案 B：像素重排（Sub-Pixel / PixelShuffle）

**思路**：先在**通道维**产生 $r^2$ 倍的通道数，再把这些通道**重排**到空间维实现放大（放大倍数 $r$）。

* 代表作：**ESPCN / PixelShuffle**（超分辨率常用）。
* 优点：**无棋盘格伪影**、参数/计算高效；对超分辨率特别友好。
* 代价：对通道布局有要求；实现时要小心权重初始化（ICNR）来避免早期格纹。
* PyTorch 例：

  ```python
  conv = nn.Conv2d(C_in, C_out * r * r, kernel_size=3, padding=1)
  shuffle = nn.PixelShuffle(r)  # r=2,3,4...
  y = shuffle(conv(x))          # (N, C_out, H*r, W*r)
  ```

---

## 方案 C：反池化（MaxUnpool）

**思路**：如果下采样用了 **MaxPool2d(return\_indices=True)**，就可以在上采样时用 **MaxUnpool2d** 将最大值的位置“放回去”，其余位置补零，再接普通卷积细化。

* 优点：**可逆性强**，结构上很直观（如 SegNet）。
* 限制：必须保存池化阶段的 indices；只适合配对的池化/反池化场景。
* PyTorch 片段：

  ```python
  pool = nn.MaxPool2d(2, stride=2, return_indices=True)
  x2, idx = pool(x)
  unpool = nn.MaxUnpool2d(2, stride=2)
  up = unpool(x2, idx, output_size=x.size())
  ```

---

## 方案 D：级联上采样（逐级 ×2 放大）

**思路**：不要一口吃成“×4/×8”，而是多次“×2 上采样 + 小卷积”逐级放大；

* 优点：**稳定、细节逐步恢复**，便于加入跳连（U-Net/HRNet 风格）。
* 适用：分割、密集预测、重建等。

---

## 方案 E：动态/可学习插值（CARAFE 等）

**思路**：根据局部上下文**自适应地生成上采样核**，对不同位置应用不同的插值权重。

* 优点：重建质量更好；
* 代价：实现复杂、计算更大；
* 适用：追求质量的研究或高端应用。


---

## 二、上采样与下采样

在卷积神经网络中，**采样（Sampling）** 是指改变特征图空间分辨率（高 H × 宽 W）的操作。采样分为两类：

### 1. 下采样（Downsampling）

**定义**：将输入特征图的空间分辨率减小（H↓，W↓），保留主要信息，丢弃部分细节。
**作用**：

* 减少计算量和参数量；
* 扩大感受野，让后续层看到更大范围的信息；
* 增强模型的平移不变性。

---

**常见方法**：

1. **最大池化（Max Pooling）**：取局部窗口的最大值，突出显著特征。
2. **平均池化（Average Pooling）**：取局部窗口的平均值，保留平滑信息。
3. **步幅卷积（Stride Convolution）**：在卷积时设置 stride > 1，输出尺寸自动减小。

**例子**：输入 $32\times32$ 图像，使用 stride=2 的卷积，下采样后输出为 $16\times16$。

---

### 2. 上采样（Upsampling）

**定义**：将输入特征图的空间分辨率放大（H↑，W↑），恢复或生成更高分辨率的表示。
**作用**：

* 在分割、检测、生成任务中，需要从小特征图恢复到大图像尺寸。
* 上采样让模型能输出逐像素级预测。

---

**常见方法**：

1. **最近邻插值（Nearest Neighbor）**：复制邻近像素，简单高效。
2. **双线性插值（Bilinear Interpolation）**：利用相邻像素加权平均，更平滑。
3. **转置卷积（Transposed Convolution / Deconv）**：卷积的逆向形式，可学习，但可能有棋盘格伪影。
4. **上采样 + 卷积**：先插值放大，再用卷积学习特征，更稳定。

---

## 三、卷积结构改进案例

### (一)、Inception 模块（GoogLeNet, 2014）

#### 1. 背景

* 在 CNN 中，不同卷积核大小捕捉不同的感受野：

  * 小卷积核（$1\times1$、$3\times3$）适合细节；
  * 大卷积核（$5\times5$）适合大范围上下文。
* 问题：如果堆叠多种卷积，参数和计算量过大。

![bg right:40% 90%](https://s.ar8.top/img/picgo/20250827194132750.webp)

---

#### 2. Inception 的思想

* **并行多尺度卷积**：同时使用 $1\times1$、$3\times3$、$5\times5$ 卷积，以及 $3\times3$ 最大池化。
* **1×1 卷积降维**：在大卷积前先用 $1\times1$ 卷积减少通道数，大幅降低参数量。


#### 3. 模块结构

![](https://s.ar8.top/img/picgo/20250827194401865.webp)

---

#### 4. 优点

* 多尺度特征融合；
* 使用 1×1 卷积减少计算量；
* 提升表达能力而不显著增加开销。

#### 5. 应用

* GoogLeNet（Inception v1）首次提出，参数量大幅减少（约 500 万，对比 AlexNet 的 6000 万）。
* 后续 Inception v2/v3 改进了卷积分解和正则化。

---

### (一)、ResNet 残差网络（2015）

#### 1. 背景

* 随着网络加深，训练困难：

  * **梯度消失/爆炸**：深层网络无法有效传播梯度。
  * **退化问题**：更深的网络准确率反而下降。

#### 2. 残差学习思想

* 假设要学习的目标函数是 $H(x)$，直接学习困难。
* ResNet 改为学习残差函数：

$$
F(x) = H(x) - x \quad \Rightarrow \quad H(x) = F(x) + x
$$

* 即输出 = 输入 + 残差映射。

---

#### 3. 残差块（Residual Block）

![bg right:50% 100%](https://s.ar8.top/img/picgo/20250827194731453.webp)

数学形式：

$$
y = F(x, W) + x
$$

其中 $F(x, W)$ 是卷积层堆叠学习到的残差。

---

#### 4. 优点

* 缓解梯度消失，支持训练上百甚至上千层网络；
* 残差连接让优化问题更简单：若最优解接近恒等映射，残差学习比直接学习更容易。

#### 5. 实践细节

* 如果输入输出维度不一致，用 $1\times1$ 卷积调整通道数。
* 在 ResNet v2 中，采用 **BN → ReLU → Conv** 的顺序（pre-activation），效果更好。

#### 6. 应用

* ResNet 在 2015 年 ImageNet 比赛中夺冠（152 层），成为深度学习的重要里程碑。
* 许多后续模型（DenseNet、ResNeXt、Transformers）都受其启发。
