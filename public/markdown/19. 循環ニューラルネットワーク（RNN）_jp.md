---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第19講：再帰型ニューラルネットワーク（RNN）

---

## 一、RNN（再帰型ニューラルネットワーク）とは？

* **正式名称**：Recurrent Neural Network（再帰型ニューラルネットワーク）
* **核心目標**：**系列データ**を処理すること

  * 通常のニューラルネットワーク（全結合ネットワークやCNN）は入力と出力の長さが固定されている（例：画像は固定サイズ）
  * しかし自然言語、音声、時系列データは入力の長さが可変であり、**前後依存**の特徴を持つ
* **RNN の考え方**：ネットワークに**「記憶」**を導入し、前のステップの情報を後のステップへ伝える

---

### 例を挙げる

* 文の次の単語を予測するとする：

  * 入力系列："私 → は → 自然 → 言語 → ..."
  * 次の単語を予測
* **通常のネットワーク**：現在の入力（例：「言語」）しか見られず、前の「自然」を知らない
* **RNN**：前の単語情報を記憶し、「自然」の文脈を伝えて「処理」を予測できる

したがって、RNN は特に以下に適する：

* **テキスト**（言語モデル、機械翻訳）
* **音声**（音声認識）
* **時系列**（株価予測、センサーデータ解析）

---

## 二、RNN の動作原理

### 2.1 通常のニューラルネットとの違い

* 通常のニューラルネット：入力 → 出力（1回きりの計算、記憶なし）
* RNN：前の出力を**フィードバック**し、次の計算の一部に利用する

---

### 2.2 コアメカニズム

* 各時刻 $t$ で RNN は以下を受け取る：

  * 現在の入力 $x_t$（例：単語ベクトル）
  * 前時刻の隠れ状態 $h_{t-1}$（記憶）

* 出力するのは：

  * 現在の隠れ状態 $h_t$（新しい記憶）
  * 現在の出力 $y_t$（予測に利用可能）

![bg right:50% 90%](https://s.ar8.top/img/picgo/20251001182151853.webp)

---

まとめると：

$$
h_t = f(Wx_t + Uh_{t-1} + b)
$$

* ここで $h_t$ は「記憶」であり、各ステップで更新される
* まるで流れ作業のように「現在の入力＋過去の記憶」が次へ伝わる

---

### 2.3 直感的な比喩

* 文を読むとき：

  * 「私」を読む → 記憶更新：主語は「私」
  * 「は」を読む → 記憶更新：動詞が来ることを期待
  * 「自然」を読む → 記憶更新：対象は「自然」
  * 「言語」を読む → 記憶更新：「自然言語」というまとまり
* 各単語が現れるたびに、「過去の記憶」を使って現在を理解する

これが RNN の本質：**順序処理＋文脈記憶**

---

## 三、RNN の構造と数式導出

### 3.1 構造の直観的理解

RNN は **鎖のような構造**：

* 各時刻 $t$ に小さな「RNN セル」がある
* セル間は隠れ状態で接続され、過去の知識を次に伝える

時間展開図：

```

x1 → [RNN cell] → h1 → y1
↑
x2 → [RNN cell] → h2 → y2
↑
x3 → [RNN cell] → h3 → y3

```

---

![bg 80%](https://s.ar8.top/img/picgo/20251001182151853.webp)

---

### 3.2 数式導出

#### 入力定義

* $x_t$：時刻 $t$ の入力ベクトル（単語の埋め込みなど）
* $h_t$：時刻 $t$ の隠れ状態（記憶）
* $y_t$：時刻 $t$ の出力

#### 更新規則

隠れ状態の更新：

$$
h_t = \tanh(W_x x_t + W_h h_{t-1} + b_h)
$$

---

* $W_x$：入力の重み行列
* $W_h$：隠れ状態の重み行列（記憶処理）
* $\tanh$：活性化関数
* $h_{t-1}$：前時刻の隠れ状態

出力層：

$$
y_t = \text{softmax}(W_y h_t + b_y)
$$

---

#### 流れの理解

* **初期化**：$h_0=0$ とする
* **第1ステップ**：$x_1$ を入力し $h_1$ を計算
* **第2ステップ**：$x_2$ を入力し $h_1$ と組み合わせて $h_2$
* **第3ステップ**：繰り返して全単語を処理
* 各ステップで「過去の記憶＋現在入力」を組み合わせて理解する

---

## 四、勾配消失と勾配爆発（RNN 最大の難題）

### 4.1 RNN の特殊性

* 前向きネットワークでは勾配伝播の層は有限
* RNN では系列が長いと、同じ $W_h$ が繰り返し使われる
* 逆伝播では勾配が $(W_h)^t$ のように繰り返し掛け合わされる

---

### 4.2 勾配消失

* $|W_h| < 1$ の場合、繰り返しで勾配は 0 に近づく
* 長期依存を学習できず、短期文脈しか記憶できない

### 4.3 勾配爆発

* $|W_h| > 1$ の場合、勾配が無限大に近づく
* 学習が発散する

---

### 4.4 現実での状況

* 勾配消失が圧倒的に多い
* そのため RNN は長距離依存を捉えるのが苦手

### 4.5 解決策

* **勾配クリッピング**
* **改良構造**：LSTM、GRU
* **活性化関数変更**：ReLU
* **初期化・正則化工夫**

---

## 五、代表的な活性化関数

### 5.1 Sigmoid

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

* 出力範囲 (0,1)
* 確率解釈可能
* 勾配消失しやすい

---

### 5.2 Tanh

$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

* 出力範囲 (-1,1)
* 中心化されており Sigmoid より良い
* しかし勾配消失あり
* RNN で最もよく使われる

---

### 5.3 ReLU

$$
\text{ReLU}(x)=\max(0,x)
$$

* 出力範囲 [0,∞)
* 勾配消失をある程度回避
* ただし「死んだニューロン」問題あり

---

### 5.4 その他

* Leaky ReLU
* Swish / GELU（Transformer 系）

---

## 六、RNN の種類

### 6.1 単方向 RNN

* 左から右へ流れる
* $h_t$ は過去のみ依存
* 言語モデル、株価予測などに利用
* 欠点：未来情報が利用できない

---

### 6.2 双方向 RNN（Bi-RNN）

* 順方向と逆方向の 2 つを組み合わせ
* 過去と未来の文脈を同時に利用可能
* 用途：NER、POS タグ付け、翻訳のエンコーダ
* 欠点：リアルタイムには使えない

---

### 6.3 多層 RNN（Stacked RNN）

* 層を積み重ねる
* より複雑なパターンを捉える
* ただし過学習しやすく訓練困難

---

### 6.4 Encoder-Decoder RNN

* 入力系列を固定ベクトルにエンコード → デコーダで系列に復元
* 機械翻訳（Seq2Seq）
* 長文では情報圧縮で劣化

---

## 七、RNN の限界

* 長期依存問題（勾配消失）
* 訓練効率低い（逐次処理で並列化不可）
* 遠距離依存を捉えにくい
* 計算コスト大
* LSTM、GRU、Transformer の登場により置き換えられた

---

## 八、実践：単語レベルの言語モデル

**目標**：前の単語列から次の単語を予測  
**方法**：

1. テキスト → トークン化 → 語彙表（stoi/itos）
2. **Embedding**（Word2Vec/GloVe）で ID → ベクトル
3. RNN/LSTM/GRU に入力、次単語分布を出力
4. 学習：交差エントロピー最小化、評価：Perplexity
5. 推論：単語を逐次生成
