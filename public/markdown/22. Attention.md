---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第22讲：注意力机制（Attention Mechanism）

---

## 一、什么是注意力机制？

### 1.1 提出背景

* 在 **Seq2Seq 模型** 中，编码器把整个输入压缩成一个固定向量。
  * 对于长句子，这个向量无法完整保存全部信息。
  * 结果：解码器容易“遗忘前面的内容”。
* 为了解决这个“信息瓶颈”，
  → 2015年 Bahdanau 等人提出 **Attention 机制**。

---

### 1.2 核心思想

> 不要试图“一次记住整个句子”，  
> 而是在解码每个词时，**动态地关注输入序列中最相关的部分**。

* 当模型生成第 $t$ 个输出时：
  * 它不再只依赖最后的隐藏状态；
  * 而是计算输入序列中每个词与当前解码状态的“相关性权重”。

---

### 1.3 类比理解（人的注意力）

* 当你翻译一句话时：
  * 你不会记住整句，而是**注意到当前词对应的部分**。
* 例如：
```

I love playing the piano → 我喜欢弹钢琴

```
当生成“钢琴”时，你自然会关注“piano”这个词。

* 注意力机制让模型“会看哪里”，即赋予了**选择性聚焦能力**。

---

## 二、注意力机制的基本原理

### 2.1 输入输出结构

在每个时间步 $t$：

1. **输入：**
 * 解码器当前隐藏状态 $s_t$
 * 编码器的所有隐藏状态 $h_1, h_2, ..., h_T$
2. **输出：**
 * 加权平均得到的“上下文向量” $c_t$
 * 作为该时间步的外部信息输入解码器。

---

![bg 80%](https://s.ar8.top/img/picgo/20251029193706920.webp)

---

### 2.2 计算流程

#### 第一步：计算相似度（score）

衡量 $s_t$ 与每个 $h_i$ 的匹配程度：

$$
e_{t,i} = \text{score}(s_t, h_i)
$$

常见的打分函数：

| 名称 | 公式 | 说明 |
|------|------|------|
| Dot | $s_t^\top h_i$ | 向量点积 |
| General | $s_t^\top W_a h_i$ | 加权点积 |
| Additive (Bahdanau) | $v_a^\top \tanh(W_s s_t + W_h h_i)$ | 非线性组合 |

---

#### 第二步：Softmax 归一化

将打分转为权重分布：

$$
\alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_{k=1}^{T} \exp(e_{t,k})}
$$

* $\alpha_{t,i}$ 表示第 $i$ 个输入在当前输出时的“关注程度”。
* 所有权重相加为 1。

---

#### 第三步：计算上下文向量

$$
c_t = \sum_{i=1}^{T} \alpha_{t,i} h_i
$$

* $c_t$ 是输入信息的加权平均。
* 表示当前时刻解码器“综合参考”的内容。

---

#### 第四步：结合上下文生成输出

$$
\tilde{s_t} = \tanh(W_c [s_t; c_t])
$$

然后再通过 Softmax 得到最终预测：

$$
P(y_t | y_{<t}, X) = \text{Softmax}(W_o \tilde{s_t})
$$

---

## 三、直观理解与可视化

### 3.1 图像比喻

* **Encoder 隐藏状态 $h_i$**：输入序列每个词的“记忆”。
* **Decoder 当前状态 $s_t$**：当前正在生成的“语义焦点”。
* **注意力权重 $\alpha_{t,i}$**：像探照灯一样，照亮最相关的输入部分。

---

### 3.2 示例：机器翻译中的注意力

输入：  
```

X = ["I", "love", "playing", "piano"]

```
输出：  
```

Y = ["我", "喜欢", "弹", "钢琴"]

```

---

当生成“钢琴”时：

| 输入词 | 权重 ($\alpha_{t,i}$) |
|--------|----------------|
| I | 0.01 |
| love | 0.03 |
| playing | 0.12 |
| piano | **0.84** |

模型自动“聚焦”在“piano”。

---

### 3.3 注意力矩阵（Attention Map）

在整个序列生成过程中，
可以得到一个二维矩阵：

| 输出词 | I | love | playing | piano |
|--------|--|------|----------|--------|
| 我 | 0.7 | 0.2 | 0.1 | 0.0 |
| 喜欢 | 0.1 | 0.8 | 0.1 | 0.0 |
| 弹 | 0.0 | 0.2 | 0.6 | 0.2 |
| 钢琴 | 0.0 | 0.0 | 0.1 | 0.9 |

这就是 **Attention Heatmap**，能直观看出模型在“看哪里”。

---

## 四、不同类型的注意力

### 4.1 Soft Attention vs Hard Attention

| 类型 | 说明 | 特点 |
|------|------|------|
| Soft Attention | 对所有输入加权平均（可微） | 可训练、主流 |
| Hard Attention | 只选一个输入（采样） | 不可微，需强化学习优化 |

几乎所有现代模型都使用 **Soft Attention**。

---

### 4.2 Global vs Local Attention

| 类型 | 说明 | 优点 | 缺点 |
|------|------|------|------|
| Global Attention | 对整个输入序列加权 | 全局视野 | 计算开销大 |
| Local Attention | 只关注输入的一部分窗口 | 高效 | 可能遗漏远距依赖 |

---

### 4.3 Self-Attention（自注意力）

* 注意力不仅能用于“解码器关注编码器”。
* 还可以让**序列内部的元素相互关注**。

例如句子：
> "The animal didn't cross the street because it was too tired."

* “it” 应该关注到 “animal”，而非 “street”。

→ 这就是 **Self-Attention** 的思想。

---

## 五、Self-Attention 的工作原理

### 5.1 三个核心向量

每个输入向量 $x_i$ 生成三个不同的表示：

$$
Q_i = W_Q x_i \\
K_i = W_K x_i \\
V_i = W_V x_i
$$

其中：

* **Q（Query）**：查询
* **K（Key）**：键
* **V（Value）**：值

---

![](https://s.ar8.top/img/picgo/20251029194631019.webp)

---

### 5.2 注意力计算公式

$$
\text{Attention}(Q, K, V) = \text{Softmax}\left( \frac{Q K^\top}{\sqrt{d_k}} \right) V
$$

解释：

1. $QK^\top$：计算每个 Query 与所有 Key 的相似度。
2. 除以 $\sqrt{d_k}$：防止内积过大。
3. Softmax：转为权重分布。
4. 再加权求和 Value。

---

### 5.3 多头注意力（Multi-Head Attention）

* 单一注意力头可能只学习一种关系（如主谓关系）。
* Transformer 中使用多头注意力：
  $$
  \text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
  $$
* 每个头学习不同的关注模式（如语法、语义、上下文等）。

---

## 六、从 Attention 到 Transformer

### 6.1 Transformer 的核心思想

> 用 Self-Attention 完全取代 RNN/LSTM 的顺序计算。

结构仍是 **Encoder–Decoder**：
* Encoder：多层 Self-Attention + FeedForward
* Decoder：Self-Attention + Encoder–Decoder Attention

---

### 6.2 位置编码（Positional Encoding）

* 注意力机制本身不具备“顺序”概念。
* Transformer 使用正弦/余弦函数编码位置：
  $$
  PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}}) \\
  PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})
  $$
* 使模型能感知词序信息。

---

## 七、注意力机制的优点与意义

### 7.1 优点

* 捕捉长距离依赖，无需固定长度向量。
* 提高翻译、摘要、对话等任务的准确率。
* 可解释性强 —— 可视化权重矩阵。
* 并行化友好（在 Self-Attention 结构中）。

---

### 7.2 意义

* **Attention 是连接 RNN 与 Transformer 的桥梁**。
* 从“顺序记忆”走向“全局关注”。
* 为 BERT、GPT 等大型模型奠定核心计算基础。

---

## 八、常见注意力变体

| 名称 | 关键思想 | 应用场景 |
|------|-----------|-----------|
| Additive (Bahdanau) | 使用 $\tanh$ + 可训练参数 | 经典 Seq2Seq |
| Dot-Product (Luong) | 向量点积计算相似度 | 高效翻译模型 |
| Scaled Dot-Product | 引入缩放项 $\sqrt{d_k}$ | Transformer |
| Multi-Head | 多路并行学习不同关系 | Transformer |
| Self-Attention | 元素间相互关注 | NLP、CV通用 |
| Cross-Attention | 解码器关注编码器输出 | 生成任务 |
| Sparse / Local | 限定关注范围 | 长文本、高效模型 |

---

## 九、应用举例

### 9.1 机器翻译
* 早期的 Attention Seq2Seq 提升 BLEU 分数显著。
* 能精确对齐输入与输出词语。

### 9.2 文本摘要
* 模型自动聚焦关键句段，生成更自然的摘要。

### 9.3 图像字幕生成（Image Captioning）
* 注意力机制可让模型“看”图像的不同区域。

### 9.4 语音识别 / 视频理解
* 在时序数据上选择性关注关键片段。

---

## 十、总结

| 模块 | 作用 | 公式 |
|------|------|------|
| 打分函数 | 衡量 Query 与 Key 的相关性 | $e_{t,i} = \text{score}(Q,K)$ |
| 权重分布 | Softmax 归一化 | $\alpha_{t,i} = \text{Softmax}(e_{t,i})$ |
| 加权求和 | 得到上下文向量 | $c_t = \sum_i \alpha_{t,i} V_i$ |
| Self-Attention | 序列内部相互注意 | $Q=K=V=X$ |
| Multi-Head | 并行多种关注模式 | $\text{Concat}(\text{head}_i)$ |

---

# ✅ 课后思考

1. 为什么 Attention 能解决 Seq2Seq 的“信息瓶颈”？  
2. Additive 与 Dot-Product Attention 有什么区别？  
3. Self-Attention 如何实现并行计算？  
4. Transformer 为什么能完全替代 RNN？
