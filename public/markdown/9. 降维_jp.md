---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

## <!-- fit -->📘 第9講：次元削減

---

## 次元削減技術

### なぜ次元削減が必要か

* **次元の呪い**：高次元空間では距離指標が無意味になりやすく、計算コストが増大する  
* **可視化**：3次元以上のデータは次元削減して初めて可視化できる  
* **高速化＆ノイズ除去**：冗長な特徴を取り除き、後続のモデリング性能を向上させる  

---

## 🔍 1. PCA の直感的な目的

* **次元削減**：元の高次元データを低次元部分空間へマッピングし、データの「重要情報」—すなわち分散（変動性）—をできる限り保持する  
* **可視化**：2D／3D 投影によってデータ構造やクラス分布を観察  
* **ノイズ除去**：分散の小さい方向のノイズ成分を捨てる  

---

## 📐 2. 数学的原理と導出

1. **データの標準化**

   $$
   \tilde X = \frac{X - \mu}{\sigma}
   $$

   * 各特徴量から平均 $\mu$ を引き，標準偏差 $\sigma$ で割ることで，同一スケールに揃える．

2. **共分散行列の計算**  
   標準化後のサンプル行列 $\tilde X \in \mathbb{R}^{n\times d}$ に対し，  
   $$
   C = \frac{1}{n-1}\,\tilde X^\top \tilde X 
   \;\in\;\mathbb{R}^{d\times d}
   $$

---

3. **固有値分解（Eigen decomposition）**  
   共分散行列 $C$ の**固有値**$\lambda_1,\dots,\lambda_d$ と対応する**固有ベクトル**（単位ベクトル）$\mathbf{v}_1,\dots,\mathbf{v}_d$ を求める：  
   $$
   C\,\mathbf{v}_i = \lambda_i\,\mathbf{v}_i,
   \quad
   \lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_d \ge 0.
   $$

4. **主成分の選択**

   * 第 $k$ 主成分の方向は，$k$ 番目に大きい固有値 $\lambda_k$ に対応する固有ベクトル $\mathbf{v}_k$  
   * 上位 $m$ 成分を並べた射影行列  
     $$
     V_m = [\mathbf{v}_1,\dots,\mathbf{v}_m]
     \in \mathbb{R}^{d\times m}
     $$

---

5. **投影**  
   標準化データ $\tilde X$ を主成分部分空間へ投影：  
   $$
   Z = \tilde X \,V_m
   \quad(\in\mathbb{R}^{n\times m})
   $$

6. **分散寄与率**  
   第 $k$ 主成分が保持する分散の割合  
   $$
   \mathrm{ExplainedRatio}_k
   = \frac{\lambda_k}{\sum_{i=1}^d \lambda_i}.
   $$
   上位 $m$ 成分の累積寄与率
   $\sum_{k=1}^m \mathrm{ExplainedRatio}_k$ で，次元削減後の情報保持度を判断する．

---

## 共分散行列（Covariance Matrix）

1. **概念の復習**  
   $d$ 個の特徴をもつデータセットにおいて，各特徴間の“線形的な関係性”を知りたい．

   * **共分散** $\mathrm{Cov}(X_i, X_j)$ は，特徴 $i$ と $j$ が同時に増減する傾向を表す：  
     $$
     \mathrm{Cov}(X_i, X_j)
     = \mathbb{E}\bigl[(X_i - \mu_i)(X_j - \mu_j)\bigr]
     = \frac{1}{n-1}\sum_{k=1}^n (x_{k,i}-\mu_i)(x_{k,j}-\mu_j).
     $$
   * 正の値は“同方向”，負の値は“逆方向”変動，ゼロなら線形無相関（必ずしも独立ではない）．

---

2. **行列形式**  
   全特徴間の共分散をまとめた $d\times d$ 行列が**共分散行列**：  
   $$
   C =
   \begin{pmatrix}
     \mathrm{Cov}(X_1,X_1) & \mathrm{Cov}(X_1,X_2) & \cdots & \mathrm{Cov}(X_1,X_d) \\
     \mathrm{Cov}(X_2,X_1) & \mathrm{Cov}(X_2,X_2) & \cdots & \mathrm{Cov}(X_2,X_d) \\
     \vdots               & \vdots               & \ddots & \vdots               \\
     \mathrm{Cov}(X_d,X_1) & \mathrm{Cov}(X_d,X_2) & \cdots & \mathrm{Cov}(X_d,X_d)
   \end{pmatrix}.
   $$
   * 対角要素は各特徴の分散．  
   * 対称行列：$\mathrm{Cov}(X_i,X_j)=\mathrm{Cov}(X_j,X_i)$．

---

## 固有値分解（Eigen Decomposition）

1. **狙い**  
   共分散行列 $C$ の“主軸”（principal axes）を見つけ，データの最も大きな変動方向へ投影して次元削減を行う．

2. **数式**  
   $$
     C\,\mathbf{v}_i = \lambda_i\,\mathbf{v}_i,\quad i=1,\dots,d
   $$
   - $\lambda_i$: **固有値**（eigenvalue）、スカラー  
   - $\mathbf{v}_i$: **固有ベクトル**（eigenvector）、長さ $d$ の単位ベクトル  
   - 相異なる固有ベクトル同士は直交  

---

3. **意味**  
   * **固有ベクトル $\mathbf{v}_i$**：データの主軸方向を示す  
   * **固有値 $\lambda_i$**：その方向への分散（情報量）を表す。大きいほど情報量が多い．

4. **並べ替えと投影**  
   $\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_d$ の順に並べ，  
   上位 $m$ ベクトルで射影行列を組成：  
   $$
     V_m=[\mathbf{v}_1,\dots,\mathbf{v}_m]
   $$
   元データ $X$ を投影：  
   $$
     Z=X\,V_m
   $$
   で，$m$ 次元表現を得る．

---

## 💻 3. 実践：Iris データセットでの PCA コード例

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# 1. データ読み込み
data = load_iris()
X, y = data.data, data.target
target_names = data.target_names

# 2. 標準化
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

# 3. PCA で 2 次元へ
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_std)

# 4. 分散寄与率の確認
explained = pca.explained_variance_ratio_
print(f"PC1 は {explained[0]*100:.1f}% を説明")
print(f"PC2 は {explained[1]*100:.1f}% を説明")
print(f"累計 {explained.sum()*100:.1f}% を説明")

# 5. 投影図の可視化
plt.figure(figsize=(6,5))
for i, color in zip(range(3), ['r','g','b']):
    plt.scatter(X_pca[y==i, 0], X_pca[y==i, 1],
                label=target_names[i], alpha=0.7, edgecolor='k')
plt.xlabel('主成分 1')
plt.ylabel('主成分 2')
plt.title('Iris データセット PCA 投影')
plt.legend()
plt.grid(True)
plt.show()

# 6. 分散寄与率の棒グラフ
plt.figure(figsize=(6,4))
components = np.arange(1, len(pca.explained_variance_ratio_)+1)
plt.bar(components, pca.explained_variance_ratio_ * 100, alpha=0.7)
plt.xlabel('主成分')
plt.ylabel('分散寄与率 (%)')
plt.title('PCA 分散寄与率')
plt.xticks(components)
plt.grid(axis='y')
plt.show()
````

---

## 📊 4. 結果の解釈

* **投影図**

  * Setosa（赤）は明確に分離
  * Versicolor（緑）と Virginica（青）は一部重なり

* **分散寄与率**

  * PC1 ≈ 72.9%、PC2 ≈ 23.0%、累計 ≈ 95.9%
  * 4次元から2次元へ次元削減しても約96%の情報を保持

---

## ✅ 5. まとめと実践のポイント

1. **PCA を使う場面**

   * 高次元データを可視化したいとき
   * 特徴の冗長性除去や前処理としてノイズを落としたいとき
   * クラスタリングや分類・回帰の前処理として次元削減

2. **注意点**

   * PCA は分散の大きい方向を重要視するため、分散が小さいが重要な特徴を見落とす場合がある
   * 事前に**標準化**を必ず行わないと，大きな値の特徴が主成分を支配する

---

3. **拡張**

   * **カーネル PCA**：非線形構造の処理
   * **スパース PCA**：特徴をスパースに選択
   * **インクリメンタル PCA**：大規模データ向け

---

# 🎓 t-SNE 可視化技術 詳細解説

---

## 1. t-SNE の目的と利用場面

### 1.1 t-SNE とは？

* **t-SNE**（t 分布型確率的近傍埋め込み）は非線形次元削減技術で、主に高次元データを 2D/3D に写像し、データの**局所構造**を可視化する。
* **目的**：低次元空間で元のデータ点間の相対的な類似性をできるだけ保ち、データクラスタを浮かび上がらせる。

### 1.2 利用シーン

* **可視化**：2D/3D 平面上でクラスタやクラス境界を理解
* **クラスタリング補助**：高次元クラスタの分布確認
* **多様体学習**：手書き数字や画像などの流形構造の検出

---

## 2. t-SNE アルゴリズムのコア原理

t-SNE は高次元⇔低次元の相似度分布の差を最小化することで埋め込みを学習する。

### 2.1 高次元空間の相似度（条件付き確率）

点 \$x\_i\$ に対し，\$x\_j\$ が隣接する確率 \$p\_{j|i}\$ を次式で定義：

$$
p_{j|i}
= \frac{\exp\bigl(-\|x_i - x_j\|^2 / (2\sigma_i^2)\bigr)}
       {\sum_{k\neq i} \exp\bigl(-\|x_i - x_k\|^2 / (2\sigma_i^2)\bigr)}.
$$

* \$|x\_i - x\_j|^2\$：ユークリッド距離の二乗
* \$\sigma\_i\$：困惑度（Perplexity）から決定されるパラメータ

---

### 2.2 対称化した結合確率

対称性を保つため，高次元空間の**結合確率**を

$$
p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}
$$

で定義し，\$p\_{ij}=p\_{ji}\$ とする。

### 2.3 低次元空間の相似度（Student’s t 分布）

低次元座標 \$y\_i,y\_j\$ 間の相似度を

$$
q_{ij}
= \frac{(1 + \|y_i - y_j\|^2)^{-1}}
       {\sum_{k\neq\ell} (1 + \|y_k - y_\ell\|^2)^{-1}}.
$$

* 自由度1の t 分布を使うことで，局所クラスタに対して重い裾を持たせる。

---

### 2.4 KL ダイバージェンスの最小化

高次元と低次元の分布差を

$$
C = \mathrm{KL}(P\parallel Q)
  = \sum_{i\neq j} p_{ij}\log\frac{p_{ij}}{q_{ij}}
$$

の KL 散逸で測り、これを最小化する。

### 2.5 勾配降下法

$$
\frac{\partial C}{\partial y_i}
= 4\sum_j (p_{ij} - q_{ij})(y_i - y_j)\bigl(1 + \|y_i - y_j\|^2\bigr)^{-1}
$$

の勾配を用い、反復更新して収束させる。

---

## 3. t-SNE のパラメータ調整

### 3.1 Perplexity（困惑度）

* 各点の“情報量”を制御する
* 小さい値→局所構造重視、大きい値→広域構造も考慮

### 3.2 学習率（Learning Rate）

* 初期値200程度が一般的だが，データセットに応じて調整
* 大きすぎると発散，小さすぎると収束遅延

---

### 3.3 初期配置

* ランダム初期化 or PCA による初期化で収束を早める

### 3.4 イテレーション回数

* 通常は1000イテレーション程度で十分だが，増やすほど細かな構造を捉えやすい

---

## 4. t-SNE の長所と短所

### 4.1 長所

* **非線形次元削減**：複雑な局所構造を捉えられる
* **可視化適性**：クラスタ分離が明瞭なデータに強い

### 4.2 短所

* **計算コスト高**：時間計算量 \$O(n^2)\$
* **グローバル構造非保持**：全体的な距離やクラス間関係は反映しづらい
* **結果の不確定性**：ランダム性があり，実行ごとに異なる結果を得る場合がある

---

## 5. 実践：MNIST データセットでの t-SNE 可視化

### 5.1 MNIST データの読み込み

```python
from sklearn.datasets import fetch_openml
import numpy as np

# MNIST 読み込み
mnist = fetch_openml('mnist_784', version=1)
X = mnist.data / 255.0
y = mnist.target.astype(int)

# ランダムに1000サンプル選択
np.random.seed(42)
indices = np.random.choice(len(X), 1000, replace=False)
X_sample, y_sample = X.iloc[indices], y.iloc[indices]
```

### 5.2 t-SNE 降次元と可視化

```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# t-SNE で 2 次元へ
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X_sample)

# 可視化
plt.figure(figsize=(8,6))
plt.scatter(X_tsne[:,0], X_tsne[:,1], c=y_sample, cmap='tab10', s=10, alpha=0.7)
plt.title("MNIST の t-SNE 可視化")
plt.xlabel('t-SNE 成分 1')
plt.ylabel('t-SNE 成分 2')
plt.colorbar()
plt.show()
```

