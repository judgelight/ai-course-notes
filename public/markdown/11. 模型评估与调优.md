---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

# <!-- fit -->📘 第11讲：模型评估与调优

---

## 一、分类任务的评估指标

### 1. 混淆矩阵（Confusion Matrix）

对于二分类问题，设正类为“1”，负类为“0”，模型预测与真实标签的四种组合构成混淆矩阵：

|               | 预测为正类 (1) | 预测为负类 (0) |
| ------------- | --------- | --------- |
| **真实为正类 (1)** | 真阳性 (TP)  | 假阴性 (FN)  |
| **真实为负类 (0)** | 假阳性 (FP)  | 真阴性 (TN)  |

* **TP (True Positive)**：把正类预测成正类
* **TN (True Negative)**：把负类预测成负类
* **FP (False Positive)**：把负类错判为正类
* **FN (False Negative)**：把正类错判为负类

![bg right:50% 90%](https://s.ar8.top/img/picgo/20250716164746646.webp)

---

### 2. 准确率（Accuracy）

衡量整体预测正确的比例：

$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

* 优点：直观易懂
* 缺点：当样本类别高度不平衡时（如正负样本比 1:100），Accuracy 会严重误导。

---

### 3. 精确率（Precision）与召回率（Recall）

* **精确率（Precision）**：在所有被预测为正类的样本中，真正为正类的比例。

  $$
    \text{Precision} = \frac{TP}{TP + FP}
  $$
* **召回率（Recall）**（也称灵敏度 Sensitivity 或真正率 TPR）：在所有真实正类样本中，被正确预测为正类的比例。

  $$
    \text{Recall} = \frac{TP}{TP + FN}
  $$

| 指标        | 强调的目标    | 适用场景         |
| --------- | -------- | ------------ |
| Precision | 减少误报（FP） | 病毒检测、垃圾邮件过滤等 |
| Recall    | 减少漏报（FN） | 疾病筛查、安全监控等   |

---

### 4. F1 分数（F1 Score）

Precision 和 Recall 的调和平均，综合考虑误报和漏报：

$$
F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$

* 当 Precision 和 Recall 不平衡时，F1 倾向于取二者中较低者
* 适合在正负样本不平衡、又希望兼顾误报和漏报的场景

---

### 5. 特殊曲线与面积指标

1. **ROC 曲线（Receiver Operating Characteristic）**

   * 横轴：假阳性率 $\mathrm{FPR} = \frac{FP}{FP + TN}$
   * 纵轴：真阳性率 $\mathrm{TPR} = \mathrm{Recall}$
   * 曲线下的面积即 **AUC（Area Under Curve）**，越接近 1 性能越好。

![](https://s.ar8.top/img/picgo/20250716170253975.webp)![](https://s.ar8.top/img/picgo/20250716170322267.webp)

---

2. **PR 曲线（Precision–Recall Curve）**

   * 横轴：Recall
   * 纵轴：Precision
   * 在正负样本严重不平衡时，PR 曲线及其下的面积（Average Precision）能更真实地反映模型在正类上的性能。

![bg right:40% 90%](https://s.ar8.top/img/picgo/20250716173234347.webp)

---

## 二、回归任务的评估指标

### 1. 均方误差（MSE, Mean Squared Error）

$$
\mathrm{MSE} = \frac{1}{n}\sum_{i=1}^n (y_i - \hat y_i)^2
$$

* 对离群点（大误差）敏感，因为误差平方放大了偏差
* 单位是标签的平方

---

### 2. 均方根误差（RMSE, Root MSE）

$$
\mathrm{RMSE} = \sqrt{\mathrm{MSE}} = \sqrt{\frac{1}{n}\sum (y_i - \hat y_i)^2}
$$

* 与原标签单位相同，易于直观比较

---

### 3. 平均绝对误差（MAE, Mean Absolute Error）

$$
\mathrm{MAE} = \frac{1}{n}\sum_{i=1}^n |y_i - \hat y_i|
$$

* 对离群点不如 MSE 敏感，更稳健

---

### 4. 决定系数（$R^2$ Score）

$$
R^2 = 1 - \frac{\sum_{i}(y_i - \hat y_i)^2}{\sum_{i}(y_i - \bar y)^2}
$$

* 取值范围不固定，可为负（模型不如简单预测平均值）
* 越接近 1，说明模型对数据的拟合越好

---

## 三、如何选择合适的指标？

1. **根据任务类型**

   * 分类 vs. 回归
2. **根据业务需求**

   * 是否更在意误报（FP）还是漏报（FN）？
   * 是否能容忍一定量的离群点误差？
3. **样本分布**

   * 是否存在类别不平衡？
4. **可解释性**

   * 简单指标（Accuracy/MAE）易于讲解；复杂指标（AUC/F1）更全面但不够直观。

---

## 四、为什么需要交叉验证

* **评估稳定性**：单次划分（如一次训练/测试拆分）评估结果容易受偶然因素影响，难以判断模型的真实泛化能力。
* **充分利用数据**：在样本量有限的情况下，交叉验证能让每个样本既参与训练又参与验证，提高数据利用率。
* **对比模型与调参**：能在相同的数据划分策略下，公平地比较不同模型或不同超参数的性能。

---

## 五、常见的交叉验证方法

### 1. K‑折交叉验证（K‑Fold CV）

1. 将数据集随机分成 $K$ 份（folds）大小尽量相等。
2. 进行 $K$ 轮实验：

   * 第 $i$ 轮以第 $i$ 个 fold 作为验证集，其余 $K-1$ 个 folds 作为训练集。
   * 记录第 $i$ 轮的评估指标（如 Accuracy、MSE、F1 等）。
3. 最终模型性能取 $K$ 个指标的平均值（也可报告方差或标准差）。

> **推荐设置**：常见的 $K$ 值是 5 或 10。5‑Fold 计算快，10‑Fold 更稳定。

---

**优点**

* 简单易实现。
* 可并行计算每一折。

**缺点**

* 每一折都要训练 $K$ 次模型，计算量是一次划分的 $K$ 倍。
* 随机划分可能打破类别/组的关联。

---

### 2. 分层抽样 K‑折（Stratified K‑Fold）

* 在分类任务中，保证每一折中各类别样本比例与整体数据集相同。
* 避免类别极度不平衡时某些折缺少少数类的问题。

**使用场景**

* 二分类或多分类，且类别分布不均衡时。

---

## 六、过拟合 欠拟合
### 1. 概念

|          | 训练集表现    | 验证/测试集表现 |
| -------- | -------- | -------- |
| **欠拟合**  | 差（高误差）   | 差（高误差）   |
| **合适拟合** | 好（低误差）   | 好（低误差）   |
| **过拟合**  | 很好（极低误差） | 较差（高误差）  |

* **欠拟合**：模型容量不足（太简单），既无法在训练集上学到足够规律，也无法泛化到新数据。
* **过拟合**：模型容量过大（太复杂），在训练集上“记住”了噪声和偶然性特征，但这些特征在新数据上并不成立，导致验证集/测试集性能下降。

---

### 2、Bias–Variance 视角

模型预测误差可以分解为三部分：

$$
\mathbb{E}[(y - \hat f(x))^2] = \underbrace{\text{Bias}^2}_{\text{偏差}^2} + \underbrace{\text{Variance}}_{\text{方差}} + \underbrace{\sigma^2}_{\text{噪声}}
$$

* **Bias（偏差）**：模型预测值的期望与真实值之间的差距。偏差大 ⇒ 模型不够灵活，易欠拟合。
* **Variance（方差）**：模型预测的波动。方差高 ⇒ 对训练数据敏感，易过拟合。
* **Noise（噪声）**：数据本身的随机性，无法通过任何模型消除。

两者的关系通常呈“倒 U 型”曲线：

* 随着模型复杂度增加，Bias 下降，Variance 升高。
* **目标**：在 Bias² 与 Variance 之间取得平衡，使总误差最小。

---

### 3、成因与诊断

#### (1). 欠拟合

* **成因**

  * 模型过于简单（如线性模型用于高度非线性数据）。
  * 特征太少或信息不足。
  * 过强的正则化（如 λ 非常大）。
* **诊断**

  * 看训练集误差高且与验证集误差接近。
  * 学习曲线（Learning Curve）上，训练误差长期高位不降。
---

### (2). 过拟合

* **成因**

  * 模型过于复杂（如深度过深的决策树、过多的神经网络参数）。
  * 特征过多且包含噪声。
  * 训练数据量不足。
  * 正则化弱或缺失。
* **诊断**

  * 训练误差很低，而验证误差明显高于训练误差。
  * 学习曲线上，训练误差持续下降，验证误差在某点后回升。

---

### 4、应对策略

#### (1). 针对欠拟合

* **增加模型复杂度**

  * 换用更灵活的模型（如从线性回归换到多项式回归、决策树）
  * 增加隐藏层/节点（神经网络）
* **特征工程**

  * 增加更多有信息量的特征
  * 引入多项式特征或交互特征
* **减弱正则化**

  * 调小 L1/L2 正则化系数 λ

---

#### (2). 针对过拟合

* **正则化**

  * L2（Ridge）、L1（Lasso）、弹性网
* **剪枝与限制**

  * 决策树：限制最大深度、最小样本分裂数
  * 神经网络：Dropout、早停（Early Stopping）
* **增加数据**

  * 收集更多样本
  * 数据增强（图像翻转、噪声注入等）

---

* **特征选择**

  * 去掉低信息或高相关特征
  * 使用 PCA 等降维
* **集成方法**

  * Bagging（随机森林）天然防过拟合
  * Boosting（如 XGBoost）带有正则化项

---

### 5、示例：学习曲线（Learning Curve）

通过绘制训练集误差与验证集误差随训练样本数或模型复杂度的变化，可以直观判断欠拟合或过拟合：

![](https://s.ar8.top/img/picgo/20250716184851860.webp) ![](https://s.ar8.top/img/picgo/20250716184936472.webp)

* **欠拟合**：两条曲线都高且相近。
* **过拟合**：训练误差低，验证误差高；两条曲线差距大。
* **合适**：两条曲线均低且接近。

---

> **小结**：
>
> * 欠拟合：模型太简单 ⇒ 提高复杂度、丰富特征、弱化正则化。
> * 过拟合：模型太复杂 ⇒ 加强正则化、限制模型、增加数据或做集成。
> * 通过 Bias–Variance 分析与学习曲线，可以有效诊断并指导调优策略。

---

## 七、超参数调优
### 1、超参数 vs. 参数

* **模型参数（Parameters）**
  由算法在训练过程中自动学习得到的值，例如线性回归的权重 $w$、神经网络的连接权重和偏置。

---

* **超参数（Hyperparameters）**
  在训练前需要设定的值，不由模型直接学习，例如：

  * 决策树的最大深度 `max_depth`
  * 随机森林的树数量 `n_estimators`
  * 学习率 `learning_rate`
  * 正则化系数 $\lambda$
  * 神经网络的批大小 `batch_size`、隐藏层尺寸等

超参数的选择往往会显著影响模型性能，因此需要借助调优策略在验证集或交叉验证中寻找最佳组合。

---

### 2、常见超参数调优方法

| 方法            | 原理                                                      | 优点                        | 缺点                               |
| ------------- | ------------------------------------------------------- | ------------------------- | -------------------------------- |
| **网格搜索**      | 枚举所有候选值的笛卡尔积，对每一组做交叉验证，选最优。                             | 实现简单；可并行化；结果可复现。          | 组合数随超参数和候选值数量指数增长，计算成本高。         |
| **随机搜索**      | 在给定搜索空间内随机采样固定次数，评估并记录表现最好的。                            | 在相同预算下覆盖更多维度；对高维空间更高效。    | 仍需手动设定采样次数；无“智能”引导，可能浪费采样在不重要区域。 |
| **贝叶斯优化**     | 构建代理模型（如 Gaussian Process、TPE），根据历史试验结果智能选点。            | 更少的试验次数即可找到接近全局最优；样本高效。   | 实现较复杂；代理模型构建与更新有额外开销。            |
---

| 方法            | 原理                                                      | 优点                        | 缺点                               |
| ------------- | ------------------------------------------------------- | ------------------------- | -------------------------------- |
| **Hyperband** | 基于 Successive Halving 思路，先大规模随机采样，快速淘汰表现差的组合，再精细评估最优候选。 | 自动分配资源，快速排除低效组合；无需预设试验次数。 | 对早期性能波动敏感；实现比随机搜索稍复杂。            |
| **遗传算法**      | 模拟自然选择，使用交叉、变异等操作在“种群”中进化超参数。                           | 适合大规模、离散或复杂空间；可并行。        | 调参本身也需设定种群大小、变异率等；收敛速度可能较慢。      |
| **Optuna**    | 基于 TPE 的现代框架，支持动态搜索空间、分布式并行与中断恢复。                       | API 简洁易用；自动化程度高；社区活跃。     | 需要学习框架用法；和自建贝叶斯优化原理相似，需要理解其黑盒。   |

---

## 八、调优实战流程

1. **确定搜索空间**

   * 针对业务与模型经验，给出每个超参数的候选区间或分布（离散集合 vs. 连续区间）。
   * 例如：

     ```yaml
     max_depth: [3, 5, 7, 9]
     learning_rate: uniform(0.01, 0.3)
     n_estimators: [50, 100, 200]
     ```

2. **选择调优方法**

   * **预算充足**（有强大的计算资源 & 时间）：可优先尝试**网格搜索**。
   * **预算有限**或**超参数较多**：先用**随机搜索**或**Hyperband**快速探索；
   * **对试验次数敏感**：使用**贝叶斯优化**或**Optuna**。

---

3. **集成交叉验证**

   * 在每个超参数组合的评估中，都用固定的 K‑折交叉验证（或其它 CV 策略），取平均指标。
   * 推荐用 `GridSearchCV`、`RandomizedSearchCV`（scikit‑learn），或 Optuna/Hyperopt 等框架。

4. **记录与可视化**

   * 保存每次试验结果（超参数组合 + 验证分数）。
   * 可用热力图（Grid Search）或散点图（Random Search）观察超参数影响。

---

5. **选定最优组合**

   * 根据目标指标（如验证集最高 F1、最低 RMSE）确定最优超参数。
   * 注意同时查看指标的标准差，避免偶然性最优。

6. **重训练最终模型**

   * 在**全量训练集**上，用最优超参数重新训练一次模型。
   * 并在**独立测试集**或**留出集**上做最终评估，确保性能可靠。

