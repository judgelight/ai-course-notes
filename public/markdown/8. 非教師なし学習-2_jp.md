---
marp: true
theme: default
paginate: true
math: katex
---

<style>
section {
  font-family: "Microsoft YaHei", "PingFang SC", "Source Han Sans", "Noto Sans CJK SC", sans-serif;
}
</style>

## <!-- fit -->📘 第8講：非教師なし学習-2

---

# 🎓 非教師なし学習実践 · DBSCAN クラスタリング

---

## 1. なぜ DBSCAN を使うのか？

* **K-Means** や **階層型クラスタリング** はクラスタ数 $K$ を指定する必要があり、ノイズに敏感で、凸形クラスタしか発見できません。
* **DBSCAN**（Density-Based Spatial Clustering of Applications with Noise）はデータ点の密度に基づき、ノイズを自動で識別し、任意形状のクラスタを発見できます。

> 🔑 適用シーン：
>
> * 複雑なクラスタ形状
> * ノイズや外れ値が多い
> * 事前にクラスタ数が不明

---

## 2. コア概念とパラメータ

1. **パラメータ**

   * $
     \varepsilon$（eps）：ネイバー半径
   * $\mathit{MinPts}$：最小サンプル数の閾値

2. **ε-ネイバー（ε-Neighborhood）**
   データ点 $x\_i$ に対し、

   $$
   N_{\varepsilon}(x_i) = \{\,x_j \mid \|x_j - x_i\| \le \varepsilon\}\,.
   $$

3. **コア点（Core Point）**
   $|N\_{\varepsilon}(x\_i)| \ge \mathit{MinPts}$ の場合、$x\_i$ はコア点。

---

4. **ボーダー点（Border Point）**
   $|N\_{\varepsilon}(x\_i)| < \mathit{MinPts}$ だが、$x\_i$ があるコア点の ε-ネイバー内にある場合、ボーダー点。

5. **ノイズ点（Noise）**
   コア点でもなく、いかなるコア点のネイバー内にも含まれない点。

6. **直接的な密度到達（Directly Density-Reachable）**
   $x\_j$ が $x\_i$ の ε-ネイバー内かつ $x\_i$ がコア点の場合：

   $$
   x_j \in N_{\varepsilon}(x_i)
   \quad\text{かつ}\quad
   |N_{\varepsilon}(x_i)| \ge \mathit{MinPts}.
   $$

---

7. **密度到達（Density-Reachable）**
   コア点の連鎖 $x\_i = p\_1, p\_2, \dots, p\_k = x\_j$ が存在し、各ステップが直接的に密度到達可能：

   $$
   p_{m+1} \in N_{\varepsilon}(p_m),\quad |N_{\varepsilon}(p_m)| \ge \mathit{MinPts}.
   $$

8. **密度連結（Density-Connected）**
   あるコア点 $p$ が存在し、$x\_i$ と $x\_j$ の両方が $p$ に密度到達可能な場合。

---

## 3. アルゴリズムの流れと擬似コード

**入力**：データセット $X$, 半径 $\varepsilon$, 最小サンプル数 $\mathit{MinPts}$

**初期化**：全ての点を「未訪問」とマーク。

---

```text
for each point x in X:
  if x は既訪問: continue
  x を既訪問にマーク
  N = N_ε(x)  # ε-ネイバーを計算
  if |N| < MinPts:
    x をノイズにマーク
  else:
    新しいクラスタ C を作成
    x を C に追加
    seeds = N \ {x}
    for each point y in seeds:
      if y は未訪問:
        y を既訪問にマーク
        M = N_ε(y)
        if |M| ≥ MinPts:
          seeds = seeds ∪ (M \ seeds)
      if y がまだどのクラスタにも属していない:
        y を C に追加
```

* **コアアイデア**：各コア点から到達可能領域を拡張し、これ以上新しい点が追加されなくなるまで続ける。

---

## 4. 主要演算式まとめ

| 概念       | 公式／定義 |
| -------- | ------------------------------------------------------------ |
| ε-ネイバー   | $N_\varepsilon(x) = \{\,y \mid \|y - x\| \le \varepsilon\}$ |
| コア点判定    | $\lvert N_\varepsilon(x)\rvert \ge \mathit{MinPts}$          |
| 直接的密度到達  | $y \in N_\varepsilon(x)$ 与 $x$ 是核心点                          |
| 密度到達（連鎖） | 存在链 $p_1,\dots,p_k$ 且每步直接密度可达                                |
| 密度連結     | 存在核心点 $p$，使 $x_i$ 和 $x_j$ 都密度可达于 $p$                         |

---

## 5. パラメータ選び

* **ε**

  * 小さすぎる：ほとんどの点がノイズ扱い
  * 大きすぎる：全ての点が1つのクラスタに

* **MinPts**

  * 通常 ≥ データ次元 + 1
  * 値が大きいほどクラスタはより高密度になる

---

## 6. sklearn 実践プレビュー

```python
from sklearn.cluster import DBSCAN

db = DBSCAN(eps=0.5, min_samples=5, metric='euclidean')
labels = db.fit_predict(X)

# labels == -1 はノイズ点を示す
```

* `eps` が ε に対応
* `min_samples` が MinPts に対応
* `metric` はユークリッド距離やマンハッタン距離などを選択可能

---

## 7. 長所と短所、実践ガイド

| 長所                       | 短所                     |
| ------------------------ | ---------------------- |
| クラスタ数を自動判定、$K$ の事前設定不要 | ε と MinPts の選択に敏感      |
| 任意形状のクラスタを発見可能           | スパースな高次元データではパラメータ選定困難 |
| ノイズを自動で除去                | 計算コストが高く、大規模データでは加速が必要 |

---

**実践ガイド**：

1. **k-distance plot**（第 $k$ 最近傍距離）で eps の候補を探す。
2. 高次元データでは**次元削減**や**特徴選択**を先行。
3. クラスタ密度が大きく異なる場合は、階層的またはローカルなパラメータ調整を検討。
